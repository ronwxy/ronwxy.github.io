{"meta":{"title":"半路雨歌","subtitle":"Sing In The Storm","description":"他说风雨中这点痛算什么，至少我们还有梦，至少还可以歌唱","author":"雨歌","url":"http://blog.jboost.cn","root":"/"},"pages":[{"title":"关于我","date":"2020-05-07T03:09:25.845Z","updated":"2020-05-07T03:09:25.845Z","comments":true,"path":"about/index.html","permalink":"http://blog.jboost.cn/about/index.html","excerpt":"","text":"上海交通大学计算机应用技术硕士毕业，十多年软件技术研发经验 多年软件及互联网行业从业经验。从世界500强到国企，到民企，再到创业公司，经历过几百人参与的跨国大项目，也从0到1主管研发过多款互联网产品 以技术负责人身份主导过多个项目的微服务架构实践，负责过日均TB级大数据平台的研发与运维 多年技术团队管理经验，担任过技术主管，技术经理，技术总监等职务，目前在一公司担任技术总监、研发副总职位 曾利用空闲时间整理过一些技术分享，但终究由于阶段性太忙或其它原因未能坚持。 虽然现在较少写代码，但一直有意愿将多年企业产品与项目技术实践及团队管理的经验分享出来，一方面给有需求的软件与互联网行业技术人员（尤其是计算机相关专业，并有志于从事软件技术工作的高校学生）以参考，另一方面也是对自己技术与经验的梳理、总结。于是花了点时间整了这个博客，希望能坚持下来。 我的github地址，里面有文章涉及源码或其它项目，欢迎follow、打星 为了能及时收到更新的分享文章，欢迎关注我的微信公众号"}],"posts":[{"title":"写一个自己的脚手架——单体应用篇","slug":"framework-boot","date":"2021-01-21T06:20:51.000Z","updated":"2021-01-22T09:55:56.212Z","comments":true,"path":"framework-boot.html","link":"","permalink":"http://blog.jboost.cn/framework-boot.html","excerpt":"接上一篇 写一个自己的脚手架——基础组件篇，本篇为基于基础组件搭建的单体应用框架介绍。单体应用框架提供常见的一些基础通用功能实现，目前主要为权限管理，提供前后端分离的后端 Spring Boot 项目与前端 Vue 项目。 本项目还在持续完善中。","text":"接上一篇 写一个自己的脚手架——基础组件篇，本篇为基于基础组件搭建的单体应用框架介绍。单体应用框架提供常见的一些基础通用功能实现，目前主要为权限管理，提供前后端分离的后端 Spring Boot 项目与前端 Vue 项目。 本项目还在持续完善中。 后端 Spring Boot 项目依赖基础组件项目：https://github.com/ronwxy/jboost-base 目前包含模块： jboost-auth： 权限管理， 包含基本功能： 登录、鉴权 用户管理 角色管理 部门管理 菜单管理 jboost-bootstrap： 项目启动入口 目前暂无其它功能 开发规范 模块命名使用 jboost-模块名，如 jboost-auth 模块根包命名使用 cn.jboost.boot.模块名 开头，如 cn.jboost.boot.auth 但 jboost-bootstrap 模块根包名为 cn.jboost.boot，便于启动类扫描整个项目 各模块包含基本结构： 12345678-- controller -- mapper -- pojo -- dto -- converter -- entity -- query -- service Service 层对于只有一个实现的，无需定义 Interface 包名、类名都使用单数形式，如 User， 而不是 Users 以上代码结构可使用 jboost-base 中的 jboost-generator 生成，生成配置参考项目 resource/generator.yaml 中的注释说明。 前端 Vue 项目基于 vue-admin-template 搭建 vue-admin-template 是一个极简的 vue admin 管理后台。它只包含了 Element UI &amp; axios &amp; iconfont &amp; permission control &amp; lint，这些搭建后台必要的东西。 项目包含基本的登录，含用户名登录与手机验证码登录（需自行配置 sms 服务），登录页效果如图 及基本的系统管理功能，如下图所示 项目的运行及打包可参考项目的 README 文档。 总结单体应用框架包含一个系统常用的基础功能，基于此可进行其它业务模块开发，提高开发效率。本文对单体应用框架的前后端项目进行了简单介绍，目前项目只包含基本的权限管理功能，其它基础功能待持续完善。因水平有限，疏漏、错误之处难免，欢迎指正。 单体应用框架项目源码： 服务端： https://github.com/ronwxy/jboost-boot.git Web端： https://github.com/ronwxy/jboost-admin.git 如对你有帮助，可以大方地 star 一下。","categories":[{"name":"Framework","slug":"Framework","permalink":"http://blog.jboost.cn/categories/Framework/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"}]},{"title":"写一个自己的脚手架——基础组件篇","slug":"framework-base","date":"2021-01-15T10:44:57.000Z","updated":"2021-01-16T03:51:39.070Z","comments":true,"path":"framework-base.html","link":"","permalink":"http://blog.jboost.cn/framework-base.html","excerpt":"现在很多 Web 项目，包括普通的前后端分离项目，微服务项目，都基于 Spring Boot 搭建，项目中一般需要包含一些通用的特性或功能，如统一异常处理、切面日志功能、权限管理功能等。写一个自己的脚手架，对一些基本规范、基础功能进行沉淀，基于此可只需要关注业务实现，提高开发效率。 目前整个脚手架包含三部分： 基础组件：对单个项目（服务）的一些基本特性、功能等进行封装，便于复用，或达到常说的“开箱即用” Spring Boot 单体项目脚手架：基于基础组件，提供常见的一些基础通用功能实现的单体项目框架，如权限管理，提供前后端分离的后端 Spring Boot 项目与前端 Vue 项目 Spring Cloud（包含 Spring Cloud Alibaba）微服务项目框架：基于基础组件，提供微服务框架的基础组件与功能实现，如网关（Spring Cloud Gateway），服务注册中心与配置管理（Nacos），服务限流降级（Sentinel），统一权限管理（Spring Security Oauth2）等。 本项目还在持续完善中，本篇为基础组件介绍。","text":"现在很多 Web 项目，包括普通的前后端分离项目，微服务项目，都基于 Spring Boot 搭建，项目中一般需要包含一些通用的特性或功能，如统一异常处理、切面日志功能、权限管理功能等。写一个自己的脚手架，对一些基本规范、基础功能进行沉淀，基于此可只需要关注业务实现，提高开发效率。 目前整个脚手架包含三部分： 基础组件：对单个项目（服务）的一些基本特性、功能等进行封装，便于复用，或达到常说的“开箱即用” Spring Boot 单体项目脚手架：基于基础组件，提供常见的一些基础通用功能实现的单体项目框架，如权限管理，提供前后端分离的后端 Spring Boot 项目与前端 Vue 项目 Spring Cloud（包含 Spring Cloud Alibaba）微服务项目框架：基于基础组件，提供微服务框架的基础组件与功能实现，如网关（Spring Cloud Gateway），服务注册中心与配置管理（Nacos），服务限流降级（Sentinel），统一权限管理（Spring Security Oauth2）等。 本项目还在持续完善中，本篇为基础组件介绍。 基础组件基础组件提供 jboost-common：基础工具类，常量类 jboost-dependencies：统一依赖版本管理 jboost-generator：代码生成，根据数据库表自动生成各层源代码 jboost-parent：父项目，集成了数据库、redis、统一异常处理、统一响应封装、切面日志等功能 jboost-starters：常用功能 starter jboost-starter-alimq：阿里 RocketMq 消息队列 jboost-starter-alioss：阿里云对象服务，上传图片、视频等 jboost-starter-alisms：阿里短信服务 jboost-starter-error：统一异常处理 jboost-starter-limiter：基于 Redis 的分布式锁，基于 Guava RateLimiter（令牌桶算法）的限速，及基于 Redis Lua 的限量（时间窗口内限制访问量）实现 jboost-starter-logging：切面日志功能 jboost-starter-web：日期格式化、跨域、添加请求ID、请求响应封装、Swagger集成等 下载安装123456git clone https://github.com/ronwxy/jboost-base.gitcd jboost-base#安装到本地mvn clean install #部署到maven仓库#mvn clean deploy jboost-common基础类库，包括工具类，常量类等 Bean 转换BaseConverter： Bean 转换，主要用于 entity 与 dto 之间的转换，使用 MapStruct 框架。 针对需要转换的 Bean，创建相应接口，示例 123@Mapper(componentModel = BaseConverter.SPRING,uses = &#123;&#125;,unmappedTargetPolicy = ReportingPolicy.IGNORE)public interface UserConverter extends BaseConverter&lt;User, UserDTO&gt; &#123;&#125; 异常定义 BizException： 业务异常基类 ClientSideException： 客户端异常，如参数错误 UnauthorizedException： 鉴权失败异常，如token过期 ForbiddenException： 访问受限异常，如访问资源未授权 ServerSideException： 服务端异常，服务端内部操作异常，如数据库访问出错 ExceptionUtil： 异常工具类 业务中处理异常有两种形式。 根据条件抛出异常，交由统一异常处理机制处理（如记录日志，返回客户端错误提示） 1234User existUser = findUserByName(name);if (existUser != null) &#123; ExceptionUtil.rethrowClientSideException(\"用户名已存在\");&#125; 捕获异常后重新抛出异常，交由统一异常处理机制处理（如记录日志，返回客户端错误提示） 12345try &#123; aliSmsProvider.sendVerifyCode(phone, verifyCode.getCode());&#125; catch (ClientException ex) &#123; ExceptionUtil.rethrowServerSideException(\"发送验证码失败，请稍后重试\", ex);&#125; 消息队列抽象使用消息队列的地方可以基于该接口与消息对象调用，屏蔽具体消息中间件信息 MqMessage：消息对象，自身包含 topic, tag, key 等信息 IMqProducer：消息生产者接口，使用具体消息中间件时，生产者服务实现该接口 其它工具与常量类安全相关： EncryptUtil：RSA非对称加密（公钥/私钥）工具类 JwtUser：用于 Spring Security 认证的 user bean JwtUtil：Jwt 工具类 SecurityUtil：Security 工具类，可获取当前登录用户的相关信息 其它： EasyPoiUtil：文件上传工具类 FileUtil：文件操作工具类 LocalDateTimeUtil：日期工具类 MyBatisUtil：可用于 mybatis xml 文件条件判断的工具类 ResponseWrapper：客户端响应封装 VerifyCodeUtil：图形验证码工具类 WebUtil：Web工具类，设置req-id；获取客户端IP；输出响应 依赖管理jboost-dependencies：依赖版本管理，管理常用依赖库的版本，其它继承或以 dependencyManagement 引入 jboost-dependencies 的项目引用依赖时可省略版本号 12345678910111213141516171819202122&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;cn.jboost.boot&lt;/groupId&gt; &lt;artifactId&gt;jboost-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 代码生成jboost-generator： 代码生成工具，采用 mybatis-plus-generator 实现，配置详见 resource/generator.yaml 文件示例 针对每个数据库表，可生成如下代码结构 123456789101112131415--controller UserController.java--mapper UserMapper.java--pojo----dto------converter UserConverter.java UserDTO.java----entity UserEntity.java----query UserQuery.java--service UserService.java 父项目jboost-parent：父项目，集成了数据库、redis、统一异常处理、统一响应封装、切面日志等功能 starterjboost-starters jboost-starter-alimq阿里 RocketMq 消息队列使用示例 项目 pom.xml 中引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;cn.jboost.boot&lt;/groupId&gt; &lt;artifactId&gt;jboost-starter-alimq&lt;/artifactId&gt;&lt;/dependency&gt; application.yaml 文件中添加配置： 123456789aliyun: rocketmq: accessKey: xxxx secretKey: xxxx namesrvAddr: http://xxx.mqrest.cn-hangzhou.aliyuncs.com sendMsgTimeoutMillis: 3000 #topic 所属实例ID instanceId: xxxx groupId: xxxx 使用生产者发送消息（RocketMqAutoConfiguration 已配置了本地开发测试环境走HTTP协议 RocketMqRemoteProducer，云端生产环境走TCP协议 RocketMqInternalProducer） 12345678910111213@Value(\"$&#123;socket.mq.topic&#125;\")private String socketMqTopic;@Value(\"$&#123;socket.mq.tag&#125;\")private String socketMqTag;@Autowiredprivate IMqProducer mqProducer;public void noticeWeb(String macAddress, String data) &#123; SocketMqMessage socketMqMessage = new SocketMqMessage(macAddress, data); mqProducer.sendMessage(new MqMessage(socketMqTopic, socketMqTag, SocketMQCommandEnum.noticeWebChannel.getCode(), socketMqMessage));&#125; 消息消费者 jboost-starter-alioss阿里云对象服务，上传图片、视频等 项目 pom.xml 中引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;cn.jboost.boot&lt;/groupId&gt; &lt;artifactId&gt;jboost-starter-alioss&lt;/artifactId&gt;&lt;/dependency&gt; application.yaml 文件中添加配置： 1234567aliyun: oss: accessKeyId: xxxx accessKeySecret: xxxx bucket: xxxx domain: http://xxx.oss-cn-hangzhou.aliyuncs.com/ endpoint: https://oss-cn-hangzhou.aliyuncs.com 使用 AliOssProvider 来上传文件，或对文件路径进行签名（针对需要授权访问的资源） 123456@Autowiredprivate AliOssProvider aliOssProvider;//保存文件String fileSave(String bizType, File file, String fileName);//url签名void signUrl(List&lt;T&gt; list, List&lt;String&gt; urlFields, String bucketName, long expire) 更多方法参考 cn.jboost.base.starter.alioss.AliOssProvider jboost-starter-alisms阿里短信服务 项目 pom.xml 中引入依赖：1234&lt;dependency&gt; &lt;groupId&gt;cn.jboost.boot&lt;/groupId&gt; &lt;artifactId&gt;jboost-starter-alisms&lt;/artifactId&gt;&lt;/dependency&gt; application.yaml 中添加配置 12345678aliyun: #阿里短信 sms: accessKeyId: xxxx accessKeySecret: xxxx signName: 签名 templateCode: xxxx regionId: xxxx 使用 123456@Autowiredprivate AliSmsProvider aliSmsProvider;//发送验证码aliSmsProvider.sendVerifyCode(phone, verifyCode.getCode());//发送短信,支持以逗号分隔的形式进行批量调用，批量上限为20个手机号码,批量调用相对于单条调用及时性稍有延迟,验证码类型的短信推荐使用单条调用的方式aliSmsProvider.sendSms(String phoneNumber, String signName, String smsTempCode, String tempParam) jboost-starter-error统一异常处理 项目 pom.xml 中引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;cn.jboost.boot&lt;/groupId&gt; &lt;artifactId&gt;jboost-starter-error&lt;/artifactId&gt;&lt;/dependency&gt; 使用 @RestControllerAdvice + @ExceptionHandler 统一处理抛出的异常 BizException：以 400 返回异常消息 AccessDeniedException：以 403 返回异常消息 IllegalArgumentException，IllegalStateException：以 400 返回异常消息 Exception： 以 500 返回异常消息 打印异常栈 针对 profile “default”,”local”,”dev”， 打印异常栈； 针对 profile “test”, “formal”, “prod”，不打印异常栈。 org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController 现在通过 request 传 trace=true 参数的方式来控制是否打印异常栈，后续可做相应调整 jboost-starter-limiter提供基于 Redis 的分布式锁，基于 Guava RateLimiter（令牌桶算法） 的限速，及基于 Redis Lua 的限量实现 项目 pom.xml 中引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;cn.jboost.boot&lt;/groupId&gt; &lt;artifactId&gt;jboost-starter-limiter&lt;/artifactId&gt;&lt;/dependency&gt; 分布式锁在方法上添加注解 @DistributedLockable，对方法进行分布式环境下的同步， 1234@DistributedLockable(key=\"\", prefix=\"disLock:\", expire=5)public void syncDistributed() &#123; //... &#125; key：redis 使用 prefix+key 来作为缓存key prefix：redis key 前缀，默认为 disLock: expire：过期时间，默认为10s 限流 限速：使用 @RateLimit 注解严格控制访问速率，在一次访问后，必须经过设定的时间间隔才能进行下一次访问 12345@GetMapping(\"/rate\")@RateLimit(rate = 1.0/5, burst = 5.0, expire = 120, timeout = 1)public String rateLimit(@RequestParam(\"key\") String key)&#123; return \"test rate limiter...\";&#125; 上例表示以限制访问速度为5秒1次，一次访问后，直到5s之后才能再次访问。 支持的属性配置： key： redis 使用 prefix+key 来作为缓存 key prefix：redis key 前缀， 默认为 “rateLimit:”; expire：表示令牌桶模型 RedisPermits redis key 的过期时间/秒，默认为 60; rate：permitsPerSecond 值，表示以每秒rate的速率放置令牌，默认为 1.0; burst：maxBurstSeconds 值，表示最多保留burst秒的令牌，默认为 1.0; timeout：取令牌的超时时间，秒，默认为 0，表示不等待立即返回; limitType： 默认 LimitType.METHOD; LimitType 主要用于控制 key 的值，支持类型如下， IP：根据客户端IP限流 USER：根据用户限流，用户已经登录，通过SecurityUtil.getUserId()获取当前用户ID METHOD：根据方法名全局限流 CUSTOM：自定义，需要设置 key 的值，自定义 key 支持表达式，如 #{id}, #{user.id} 限量：使用 @CountLimit 注解来控制在一个时间窗口内，允许访问的次数，在允许次数内对访问速率不限制 12345@GetMapping(\"/count\")@CountLimit(key = \"#&#123;key&#125;\", limit = 2, period = 10, limitType = LimitType.CUSTOM)public String countLimit(@RequestParam(\"key\") String key)&#123; return \"test count limiter...\";&#125; 上例表示在10s内限制访问2次，至于这两次以什么样的时间间隔访问不做限制。 支持的属性配置： key：redis 使用 prefix+key 来作为缓存 key prefix：key 前缀，默认 “countLimit:”; limit：period 时间段内限制访问的次数，默认1; period：表示时间段/秒，默认1; limitType： 默认 LimitType.METHOD; 非注解形式限流：也可以使用如下形式对某段代码进行限流控制 123456789@GetMapping(\"/rate2\")public String testRateLimit()&#123; RedisRateLimiter limiter = redisRateLimiterFactory.build(\"LimiterController.testRateLimit\", 1.0/30, 30, 120); if(!limiter.tryAcquire(\"app.limiter\", 0, TimeUnit.SECONDS)) &#123; System.out.println(LocalDateTime.now()); ExceptionUtil.rethrowClientSideException(\"您的访问过于频繁，请稍后重试\"); &#125; return \"test rate limiter 2...\";&#125; 更多详细内容参考 一个轻量级的基于RateLimiter的分布式限流实现 jboost-starter-logging切面日志功能 对用注解 cn.jboost.base.starter.logging.annotation.Log 修饰的类方法进行日志记录 项目 pom.xml 中引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;cn.jboost.boot&lt;/groupId&gt; &lt;artifactId&gt;jboost-starter-logging&lt;/artifactId&gt;&lt;/dependency&gt; 在方法或类上（对类内所有方法有效，一般用于 Controller 类上）添加注解 @Log， 123456import cn.jboost.base.starter.logging.annotation.Log;@Logpublic class UserController &#123; //...&#125; 日志打印添加注解 @Log 后，当方法被调用时，默认将打印调用与返回日志，如 12jboost-boot - [2021-01-09 09:41:21] [http-nio-8000-exec-1] INFO [5ff909c1a94b6d1f4c62f021 - ] c.j.b.auth.controller.AuthController - call: getCaptcha()jboost-boot - [2021-01-09 09:41:21] [http-nio-8000-exec-1] INFO [5ff909c1a94b6d1f4c62f021 - 216] c.j.b.auth.controller.AuthController - return: getCaptcha():VerifyCodeUtil.VerifyCode(code=xqho, uuid=9d8ffc7e9e324ed2a5a9566d9a7f115d) 其它配置 @Log 注解提供了两个属性配置 logPoint：可配置在什么位置打印日志，有 LogPoint.IN（调用时打印）, LogPoint.OUT（返回时打印）, LogPoint.BOTH（调用与返回时都打印） logException：是否对异常进行日志记录，默认为true 可对日志输出的实现进行定制化，默认使用 cn.jboost.base.starter.logging.provider.Slf4jLogProvider，采用项目中slf4j的框架进行输出，如logback,log4j2等。如果要自定义日志输出，则可提供一个 cn.jboost.base.starter.logging.provider.ILogProvider 接口的实现类，然后配置 123aoplog: service-impl-class: aop日志记录实现类，默认为 cn.jboost.base.starter.logging.service.Slf4jLogService collection-depth-threshold: 5 #集合类参数输出元素个数，默认为10 jboost-starter-web日期格式化、跨域、添加请求ID、请求响应封装、Swagger集成等 项目 pom.xml 中引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;cn.jboost.boot&lt;/groupId&gt; &lt;artifactId&gt;jboost-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 日期格式化（序列化与反序列化时） LocalDateTime： yyyy-MM-dd HH:mm:ss LocalDate： yyyy-MM-dd LocalTime： HH:mm:ss 跨域 默认开启，如要关闭，配置 12web: cors: disable # 关闭跨域 添加请求ID 默认开启，如要关闭，配置 12web: reqId: disable # 关闭日志中加入请求id 开启后，会在每一个请求的 Header 中添加 key 为 Req-Id， value 为 uuid 的 header，下游可通过 WebUtil.getRequestId() 获取（例如将其存入MDC中，在日志打印时输出请求ID，将日志进行串联） 请求响应封装 默认开启，如要关闭，配置（建议默认开启，如果关闭，还需要在异常处理中将响应格式进行处理） 12web: responseWrapper: disable # 关闭响应消息封装 开启后，对请求响应按照 cn.jboost.base.common.util.ResponseWrapper 的结构进行封装 swagger集成 默认关闭，如要开启，配置 123456789swagger: #是否开启 swagger-ui enabled: true #定义扫描包,可配置多条扫描包,包之间用逗号隔开 basePackages: cn.jboost #其它配置，可省略，使用默认配置 title: \"服务端接口文档\" version: 1.0 tokenHeader: Authorization 建议只在开发环境（application-dev.yaml）中开启 过滤器中异常处理 主要是对响应结果进行封装统一： cn.jboost.base.starter.web.ExceptionHandlerFilter 总结基础组件是后续单体应用框架、微服务项目框架的基础。本文对基础组件目前提供的功能、特性进行了介绍，因水平有限，疏漏、错误之处难免，欢迎指正。基础组件项目源码： https://github.com/ronwxy/jboost-base.git， 如对你有帮助，可以大方地 star 一下。","categories":[{"name":"Framework","slug":"Framework","permalink":"http://blog.jboost.cn/categories/Framework/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"}]},{"title":"Spring Cloud（十）：配置管理，除了Config，还有Nacos","slug":"springcloud-10","date":"2020-12-12T07:26:21.000Z","updated":"2020-12-17T11:17:06.210Z","comments":true,"path":"springcloud-10.html","link":"","permalink":"http://blog.jboost.cn/springcloud-10.html","excerpt":"使用 Spring Cloud Config 我们可以实现配置的集中化管理，但 Spring Cloud Config 如果要实现配置的动态更新， 则需要借助 Spring Cloud Bus（参考 Spring Cloud（八）：使用Spring Cloud Bus来实现配置动态更新）。 Nacos 不仅可作为服务注册中心，同时还可以作为配置的集中化管理中心，且其自身默认就支持动态更新，集成非常方便。","text":"使用 Spring Cloud Config 我们可以实现配置的集中化管理，但 Spring Cloud Config 如果要实现配置的动态更新， 则需要借助 Spring Cloud Bus（参考 Spring Cloud（八）：使用Spring Cloud Bus来实现配置动态更新）。 Nacos 不仅可作为服务注册中心，同时还可以作为配置的集中化管理中心，且其自身默认就支持动态更新，集成非常方便。 Nacos 配置管理集成 pom.xml 中添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;&lt;/dependency&gt; bootstrap.yaml 中添加配置： 12345678spring: cloud: nacos: discovery: server-addr: 192.168.40.201:8848 config: server-addr: 192.168.40.201:8848 file-extension: yaml # 默认是properties格式 spring.cloud.nacos.config.server-addr 如果配置的是域名， 则端口不能省略， 即便是 80 端口也不能省略。 访问配置 在程序中可通过 @Value 注解或 @ConfigurationProperties 修饰的类来访问配置信息 123456789101112@RestController@RefreshScopepublic class TestController &#123; @Value(\"$&#123;test.hello&#125;\") private String hello; @GetMapping(\"test\") public String test(@RequestParam(\"name\")String name) &#123; return hello + name; &#125;&#125; Nacos 默认是支持动态更新配置的，当你在控制台修改后，本地立即可获取到更新的配置，但如果需要程序动态更新，则仍需要在相应类上添加 @RefreshScope 注解 控制台添加配置 在 Nacos 控制台“配置管理”中，可以针对项目添加配置，如图 每一个配置通过 Group 与 DataId 来确定，Group 默认为 DEFAULT_GROUP（可通过 spring.cloud.nacos.config.group 指定）， 如图 配置格式需要与前面配置的 spring.cloud.nacos.config.file-extension 一致，默认是 properties 格式。 配置支持 profile，spring.profiles.active 必须配置在 bootstrap.properties（yaml） 文件中。 启动项目时， 启动日志中能看到如下信息 1Located property source: [BootstrapPropertySource &#123;name='bootstrapProperties-robot-dev.yaml,DEFAULT_GROUP'&#125;, BootstrapPropertySource &#123;name='bootstrapProperties-robot.yaml,DEFAULT_GROUP'&#125;, BootstrapPropertySource &#123;name='bootstrapProperties-robot,DEFAULT_GROUP'&#125;] Nacos 默认会加载 DataId 为 {spring.application.name}-{spring.profiles.active}.yaml，{spring.application.name}.yaml，{spring.application.name} 的配置源，其中文件扩展名 yaml 与你配置有关，默认为 properties。配置优先级顺序为从前往后依次降低。 动态更新Nacos 默认是支持动态更新配置，如果要禁用，可配置 spring.cloud.nacos.config.refresh-enabled=false。如果需要程序不重启实现配置的动态更新，还需在相应类上添加 @RefreshScope 注解 命名空间Nacos 支持自定义 namespace， 在多租户环境中，或同一租户的不同环境中（如，开发环境、测试环境、生产环境）可以使用不同的 namespace 来进行配置隔离。 在 Nacos 控制台，我们可以根据需要添加命名空间，如图 然后在 bootstrap.yaml 中，针对不同的 profile， 指定不同的 namespace ID， 如下 dev 环境指定 namespace 名称为 dev 的 ID。 123456789---spring: profiles: dev cloud: nacos: discovery: namespace: 1a743385-86e7-4405-b73f-0108df042710 config: namespace: 1a743385-86e7-4405-b73f-0108df042710 上述配置在服务以 spring.profiles.active=dev 启动后，将注册到 dev namespace 中，并从 dev namespace 中获取配置。 如果spring.cloud.nacos.config.namespace未指定，则默认使用 Public 自定义 DataIdDataId 名称默认情况下从如下三个配置获取， spring.cloud.nacos.config.prefix spring.cloud.nacos.config.name spring.application.name 首先取 prefix，如果没有则取 name， 最后才取 spring.application.name。 我们可以通过 extension-configs 来自定义 DataId， 示例如下 1234567891011# 1. 默认使用 DEFAULT_GROUP，不支持配置的动态更新 spring.cloud.nacos.config.extension-configs[0].data-id=ext-config-1.properties # 可以配置多个data id，逗号隔开# 2. 指定 Group，不支持配置的动态更新spring.cloud.nacos.config.extension-configs[1].data-id=ext-config-2.properties spring.cloud.nacos.config.extension-configs[1].group=GLOBALE_GROUP # 可以配置多个group# 3. 指定 Group，支持配置的动态更新spring.cloud.nacos.config.extension-configs[2].data-id=ext-config-3.properties spring.cloud.nacos.config.extension-configs[2].group=REFRESH_GROUP spring.cloud.nacos.config.extension-configs[2].refresh=true 自定义 DataId 必须有文件扩展名（properties 或 yaml/yml），这与 spring.cloud.nacos.config.file-extension 配置的扩展名没什么干系。自定义 DataId 默认不支持动态更新，需显示配置。自定义 DataId 配置的优先级与 extension-configs[n] 中的 n 有关，n 越大， 优先级越高。 extension-configs 的 DataId 配置优先级与默认 DataId 的优先级要低。 我们将配置调整如下， 12345678910111213spring: cloud: nacos: discovery: server-addr: 192.168.40.201:8848 config: server-addr: 192.168.40.201:8848 file-extension: yaml extension-configs: - data-id: ext-config-1.properties refresh: true - data-id: ext-config-2.properties refresh: true 重启服务，从启动日志中我们也可以看到配置的优先级顺序。 1Located property source: [BootstrapPropertySource &#123;name='bootstrapProperties-robot-dev.yaml,DEFAULT_GROUP'&#125;, BootstrapPropertySource &#123;name='bootstrapProperties-robot.yaml,DEFAULT_GROUP'&#125;, BootstrapPropertySource &#123;name='bootstrapProperties-robot,DEFAULT_GROUP'&#125;, BootstrapPropertySource &#123;name='bootstrapProperties-ext-config-2.properties,DEFAULT_GROUP'&#125;, BootstrapPropertySource &#123;name='bootstrapProperties-ext-config-1.properties,DEFAULT_GROUP'&#125;] robot-dev.yaml robot.yaml robot ext-config-2.properties ext-config-1.properties 从上到下优先级依次降低。 共享配置自定义的 data id 可以在多个应用之间共享配置，或一个应用使用多个配置，共享配置与自定义配置类似，只需将 extension-configs 改为 shared-configs，如下 12345678910111213spring: cloud: nacos: discovery: server-addr: 192.168.40.201:8848 config: server-addr: 192.168.40.201:8848 file-extension: yaml extension-configs: - data-id: ext-config-1.properties refresh: true shared-configs: - data-id: commom.properties shared-configs 配置中， 后面的优于前面的；shared-configs 的 data id 也必须有扩展名，spring.cloud.nacos.config.file-extension 定义的扩展名对它没什么影响。 从启动日志中我们也可以看到共享 DataId 配置（shared-configs）的优先级低于自定义 DataId 配置（extension-configs）。 extension-configs 与 shared-configs 配置对应 com.alibaba.cloud.nacos.NacosConfigProperties.Config 类，支持三个属性： dataId：DataId 名称 group： DataId 所在的 Group，默认为 DEFAULT_GROUP refresh： 是否支持动态更新，默认为 false Nacos Config 配置属性Nacos 配置管理的属性汇总（属性前缀均为 spring.cloud.nacos.config） 属性 默认值 描述 server-addr nacos 服务的 IP：端口 prefix DataId 名称首先从 prefix 获取，如果为空则从 name 获取，最后从 spring.application.name 获取 name 同上 encode 配置内容的编码字符集 namespace public 命名空间，用于隔离管理 group DEFAULT_GROUP DataId 所在 group fileExtension properties DataId 的后缀或扩展名，支持properties 或 yaml(yml) timeout 3000 从 nacos 获取配置的超时时间，单位毫秒 endpoint nacos 服务名称，服务器地址可通过它动态获取 accessKey 阿里云账号的 accesskey secretKey 阿里云账号的 secretkey contextPath nacos 服务上下文路径 clusterName Nacos 服务集群名称 extensionConfigs 自定义 DataId， 一个列表，以Config 类构建，支持三个属性： dataId, group 与 refresh sharedConfigs 共享配置列表，与 extensionConfigs 类似 actuator 接口nacos config 提供了一个 actuator endpoint （nacos-config）来暴露三个属性： Sources：当前应用配置的数据信息，包括 dataId 与 上一次同步时间 RefreshHistory：配置刷新历史 NacosConfigProperties： 当前服务的基本 Nacos 配置及包含的 sharedConfigs， extensionConfigs， configServiceProperties，以及向后兼容的 extConfig， sharedDataids 等 通过访问：http://localhost:8080/actuator/nacos-config 可查看。 总结 Nacos 配置以 namespace, group, dataId 的组织形式进行管理，可通过 namespace 来隔离不同环境或不同租户。 配置支持 profile，spring.profiles.active 必须配置在 bootstrap.properties（yaml） 文件中。 Nacos 默认支持动态更新配置，需要程序不重启实现配置的动态更新，还需在相应类上添加 @RefreshScope 注解。 Nacos 配置包括默认 DataId（按 prefix, name, spring.application.name 顺序获取），自定义 DataId（extensionConfigs）， 共享 DataId（sharedConfigs），其优先级按默认-自定义-共享依次降低。","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/categories/SpringCloud/"}],"tags":[{"name":"nacos","slug":"nacos","permalink":"http://blog.jboost.cn/tags/nacos/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/tags/SpringCloud/"}]},{"title":"Spring Cloud（九）：服务发现，除了Eureka，还有Nacos","slug":"springcloud-9","date":"2020-12-05T02:59:41.000Z","updated":"2020-12-08T00:51:49.204Z","comments":true,"path":"springcloud-9.html","link":"","permalink":"http://blog.jboost.cn/springcloud-9.html","excerpt":"服务注册中心是微服务的核心组件之一，目前常见的服务注册中心包括Eureka（参考：Spring Cloud（一）：服务注册中心Eureka），Zookeeper，Consul，CoreDNS，及 Nacos。","text":"服务注册中心是微服务的核心组件之一，目前常见的服务注册中心包括Eureka（参考：Spring Cloud（一）：服务注册中心Eureka），Zookeeper，Consul，CoreDNS，及 Nacos。 Nacos 介绍Nacos 是阿里开源的一个服务注册与配置管理的实现，提供动态的服务发现、服务配置、服务元数据及流量管理等功能。项目地址： https://github.com/alibaba/nacos Nacos的关键特性包括： 服务发现与服务健康监测 Nacos 支持基于 DNS 和基于 RPC 的服务发现。服务提供者使用原生 SDK、OpenAPI、或一个独立的 Agent TODO 注册服务后，服务消费者可以使用 DNS TODO 或 HTTP&amp;API 来发现服务。同时，Nacos 提供对服务的实时健康检查，阻止向不健康的服务实例发送请求。Nacos 支持传输层 (PING 或 TCP) 和应用层 (如 HTTP、MySQL、用户自定义）的健康检查。 动态配置服务 动态配置服务可以以中心化、外部化和动态化的方式管理所有环境的应用配置和服务配置。动态配置使得配置更新时，服务不需要重启就能完成动态更新。配置的中心化管理让实现无状态服务变得更简单，让服务按需弹性扩展变得更容易。Nacos 提供了一个简洁易用的控制台界面来管理所有的服务和应用的配置。Nacos 还提供了包括配置版本跟踪、金丝雀发布、一键回滚配置以及客户端配置更新状态跟踪在内的一系列开箱即用的配置管理特性，帮助您更安全地在生产环境中管理配置变更和降低配置变更带来的风险。 动态 DNS 服务 动态 DNS 服务支持权重路由，让您更容易地实现中间层负载均衡、更灵活的路由策略、流量控制以及数据中心内网的简单DNS解析服务。动态DNS服务还能让您更容易地实现以 DNS 协议为基础的服务发现，以帮助您消除耦合到厂商私有服务发现 API 上的风险。Nacos 提供了一些简单的 DNS APIs TODO 帮助您管理服务的关联域名和可用的 IP:PORT 列表。 服务及其元数据管理 Nacos 能让您从微服务平台建设的视角管理数据中心的所有服务及元数据，包括管理服务的描述、生命周期、服务的静态依赖分析、服务的健康状态、服务的流量管理、路由及安全策略、服务的 SLA 以及最首要的 metrics 统计数据。 Nacos 部署下载：https://github.com/alibaba/nacos/releases ， 目前最新版 1.4.0 单机模式启动 windows 下： 1nacos-1.3.2\\bin\\startup.cmd -m standalone linux下： 1nacos-1.3.2\\bin\\startup.sh -m standalone 单机模式适合在本地或开发测试环境中使用，如果是生产环境，则需要搭建集群模式或通过 k8s 部署来实现高可用。 集群模式 集群配置 将 nacos conf 目录下的 cluster.conf.example 文件重命名为 cluster.conf，添加集群节点： 12345#it is ip#example192.168.40.111:8848192.168.40.112:8848192.168.40.113:8848 需要配置3个或3个以上的 Nacos 节点才能构成集群。 数据源配置 使用 数据库脚本文件 创建 MySQL 数据库在 conf 目录下的 application.properties 配置文件中配置数据源 1234567891011#*************** Config Module Related Configurations ***************#### If use MySQL as datasource:spring.datasource.platform&#x3D;mysql### Count of DB:db.num&#x3D;1### Connect URL of DB:db.url.0&#x3D;jdbc:mysql:&#x2F;&#x2F;192.168.40.113:3306&#x2F;nacos?characterEncoding&#x3D;utf8&amp;connectTimeout&#x3D;1000&amp;socketTimeout&#x3D;3000&amp;autoReconnect&#x3D;true&amp;useUnicode&#x3D;true&amp;useSSL&#x3D;false&amp;serverTimezone&#x3D;Asia&#x2F;Shanghaidb.user&#x3D;nacosdb.password&#x3D;nacos 生产环境可采用主备模式，或者高可用数据库。 启动服务器 在三台服务器节点上依次启动。 启动 1sh startup.sh 关闭 1sh shutdown.sh 可以结合使用 keepalived 来实现集群的访问，部署结构如图（因作者使用 k8s 部署，集群的具体部署实现这里不详述，可自行查阅 keepalived 相关资料） k8s部署Nacos 也可以以 StatefulSet 类型的控制器部署在 k8s 中，并且可借助 NFS 与 MySQL 来实现自动扩容缩容和数据持久化功能。如何在 k8s 中部署，我们后续再单独详细介绍。 Nacos 注册中心集成在 Spring Cloud 中使用 Nacos，只需要简单的配置就可以完成服务的注册与发现。 pom.xml 中添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; bootstrap.yaml 中添加配置： 123456789spring: cloud: nacos: discovery: server-addr: localhost:8848 # 此部分为动态配置服务相关 config: server-addr: localhost:8848 file-extension: yaml 在启动类上添加 @EnableDiscoveryClient 注解。不加也没关系。 启动服务后，其它注册到 Nacos 的服务就可以通过 Feign 或 @LoadBalance 注解的 RestTemplate 来负载均衡地调用该服务接口了。 访问 Nacos 控制台 浏览器访问 http://localhost:8848/nacos ，账号密码默认为： nacos/nacos。 点击 服务管理/服务列表，即可看到已经注册的服务，如图 进入 服务详情，可对服务的元数据，服务下线、上线进行编辑控制，如图 对服务实例进行下线后，其它服务将不再往该服务实例发送请求（可能有数秒的延迟）。 openAPI支持 服务注册： 1curl -X PUT &#39;http:&#x2F;&#x2F;127.0.0.1:8848&#x2F;nacos&#x2F;v1&#x2F;ns&#x2F;instance?serviceName&#x3D;nacos.naming.serviceName&amp;ip&#x3D;192.168.40.234&amp;port&#x3D;8080&#39; 服务发现： 1curl -X GET &#39;http:&#x2F;&#x2F;127.0.0.1:8848&#x2F;nacos&#x2F;v1&#x2F;ns&#x2F;instance&#x2F;list?serviceName&#x3D;nacos.naming.serviceName&#39; 其它注册中心了解 Eureka：Eureka 包含 Eureka Server 与 Eureka Client 两个端，Eureka Client 通过服务注册接口将微服务所在的 IP，端口，hostname，健康检查 url 等信息注册到 Eureka Server，其它微服务可通过服务发现接口从 Eureka Server 获取服务列表信息。Eureka Server 可以运行多个实例来构成集群，提供注册服务的高可用。Eureka Server 集群采用的是 Peer to Peer 去中心化的架构，每一个实例都是对等的，彼此通过相互注册与数据复制同步来提高可用性。从 CAP 原则的角度说，Eureka 遵循的是 AP，即 Eureka 保障可用性与分区容忍性，但不保障一致性。这样 Eureka Client 获取服务列表信息时，有可能存在不一致性，比如针对某个服务新增一个实例，在注册信息还未在各个 Eureka Server 之间完成同步时，有可能部分 Eureka Client 能获取该实例信息，部分获取不到。但这对服务间的交互影响并不大，因为服务调用最终是通过负载均衡从服务实例列表中选取一个进行的。 Eureka 的具体介绍可参考 Spring Cloud（一）：服务注册中心Eureka。 Zookeeper: Zookeeper 就是个分布式的协调服务，服务提供者通过在 Zookeeper 的某一路径上创建一个 znode 节点完成服务注册，该节点存储了服务的 IP，端口，调用方式等信息。服务消费者通过 Zookeeper 获取到相应服务的 IP 地址列表，通过负载均衡算法从 IP 地址列表中取一个进行服务调用。与 Eureka 不同， Zookeeper 集群采用 Master/Slave 架构，如果集群中的 Master 挂了，集群就要进行 Master 的选举，在此过程中是无法处理请求的。因此 Zookeeper 在 CAP 原则中遵循的是 CP，保证了强一致性与分区容忍性，但并不保证高可用。 Consul: Consul 是 HashiCorp 公司推出的开源工具，用于实现分布式系统的服务发现与配置。Consul 使用 Go 语言编写，内置了服务注册与发现框架、分布式一致性协议实现、健康检查、Key/Value 存储、多数据中心方案等，Consul 不需要依赖其他工具（比如 ZooKeeper 等），使用起来也较为简单。Consul 同样遵循 CAP 原理中的 CP 原则，保证了强一致性和分区容错性。但服务注册的时间会稍长一些，因为 Consul 的 Raft 协议要求必须过半数的节点都写入成功才认为注册成功。并且同样在 leader 挂掉了之后，重新选举出 leader 之前会导致 Consul 服务的不可用。 CoreDNS: CoreDNS 是目前 Kubernetes 默认的 DNS 方案，其目标是成为云原生环境下的 DNS 服务器和服务发现解决方案。CoreDNS 采用 Go 编写，区别于 kube-dns，CoreDNS 编译出来就是一个单独的二进制可执行文件，内置了 cache，backend storage ，health check 等功能，无需第三方组件来辅助实现其他功能，从而使得部署更方便，内存管理更为安全。 总结Nacos 不仅能提供动态服务注册发现功能， 也能提供动态配置管理功能（不需要重启服务就能使配置生效）。从注册中心的功能角色来说，一致性要求并不太高，因此遵循 AP 的 Eureka 与 Nacos（Nacos 在服务注册中遵循 AP， 在配置管理中遵循 CP）更适合用于注册中心。但因为 Eureka 2.x Netflix 已经停止更新维护了，所以新项目可以使用 Nacos 来充当服务注册与配置管理的服务。 参考：https://nacos.io/zh-cn/docs/what-is-nacos.html","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/categories/SpringCloud/"}],"tags":[{"name":"nacos","slug":"nacos","permalink":"http://blog.jboost.cn/tags/nacos/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/tags/SpringCloud/"}]},{"title":"像程序员一样思考——提高解决问题的能力","slug":"think-like-a-programmer","date":"2020-11-04T08:54:41.000Z","updated":"2020-11-05T07:26:32.742Z","comments":true,"path":"think-like-a-programmer.html","link":"","permalink":"http://blog.jboost.cn/think-like-a-programmer.html","excerpt":"在以前的文章中，曾经提过“技术人员的价值，不在于你能写出多么优美的代码，也不在于你能设计出一个多么大而全的高屋建瓴的架构，而在于你实实在在的解决问题的能力，在于你使用技术手段服务于业务的能力”。 最近一段时间，因工作中遇到一些现象，让我重又想起这句话，并且试图思考如何来提高解决问题的能力，有没有一种方法论的手段或者技术性的框架来实践？","text":"在以前的文章中，曾经提过“技术人员的价值，不在于你能写出多么优美的代码，也不在于你能设计出一个多么大而全的高屋建瓴的架构，而在于你实实在在的解决问题的能力，在于你使用技术手段服务于业务的能力”。 最近一段时间，因工作中遇到一些现象，让我重又想起这句话，并且试图思考如何来提高解决问题的能力，有没有一种方法论的手段或者技术性的框架来实践？ 先罗列一两个遇到的现象： 某同事汇报，测试提了一个Bug，当某个用户绑定的卡信息超过50个的时候，后台显示数据就会出现混乱，问能不能限制绑定的卡不超过50个。我问：数据显示出现混乱是什么意思？答：不清楚；我再问：为什么超过50个就会混乱了，少于50个有没有可能出现混乱，造成混乱的原因是什么？答：不知道。我说你先去搞清楚什么叫“混乱”，然后再搞清楚为什么会出现“混乱”再来说解决办法。经过与测试人员的一番沟通后，跟我反馈说不是显示混乱，是显示不全，自己通过查看实现是因为在服务端做了字符串拼接，超过多少就被截断了。 某同事在抱怨，这个问题很难复现，我不知道怎么解决，要不要把这块整体优化下算了。我问他你优化的目的是什么，是优化目前实现的流程、结构？还是通过优化来解决这个难以复现的问题？答：来解决这个问题。我说你问题都没定位到，怎么通过优化来解决，不怕老问题没解决，优化出新的问题出来了？ 你有没有也曾经说过或听过“这个问题太复杂了， 我解决不了”，“这个功能我没办法实现”，“我也不知道为什么会出现这个问题”之类的话语。 以上的现象与话语，可能都是一个人解决问题的能力或方式方法还不成熟的体现。那么如何来提高解决问题的能力，我想首先需要先从思维方式或思维习惯上寻求改变。在网上看到有这么一篇文章——《How to think like a programmer — lessons in problem solving》（文章地址见文末参考部分），介绍了通过5个步骤来帮忙人们建立高效解决问题的思维框架。本文以这5个步骤为基础，结合自身的理解与体会进行介绍。 “这个国家的每个人都应该学计算机编程，因为它会教你如何思考” 像程序员一样思考像程序员一样思考，到底意味着什么，需要如何来做？ 像程序员一样思考本质上来说，是一种更为有效的解决问题的方法。 解决问题的能力是一项元技能什么叫元技能？类比于元数据——描述数据的数据叫元数据，我理解元技能就是提升技能的技能，就是说当你掌握了解决问题的能力，你就可以通过这种能力去提升其它各项专业技能。 解决问题的能力也是最重要的能力，比精通编程语言，调试能力，以及系统设计能力都更为重要。 提高解决问题能力的方法我们平时解决问题的方式可能是： 尝试一种解决方案。 如果这种解决方案无效，再尝试另一种方案。 如果还是没有用，重复第二步直到你碰巧把问题解决了。 这种方法被作者 Richard Reis 定义为解决问题最糟糕的方式。因为它不但浪费时间，而且能不能达到目的还得看运气。 经过对优秀程序员在编程时的思维框架的分析，作者总结出提高解决问题能力的最好方法包括： 有一个处理问题的框架 按照这个框架反复练习 那么，当你遇到一个新的问题时，该如何来解决？ 第一步：理解遇到问题时，我们应该先要弄明白问题本身。大部分情况下，问题之所以难解决只是因为你没真正理解它们（很多时候是出于沟通的不充分），理解问题是解决问题的第一步。 如何确定自己是否真正理解一个问题？ 最有效的方法是，尝试用自己的语言来说出它，看有没有逻辑漏洞。当你能讲清楚一个问题时，说明你理解了它。优秀的程序员编程时，一般都会写下自己遇到的问题，画出流程或序列草图，或同产品经理、其它开发人员、测试人员等一起讨论确认。这个过程，就是在确定自己对问题的理解有没有偏差。 “如果你不能用简单的语言来解释一个事情，那意味着你根本就没有理解它” —— Richard Feynman 面对一个新需求时，你应该了解这个需求产生的场景——什么人，在什么时候通过执行什么操作，来达到什么目的？这个场景及其中的行为逻辑是否合理，设计是否存在漏洞，然后带着问题来与需求提出方讨论确认，而不是断章取义或不经任何思考直接编码开干。不做代码的搬运工，要做有思想的程序员。 同样，面对一个 Bug 时，你应该首先了解这个 Bug 产生的场景——什么人，在什么场景，通过什么操作会产生这个问题？要追本溯源，定位问题的本源在哪里。我认为定位问题的本源比解决问题更重要！因为你只有正确地找到了问题的症结，才有可能去解决它，而解决办法却可能有多种。且从花费的时间来说，定位问题往往会占整个解决问题时间的一半以上。 如果没有找到问题的本源，只是头痛医头脚痛医脚，那么可能不仅对解决问题无事无补，甚至还可能引进新的问题。常见的头痛医头脚痛医脚的处理方式包括，CPU占用高了，内存溢出了——升级服务器配置（可能过两天又得升级了！）；接口超时了——增大超时时间（可能导致用户投诉或其它依赖的服务级联超时），等等。 那么日常工作中，如何来定位问题的根源？对于一般问题来说，可能通过查看日志大致就能找到问题所在，对于比较棘手的问题，针对问题的性质一般可通过如下方法进行定位： 对于易复现的问题： 常用的就是 Debug，通过 IDE 断点来跟踪数据的流转与变更，一个个环节检查数据输入输出是否正确来进行排查。可借助条件断点、异常断点等技巧来提高 Debug 效率。 对于不易复现的问题：可通过对比法——对比其它地方的类似功能或实现，寻找两者之间的差异，差异之处往往就是问题所在；分析法——走读整体流程代码，捋清各个环节的逻辑，分析定位问题；日志法——在各个关键环节添加日志，将场景镜像下来，当下次复现的时候，通过分析日志定位问题。 第二步：计划理解了问题，接下来就是解决问题的方案。没有明确的方案计划时，不要轻易去着手解决问题，不要寄希望于碰运气蒙混过关。许多开发人员习惯于快速扫一眼需求，就打开 IDE 开始垒代码，垒完发现要么与需求不符，要么漏洞百出。 制定计划，就是制定解决问题的战略步骤。 不论面对需求还是 Bug，都应该好好计划你的解决方案。设计好解决方案中的各个环节，如业务需求的数据表设计、接口设计、流程逻辑，Bug 修复的具体实施步骤。并给自己一点时间思考与预演，该解决方案可能存在的漏洞与影响有哪些，除了这样处理，还有没有另外更好的解决方案。 在没有想清楚解决方案时，不要直接上来就撸代码，暂停一下，给你的大脑一些分析问题和处理信息的时间。 第三步：分解这是思维框架中最重要的一步。 分解，就是化繁为简，就是我们常说的分治思想，拆分法——将大问题拆分为若干个小问题，然后逐个击破各个小问题，再合并总结。微服务架构，MapReduce 算法，都是这一思维（或思想）的体现。 不要尝试一次解决一个复杂的大问题，而应把复杂的大问题分解成若干个简单的小问题（或子问题），从最简单的子问题开始（最简单意味着你知道怎么解决它或它更容易被解决，也或者这个子问题的解决不需要依赖于其它子问题），一个一个逐步解决。一旦你解决了所有的子问题，把它们串联起来，一般就意味着你解决了之前的那个复杂的大问题。 分解问题的能力是解决问题的基石。这也是优秀的程序员在编程中最常用到的技能，对于他们来说，分解问题的能力，要比编程语言的熟练度、系统设计等技术更为重要。 第四步：卡壳了怎么办？当你理解了问题，做出了解决方案的计划，将复杂问题分解为子问题后，在处理子问题时依然卡壳了怎么办？ 首先，淡定！然后告诉自己，这很正常，每个人都会遇到。 优秀程序员或解决问题的高手，与普通人之间的差别就在于，他们对问题更有求知欲，更有耐心，他们的注意力更多地是在如何解决问题上，而不是为此恼火或甩锅发牢骚。 当遇到卡壳的情况时，可以试试这几种方法： Debug：与前面定位问题一样，一步一步调试，直到找出究竟哪里出错了。“Debug 的艺术关键在于你究竟让软件干了些啥，而不是你以为你让软件干了些啥。”—— Andrew Singer 重新评估问题：退回去，从另一个角度重新审视问题，别让自己迷失在细节里，有时候我们容易迷失在具体的细节中而忽略了更一般的原则。重新评估问题的另一种途径是推倒重来，可以删除（回滚）所有已做的事，重新开始，有时这是非常行之有效的方式。 搜索解决方案：利用搜索引擎找到类似问题的解决办法，向他们学习。使用搜索引擎需要学会提炼关键字，关键字越有代表性，越容易找到答案。对搜索结果应该抱着参考的态度，而不是照搬，要明白为什么如此这般处理就能解决问题，并在解决问题后能依次延伸了解其上下游或相关知识，比如SQL查询慢，发现是索引未生效，则可以延伸了解都有哪些场景会导致索引失效；比如并发问题，则可以依此了解如何保证线程安全，同步机制，锁机制等相关知识。事实上，即使问题已经解决，你也可以经常这么做，因为这样你可以从其他人的解决方案中及上下游知识中学到更多。 寻求支援：当通过以上方法都无法获得解决办法时，向你的同事、上级或朋友求援，如果是开源项目，到开源社区、技术群，或 github 的 issue 列表中发帖求援。 记录问题与解决方案：将你本次遇到的问题与最终的解决方案用（电子）笔记本记录下来，便于后面回顾或参考。 第五步：练习罗马不是一天建成的，你也不可能期盼通过解决一两个问题就能成为解决问题的高手。但是，如果你能以学习的态度来寻求问题的解决办法，通过以上四个步骤来建立一套解决问题的思维框架，每一个问题的处理都是提高你能力的机会。那么距离成为一个解决问题的高手，就只差一步了，那就是：练习，练习，再练习。在问题中练习，训练你的思维方式与习惯。 “我不害怕一次练习1000个踢打动作的人，但我害怕将一个踢打动作练习1000次的人” 总结其实，解决问题的能力，不论在IT技术领域，还是在其它各个领域，都是一种最基本的技能。当你在说出“这个问题我解决不了”，“这个问题我没办法定位”前，试试本文介绍的理解、计划、分解、卡壳时怎么处理的建议方法，多一些耐心，一步步实践，说不定慢慢就看到曙光了。按照这个处理模式或习惯，在日积月累的问题处理中，你可能已在不知不觉成为了解决问题的高手。 参考： https://www.freecodecamp.org/news/how-to-think-like-a-programmer-lessons-in-problem-solving-d1d8bf1de7d2/","categories":[{"name":"Career","slug":"Career","permalink":"http://blog.jboost.cn/categories/Career/"}],"tags":[]},{"title":"Kubernetes笔记（八）：K8s中的日志采集实践——使用log-pilot（适配ELK7.x）","slug":"k8s8-logpilot-2","date":"2020-09-26T02:26:20.000Z","updated":"2020-09-26T02:42:48.251Z","comments":true,"path":"k8s8-logpilot-2.html","link":"","permalink":"http://blog.jboost.cn/k8s8-logpilot-2.html","excerpt":"前文Kubernetes笔记（七）：K8s中的日志采集实践——log-pilot介绍我们对 k8s 集群中常用日志采集模式及阿里开源的 log-pilot 进行了介绍，本文介绍如何使 log-pilot 适配 ELK 7.x 及如何将 log-pilot 部署到 k8s 集群中进行日志采集。","text":"前文Kubernetes笔记（七）：K8s中的日志采集实践——log-pilot介绍我们对 k8s 集群中常用日志采集模式及阿里开源的 log-pilot 进行了介绍，本文介绍如何使 log-pilot 适配 ELK 7.x 及如何将 log-pilot 部署到 k8s 集群中进行日志采集。 1. 下载 log-pilot 源码1git clone https://github.com/AliyunContainerService/log-pilot 如果直接使用作者已经调整过的基于 filebeat 7.3.1 的版本或直接使用作者已经构建好的 Docker 镜像，可直接跳到第4步。 2. 升级 filebeat 版本修改 Dockerfile.filebeat 文件，将 123456789ENV FILEBEAT_VERSION=6.1.1-3COPY assets/glibc/glibc-2.26-r0.apk /tmp/RUN apk update &amp;&amp; \\ apk add python &amp;&amp; \\ apk add ca-certificates &amp;&amp; \\ apk add wget &amp;&amp; \\ update-ca-certificates &amp;&amp; \\ wget http://acs-logging.oss-cn-hangzhou.aliyuncs.com/beats/filebeat/filebeat-$&#123;FILEBEAT_VERSION&#125;-linux-x86_64.tar.gz -P /tmp/ &amp;&amp; \\ mkdir -p /etc/filebeat /var/lib/filebeat /var/log/filebeat &amp;&amp; \\ 修改为 123456789ENV FILEBEAT_VERSION=7.3.1COPY assets/glibc/glibc-2.26-r0.apk /tmp/COPY filebeat-$&#123;FILEBEAT_VERSION&#125;-linux-x86_64.tar.gz /tmp/RUN apk update &amp;&amp; \\ apk add python &amp;&amp; \\ apk add ca-certificates &amp;&amp; \\ apk add wget &amp;&amp; \\ update-ca-certificates &amp;&amp; \\ mkdir -p /etc/filebeat /var/lib/filebeat /var/log/filebeat &amp;&amp; \\ 这里先将 filebeat 包下载下来放到 log-pilot 目录下，避免打镜像时下载太慢。 3. 更新 filebeat 配置修改 assets/filebeat/config.filebeat 文件， 移除 filebeat.registry_file: /var/lib/filebeat/registry将 filebeat.config.prospectors: 改为 filebeat.config.inputs: 调整后，配置文件为 123456789101112131415161718192021base() &#123;cat &gt;&gt; $FILEBEAT_CONFIG &lt;&lt; EOFpath.config: &#x2F;etc&#x2F;filebeatpath.logs: &#x2F;var&#x2F;log&#x2F;filebeatpath.data: &#x2F;var&#x2F;lib&#x2F;filebeat&#x2F;datafilebeat.shutdown_timeout: $&#123;FILEBEAT_SHUTDOWN_TIMEOUT:-0&#125;logging.level: $&#123;FILEBEAT_LOG_LEVEL:-info&#125;logging.metrics.enabled: $&#123;FILEBEAT_METRICS_ENABLED:-false&#125;logging.files.rotateeverybytes: $&#123;FILEBEAT_LOG_MAX_SIZE:-104857600&#125;logging.files.keepfiles: $&#123;FILEBEAT_LOG_MAX_FILE:-10&#125;logging.files.permissions: $&#123;FILEBEAT_LOG_PERMISSION:-0600&#125;$&#123;FILEBEAT_MAX_PROCS:+max_procs: $&#123;FILEBEAT_MAX_PROCS&#125;&#125;setup.template.name: &quot;$&#123;FILEBEAT_INDEX:-filebeat&#125;&quot;setup.template.pattern: &quot;$&#123;FILEBEAT_INDEX:-filebeat&#125;-*&quot;filebeat.config.inputs: enabled: true path: \\$&#123;path.config&#125;&#x2F;prospectors.d&#x2F;*.yml reload.enabled: true reload.period: 10sEOF&#125; 4. 获取 Docker 镜像1.如果是自己修改官方源码，则执行 ./build-image.sh 2.如果是下载作者源码，则 1234[root@kmaster]# git clone https://github.com/ronwxy/log-pilot.git[root@kmaster]# cd log-pilot/ [root@kmaster]# git checkout filebeat-7.3.1[root@kmaster]# ./build-image.sh 3.直接下载作者已经构建好的镜像 1[root@kmaster]# docker pull registry.cn-hangzhou.aliyuncs.com/jboost/log-pilot:filebeat-7.3.1 5. 在 k8s 中部署 log-pilot我们以 DaemonSet 的方式（一个 Node 一个 Pod）将 log-pilot 部署在 k8s 中，部署配置文件如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475apiVersion: apps/v1kind: DaemonSetmetadata: name: log-pilot-filebeat namespace: kube-systemspec: selector: matchLabels: app: log-pilot-filebeat template: metadata: labels: app: log-pilot-filebeat spec: containers: - name: log-pilot-filebeat #image: registry.cn-hangzhou.aliyuncs.com/acs/log-pilot:0.9.7-filebeat image: registry.cn-hangzhou.aliyuncs.com/jboost/log-pilot:filebeat-7.3.1 env: - name: \"NODE_NAME\" valueFrom: fieldRef: fieldPath: spec.nodeName - name: \"PILOT_LOG_PREFIX\" value: \"k8s\" - name: \"LOGGING_OUTPUT\" value: \"logstash\" - name: \"LOGSTASH_HOST\" value: \"&#123;your-logstash-host&#125;\" - name: \"LOGSTASH_PORT\" value: \"5044\" volumeMounts: - name: sock mountPath: /var/run/docker.sock - name: root mountPath: /host readOnly: true - name: varlib mountPath: /var/lib/filebeat - name: varlog mountPath: /var/log/filebeat - name: localtime mountPath: /etc/localtime readOnly: true livenessProbe: failureThreshold: 3 exec: command: - /pilot/healthz initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 2 securityContext: capabilities: add: - SYS_ADMIN volumes: - name: sock hostPath: path: /var/run/docker.sock - name: root hostPath: path: / - name: varlib hostPath: path: /var/lib/filebeat type: DirectoryOrCreate - name: varlog hostPath: path: /var/log/filebeat type: DirectoryOrCreate - name: localtime hostPath: path: /etc/localtime 6. 应用容器部署配置在部署应用容器时，以声明式的方式在 Deployment 配置文件的容器部分添加配置即可对容器日志进行自动采集， 如下所示，只列出了与日志配置相关部分 12345678910111213spec: containers: - env: - name: k8s_logs_frameworktest value: &#x2F;mnt&#x2F;logs&#x2F;app*.log volumeMounts: - mountPath: &#x2F;mnt&#x2F;logs name: app-log volumes: - emptyDir: &#123;&#125; name: app-log 7. 按环境与应用建立索引我们可以在 logstash 中根据不同的环境（这里将环境以 namespace 进行划分），及容器名称（即不同的应用）来创建不同的 elasticsearch 的索引。配置参考如下 12345678910111213141516171819output &#123; if [k8s_pod_namespace] &#x3D;&#x3D; &quot;develop&quot; &#123; elasticsearch &#123; hosts &#x3D;&gt; &quot;elasticsearch:9200&quot; index &#x3D;&gt; &quot;dev-%&#123;[k8s_container_name]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; user &#x3D;&gt; &quot;elastic&quot; password &#x3D;&gt; &quot;xxxxxx&quot; &#125; &#125; else &#123; elasticsearch &#123; hosts &#x3D;&gt; &quot;elasticsearch:9200&quot; index &#x3D;&gt; &quot;%&#123;[@metadata][beat]&#125;-%&#123;[@metadata][version]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; user &#x3D;&gt; &quot;elastic&quot; password &#x3D;&gt; &quot;xxxxxx&quot; &#125; &#125;&#125; 8. 相关源码与镜像log-pilot 官方源码地址：https://github.com/AliyunContainerService/log-pilot适配 ELK 7.x 源码地址： https://github.com/ronwxy/log-pilot/tree/filebeat-7.3.1适配 ELK 7.x Docker 镜像地址： registry.cn-hangzhou.aliyuncs.com/jboost/log-pilot:filebeat-7.3.1","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.jboost.cn/categories/Kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.jboost.cn/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"http://blog.jboost.cn/tags/k8s/"}]},{"title":"Kubernetes笔记（七）：K8s中的日志采集实践——log-pilot介绍","slug":"k8s7-logpilot-1","date":"2020-09-25T03:49:52.000Z","updated":"2020-09-26T02:35:58.064Z","comments":true,"path":"k8s7-logpilot-1.html","link":"","permalink":"http://blog.jboost.cn/k8s7-logpilot-1.html","excerpt":"日志是应用服务运行过程中的一个关键环节，借助日志，我们可以排查定位问题，也可以借助集中化的日志管理平台（如ELK）来做一些必要的数据统计分析与监控告警。在 K8s 环境中，容器的日志有可能是通过 STDOUT/STDERR 输出（对于标准输出，前面 Docker笔记（十三）：容器日志采集实践 有相关介绍可参考），并且一般也推荐将日志写到标准输出，但是也有一些特殊的场景，应用直接将日志写在容器内部的日志文件。对于容器的标准输出日志来说，Docker Engine 本身就提供了一个很好的日志采集能力，但是对于容器内部的文件日志采集，现在却并没有一个很好的工具能够去动态发现采集。因为在分布式的容器集群中，容器随着 Pod 调度被动态创建或删除，我们无法像虚拟机环境那样事先配置好日志采集路径等信息，目前的采集工具都是需要我们事先手动配置好日志采集方式和路径等信息，它无法自动感知到容器的生命周期变化或者动态漂移（一个 Pod 挂了，可能是在另一个节点上启动一个新的 Pod），无法进行动态的配置。因此，在 K8s 中进行日志采集将变得更为复杂。","text":"日志是应用服务运行过程中的一个关键环节，借助日志，我们可以排查定位问题，也可以借助集中化的日志管理平台（如ELK）来做一些必要的数据统计分析与监控告警。在 K8s 环境中，容器的日志有可能是通过 STDOUT/STDERR 输出（对于标准输出，前面 Docker笔记（十三）：容器日志采集实践 有相关介绍可参考），并且一般也推荐将日志写到标准输出，但是也有一些特殊的场景，应用直接将日志写在容器内部的日志文件。对于容器的标准输出日志来说，Docker Engine 本身就提供了一个很好的日志采集能力，但是对于容器内部的文件日志采集，现在却并没有一个很好的工具能够去动态发现采集。因为在分布式的容器集群中，容器随着 Pod 调度被动态创建或删除，我们无法像虚拟机环境那样事先配置好日志采集路径等信息，目前的采集工具都是需要我们事先手动配置好日志采集方式和路径等信息，它无法自动感知到容器的生命周期变化或者动态漂移（一个 Pod 挂了，可能是在另一个节点上启动一个新的 Pod），无法进行动态的配置。因此，在 K8s 中进行日志采集将变得更为复杂。 1. K8s 的日志采集模式综观 K8s 下的日志采集模式，大致有三种： 1.Node 代理模式就是在每个 Node 上部署一个日志采集代理程序（如 filebeat，fluentd，或 logstash 等），一般是以 DaemonSet 的形式在每个 Node 上部署一个 Pod，来采集这个 Node 上所有容器的日志。这种模式的优点是资源消耗少，一个节点一个 Pod， 且对应用无侵入。 2.SideCar 模式这种模式就是在 Pod 中除了运行我们的应用程序容器， 再起一个负责日志采集的容器，比如再起一个 logstash 或 fluentd 容器。当 Pod 数量一多，这种方案资源消耗很大，对日志存储后端也会占用过多的连接数，并且日志不输出到标准输出，不能通过 kubectl logs 命令查看。 3.应用程序推送模式直接在应用程序里将日志内容发送到日志采集服务，比如在程序里将日志发到 kafka， 再使用 logstash 从 kafka 拉取到 elasticsearch。这种方案对应用具有侵入性。 有没有一种方案或工具，既能采集 k8s 中标准输出日志，又能采集到容器内部的日志文件输出日志，并且资源消耗小，对应用无侵入呢。拒作者了解，阿里开源的 log-pilot 基本能满足要求，只是更新较慢，目前版本基于 ELK 6，如果要适配到 ELK 7 或以上，需进行一些必要的调整。 2. log-pilot 介绍log-pilot 是阿里开源的一个同时支持容器标准输出日志采集与容器内部文件日志动态配置采集的组件。log-pilot 具备如下特性。 1. 采集目标多log-pilot 同时支持采集标准输出日志和动态发现配置采集容器内部文件日志。 2. 声明式的日志配置log-pilot 支持声明式日志配置，可以依据容器的 Label 或者 ENV 来动态地生成日志采集的配置文件。这里主要说明两个变量： name：我们自定义的一个字符串，它在不同的场景下指代不同的含义。当日志采集到 ElasticSearch 的时候， name 表示的是 Index；当日志采集到 Kafka 的时候， name 表示的是 Topic；当日志采集到阿里云日志服务的时候，name 表示的就是 LogstoreName。 path：支持两种形式，一种是约定关键字 stdout，表示的是采集容器的标准输出日志，第二种是容器内部的具体文件日志路径，可以支持通配符的方式。比如我们要采集 tomcat 容器日志，我们可以通过配置标签 aliyun.logs.catalina=stdout 来采集 tomcat 标准输出日志，通过配置标签 aliyun.logs.access=/usr/local/tomcat/logs/*.log 来采集 tomcat 容器内部文件日志。 3. 动态配置的能力log-pilot 本身分为三部分，其中一部分就是容器的事件管理，它能够动态地监听容器的事件变化（如创建、删除），然后依据容器的标签来进行解析，生成日志采集配置文件，然后交由采集插件来进行日志采集。通过全量扫描加事件监听的方式，比如采集工具进程在起来的时候，注册事件监听，然后全量扫描一遍宿主机上的所有容器列表，然后依据容器的声明式配置来进行日志采集配置文件的动态生成。 4. 防重复和丢失log-pilot 内部具有 CheckPoint 和句柄保持的机制。 checkPoint机制： log-pilot 内部会实时跟踪日志采集的偏移量，然后维持日志文件信息与偏移量的映射关系，最后定期地持久化到磁盘中。采用偏移量的方式我们可以避免日志采集丢失和重复的问题，同时即使当采集工具宕掉再起来，它也可以通过加载持久化在磁盘上的元数据信息，从指定的日志偏移位置上继续采集日志。 句柄保持机制： log-pilot 在监测到配置的日志路径目录下有新的日志文件产生时会主动地打开其句柄，并维持打开状态，这样是为了防止因日志采集工具比较慢或者应用日志输出速率特别大，比如说当前已经生成五个日志文件但只采集到第三个，后面两个还没有开始采集，一旦这个容器退出就可能导致后面两个文件的日志丢失了。 5. 明确日志来源支持日志自动数据打标。log-pilot 在采集容器日志的时候，同时也会收集容器的元数据信息，包括容器的名称，容器所属的服务名称以及容器所属的应用名称，同时在 Kubernetes 里面也会采集容器所属的 Pod 信息，包括 Pod 的名称，Pod 所属的 namespace 以及 Pod 所在的节点信息。这样排查问题时，就可以很方便地知道这个日志是来源于哪个节点上的哪个应用容器。 6. 支持自定义 Taglog-pilot 支持自定义Tag，我们可以在容器的标签或者环境变量里配置 aliyun.logs.$name.tags: k=v，那么在采集日志的时候就会将k=v采集到容器的日志输出中。比如针对不同的环境（如开发环境、测试环境），可以使用 tag 来进行区分。也可以使用自定义 tag 来进行日志的统计、路由与过滤等。 7. 支持多种日志解析格式log-pilot 支持多种日志解析格式，通过 aliyun.logs.$name.format: &lt;format&gt; 标签就可以告诉 log-pilot 在采集日志的时候，同时以什么样的格式来解析日志记录。目前主要支持六种： none：默认格式，指不对日志记录做任何解析，整行采集出来直接输出到日志存储后端。 json：log-pilot 在采集日志的同时会将每一行日志以 json 的方式进行解析，解析出多个 KV 对，然后输出到日志存储后端。 csv：主要是针对 csv 格式的日志采集配置（需配置 fluentd 插件）。 nginx：主要是针对 Nginx 的日志采集配置（需配置 fluentd 插件）。 apache2：主要是针对 Apache 的日志采集配置（需配置 fluentd 插件）。 regexp：用户可以通过 format 标签来自定义正则表达式，告诉 log-pilot 在解析日志记录的时候以什么样的格式来进行解析（需配置 fluentd 插件）。 8. 支持自定义输出Target假设我们同时有一个生产环境和一个测试环境，应用日志都需要被采集到同一套 Kafka 中，然后由不同的 consumer 去消费。但是我们希望对环境进行区分，某条日志是由生产环境的应用容器产生的，还是测试环境的应用容器产生的，但我们在测试环境中的应用容器已经配置了 aliyun.logs.svc=stdout 标签，那么当这些应用容器的标准输出日志被采集到 kafka 中，它最终会被路由到 topic=svc 的消息队列中，那么订阅了 topic=svc 的 consumer 就能够接收测试环境的应用容器产生的日志。但当我们将该应用发布到生产环境时，希望它产生的日志只能交由生产环境的 consumer 来接收处理，那么我们就可以通过 target 的方式，给生产环境的应用容器额外定义一个 target=pro-svc，那么生产环境的应用日志在被采集到 Kafka 中时，最终会被路由到 topic 为 pro-svc 的消息队列中，那么订阅了 topic =pro-svc 的 consumer 就可以正常地接收到来自于生产环境的容器产生的日志。 因此这里的 target 本身也有三种含义： 日志对接到 ElasticeSearch 时，这个 target 字符串是 Index； 对接到 Kafka 时，它指代的是 topic； 对接到阿里云日志服务时，它代表的是 Logstore Name。 9. 支持多种采集插件目前 log-pilot 支持两种采集插件：一个是 CNCF 社区的 Fluentd 插件，一个是 Elastic 的 Filebeat 插件；同时支持对接多种存储后端，目前 Fluentd 和 Filebeat 都支持 Elasticsearch、Kafka、File、Console 作为日志存储后端，而 Fluentd 还支持 Graylog、阿里云日志服务 以及 Mongodb 作为存储后端。 3. 总结本文对 k8s 集群中日志采集的常见模式，及阿里开源的 log-pilot 支持的特性进行了介绍。但 log-pilot 目前基于 ELK 6 版本， 如果需要适配 ELK 7 或以上版本，需要对其进行必要的调整。下文将介绍如何进行适配调整及如何将 log-pilot 部署到 k8s 集群中进行日志采集。 参考： 容器日志采集利器 Log-Pilot： https://developer.aliyun.com/article/674327log-pilot 官方源码地址：https://github.com/AliyunContainerService/log-pilot","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.jboost.cn/categories/Kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.jboost.cn/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"http://blog.jboost.cn/tags/k8s/"}]},{"title":"Kubernetes笔记（六）：了解控制器 —— Deployment","slug":"k8s6-deploy","date":"2020-08-21T03:49:52.000Z","updated":"2020-08-26T01:44:35.532Z","comments":true,"path":"k8s6-deploy.html","link":"","permalink":"http://blog.jboost.cn/k8s6-deploy.html","excerpt":"Pod（容器组）是 Kubernetes 中最小的调度单元，可以通过 yaml 定义文件直接创建一个 Pod。但 Pod 本身并不具备自我恢复（self-healing）功能。如果一个 Pod 所在的节点出现故障，或者调度程序自身出现问题，以及节点资源不够或节点进入维护而驱逐 Pod 时，Pod 将被删除，且不能自我恢复。 因此，Kubernetes 中我们一般不直接创建 Pod， 而是通过 Controller（控制器）来管理 Pod。","text":"Pod（容器组）是 Kubernetes 中最小的调度单元，可以通过 yaml 定义文件直接创建一个 Pod。但 Pod 本身并不具备自我恢复（self-healing）功能。如果一个 Pod 所在的节点出现故障，或者调度程序自身出现问题，以及节点资源不够或节点进入维护而驱逐 Pod 时，Pod 将被删除，且不能自我恢复。 因此，Kubernetes 中我们一般不直接创建 Pod， 而是通过 Controller（控制器）来管理 Pod。 ControllerController 能为 Pod 提供如下特性： 水平扩展，控制 Pod 运行的副本数 rollout，即版本更新 self-healing，即自我恢复。当节点出现故障时，控制器可以自动地在另一个节点调度一个配置完全一样的 Pod，以替换故障节点上的 Pod。 Kubernetes 中支持的控制器包括： ReplicationController：用来维护一个数量稳定的 Pod 副本集合的控制器 ReplicaSet：是 ReplicationController 的升级版，比 ReplicationController 多一个特性：支持基于集合的选择器。 不支持滚动更新（RollingUpdate） Deployment：包含了 ReplicaSet，可通过声明式、滚动更新的方式更新 ReplicaSet 及其 Pod。对于无状态应用，推荐使用 Deployment 部署 StatefulSet：用于管理有状态的应用程序 DaemonSet：在节点上以守护进程的方式运行一个指定的 Pod 副本，例如监控节点、收集节点上的日志时，可使用 DaemonSet CronJob：按照预定的时间计划创建 Job，类似于 linux 的crontab Job：使用 Job 执行任务，执行完后结束 ReplicaSetKubernetes 中，虽然一般使用 Deployment 来管理 Pod， 但 Deployment 中也是通过 ReplicaSet 来维护 Pod 的副本集合的，因此此处也对 ReplicaSet 进行简单介绍。 在 ReplicaSet 的定义中，包含三部分： selector： 标签选择器，用于指定哪些 Pod 归该 ReplicaSet 管理，通过 matchLabels 来与 Pod 的 label 匹配。 replicas： 期望的 Pod 副本数，指定该 ReplicaSet 应该维持多少个 Pod 副本，默认为1。 template： Pod 定义模板，ReplicaSet 使用该模板的定义来创建 Pod。 ReplicaSet 的示例定义文档如下所示， 1234567891011121314151617apiVersion: apps/v1 # api版本kind: ReplicaSet # 资源类型metadata: # 元数据定义 name: nginx-ds # ReplicaSet 名称spec: replicas: 2 # Pod 副本数量，默认1 selector: # 标签选择器 matchLabels: app: nginx template: # Pod 定义模板 metadata: # Pod 元数据定义 labels: app: nginx # Pod 标签 spec: containers: # 容器定义 - name: nginx image: nginx ReplicaSet 通过创建、删除 Pod 容器组来确保符合 selector 选择器的 Pod 数量等于 replicas 指定的数量。 ReplicaSet 创建的 Pod 中，都有一个字段 metadata.ownerReferences 用于标识该 Pod 从属于哪一个 ReplicaSet。可通过 kubectl get pod pod-name -o yaml 来查看 Pod 的 ownerReference。 ReplicaSet 通过 selector 字段的定义，识别哪些 Pod 应该由其管理， 不论该 Pod 是否由该 ReplicaSet 创建，即只要 selector 匹配， 通过外部定义创建的 Pod 也会被该 ReplicaSet 管理。因此需要注意 .spec.selector.matchLabels 与 .spec.template.metadata.labels 的定义一致， 且避免与其他控制器的 selector 重合，造成混乱。 ReplicaSet 不支持滚动更新，所以对于无状态应用，一般使用 Deployment来部署， 而不直接使用 ReplicaSet。ReplicaSet 主要是被用作 Deployment 中负责 Pod 创建、删除、更新的一种手段。 DeploymentDeployment 对象包含 ReplicaSet 作为从属对象，并且可通过声明式、滚动更新的方式来更新 ReplicaSet 及其 Pod。ReplicaSet 现在主要是被用作 Deployment 中负责 Pod 创建、删除、更新的一种手段。使用 Deployment 时，无需关心由 Deployment 创建的 ReplicaSet，Deployment 将处理所有与之相关的细节。同时，Deployment 还能以“声明式”的方式管理 Pod 和 ReplicaSet （其本质是将一些特定场景的一系列运维步骤固化下来，以便快速准确无误的执行），并提供版本（revision）回退功能。 Deployment 定义示例， 1234567891011121314151617181920212223242526apiVersion: apps/v1kind: Deployment # 对象类型，固定为 Deploymentmetadata: name: nginx-deploy # Deployment 名称 namespace: default # 命名空间，默认为 default labels: app: nginx # 标签spec: replicas: 4 # Pod 副本数，默认1 strategy: rollingUpdate: # 升级策略为滚动升级，由于replicas为4,则整个升级过程pod个数在3-5个之间 maxSurge: 1 # 滚动升级时超过 replicas 的最大 pod 数，也可以为百分比（replicas的百分比），默认为1 maxUnavailable: 1 # 滚动升级时不可用的最大 pod 数，也可为百分比（replicas的百分比），默认为1 selector: # 标签选择器，通过标签选择该 Deployment 管理的 Pod matchLabels: app: nginx template: # Pod 定义模板 metadata: labels: app: nginx # Pod 标签 spec: # 定义容器模板，可以包含多个容器 containers: - name: nginx image: nginx:latest ports: - containerPort: 80 可通过 kubectl explain xxx 来查看支持哪些配置选项， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 查看 deployment 配置项[root@kmaster ~]# kubectl explain deployment...# 查看 deployment.spec 模块的配置项[root@kmaster ~]# kubectl explain deployment.specKIND: DeploymentVERSION: apps/v1RESOURCE: spec &lt;Object&gt;DESCRIPTION: Specification of the desired behavior of the Deployment. DeploymentSpec is the specification of the desired behavior of the Deployment.FIELDS: minReadySeconds &lt;integer&gt; Minimum number of seconds for which a newly created pod should be ready without any of its container crashing, for it to be considered available. Defaults to 0 (pod will be considered available as soon as it is ready) paused &lt;boolean&gt; Indicates that the deployment is paused. progressDeadlineSeconds &lt;integer&gt; The maximum time in seconds for a deployment to make progress before it is considered to be failed. The deployment controller will continue to process failed deployments and a condition with a ProgressDeadlineExceeded reason will be surfaced in the deployment status. Note that progress will not be estimated during the time a deployment is paused. Defaults to 600s. replicas &lt;integer&gt; Number of desired pods. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1. revisionHistoryLimit &lt;integer&gt; The number of old ReplicaSets to retain to allow rollback. This is a pointer to distinguish between explicit zero and not specified. Defaults to 10. selector &lt;Object&gt; -required- Label selector for pods. Existing ReplicaSets whose pods are selected by this will be the ones affected by this deployment. It must match the pod template's labels. strategy &lt;Object&gt; The deployment strategy to use to replace existing pods with new ones. template &lt;Object&gt; -required- 其它配置项说明： .spec.minReadySeconds：用来控制应用升级的速度。升级过程中，新创建的 Pod 一旦成功响应了就绪探测即被认为是可用状态，然后进行下一轮的替换。 .spec.minReadySeconds 定义了在新的 Pod 对象创建后至少需要等待多长的时间才能会被认为其就绪，在该段时间内，更新操作会被阻塞。 .spec.progressDeadlineSeconds：用来指定在系统报告 Deployment 失败 —— 表现为状态中的 type=Progressing、Status=False、 Reason=ProgressDeadlineExceeded 前可以等待的 Deployment 进行的秒数。Deployment controller 会继续重试该 Deployment。如果设置该参数，该值必须大于 .spec.minReadySeconds。 .spec.revisionHistoryLimit：用来指定可以保留的旧的 ReplicaSet 或 revision（版本） 的数量。默认所有旧的 Replicaset 都会被保留。如果删除了一个旧的 RepelicaSet，则 Deployment 将无法再回退到那个 revison。如果将该值设置为0，所有具有0个 Pod 副本的 ReplicaSet 都会被删除，这时候 Deployment 将无法回退，因为 revision history 都被清理掉了。 1. 创建1[root@kmaster test]# kubectl apply -f nginx-deploy.yaml --record --record 会将此次命令写入 Deployment 的 kubernetes.io/change-cause 注解中。可在后面查看某一个 Deployment 版本变化的原因。 2. 查看创建 Deployment 后，Deployment 控制器将立刻创建一个 ReplicaSet，并由 ReplicaSet 创建所需要的 Pod。 123456789101112131415# 查看 Deployment[root@kmaster test]# kubectl get deployNAME READY UP-TO-DATE AVAILABLE AGEnginx-deploy 0/2 2 0 64s# 查看 ReplicaSet[root@kmaster test]# kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deploy-59c9f8dff 2 2 1 2m16s# 查看 Pod，显示调度的节点，及标签[root@kmaster test]# kubectl get pod -o wide --show-labelsNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELSnginx-deploy-59c9f8dff-47bgd 1/1 Running 0 5m14s 10.244.1.91 knode2 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=59c9f8dffnginx-deploy-59c9f8dff-q4zb8 1/1 Running 0 5m14s 10.244.3.47 knode3 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=59c9f8dff pod-template-hash 标签是 Deployment 创建 ReplicaSet 时添加到 ReplicaSet 上的，ReplicaSet 进而将此标签添加到 Pod 上。这个标签用于区分 Deployment 中哪个 ReplicaSet 创建了哪些 Pod。该标签的值是 .spec.template 的 hash 值，不要去修改这个标签。由上可看出 ReplicaSet、 Pod 的命名分别遵循 &lt;Deployment-name&gt;-&lt;Pod-template-hash&gt;、&lt;Deployment-name&gt;-&lt;Pod-template-hash&gt;-xxx 的格式。 3. 发布更新（rollout）当且仅当 Deployment 的 Pod template（.spec.template）字段中的内容发生变更时（例如标签或容器的镜像被改变），Deployment 的发布更新（rollout）才会被触发。Deployment 中其他字段的变化（例如修改 .spec.replicas 字段）将不会触发 Deployment 的发布更新。 更新 Deployment 中 Pod 的定义（例如，发布新版本的容器镜像）。此时 Deployment 控制器将为该 Deployment 创建一个新的 ReplicaSet，并且逐步在新的 ReplicaSet 中创建 Pod，在旧的 ReplicaSet 中删除 Pod，以达到滚动更新的效果。 比如我们将上面 Deployment 的容器镜像进行修改， 123456# 方式一：直接使用 kubectl 命令设置修改 [root@kmaster ~]# kubectl set image deploy nginx-deploy nginx=nginx:1.16.1 --recorddeployment.apps/nginx-deploy image updated# 方式二：使用 kubectl edit 编辑yaml修改[root@kmaster ~]# kubectl edit deploy nginx-deploy 查看发布更新（rollout）的状态 12[root@kmaster ~]# kubectl rollout status deploy nginx-deployWaiting for deployment \"nginx-deploy\" rollout to finish: 2 out of 4 new replicas have been updated... 查看 ReplicaSet， 1234[root@kmaster ~]# kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deploy-59c9f8dff 1 1 1 3d6hnginx-deploy-d47dbbb7c 4 4 2 3m41s 我们可以看到 Deployment 的更新是通过创建一个新的4个副本的 ReplicaSet，并同时将旧的 ReplicaSet 的副本数缩容到0个副本来达成的。 因为前面我们将 maxSurge， 与 maxUnavailable 都设置为了1， 因此在更新的过程中，任何时刻两个 ReplicaSet 的 Pod 数至多为5个（4 replicas +1 maxSurge），且可用的 Pod 数至少为3个（4 replicas - 1 maxUnavailable）。 使用 kubectl describe 命令查看 Deployment 的事件部分，如下所示 1234567891011121314[root@kmaster ~]# kubectl describe deploy nginx-deploy...Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 12m deployment-controller Scaled up replica set nginx-deploy-d47dbbb7c to 1 Normal ScalingReplicaSet 12m deployment-controller Scaled down replica set nginx-deploy-59c9f8dff to 3 Normal ScalingReplicaSet 12m deployment-controller Scaled up replica set nginx-deploy-d47dbbb7c to 2 Normal ScalingReplicaSet 10m deployment-controller Scaled down replica set nginx-deploy-59c9f8dff to 2 Normal ScalingReplicaSet 10m deployment-controller Scaled up replica set nginx-deploy-d47dbbb7c to 3 Normal ScalingReplicaSet 8m56s deployment-controller Scaled down replica set nginx-deploy-59c9f8dff to 1 Normal ScalingReplicaSet 8m56s deployment-controller Scaled up replica set nginx-deploy-d47dbbb7c to 4 Normal ScalingReplicaSet 5m55s deployment-controller Scaled down replica set nginx-deploy-59c9f8dff to 0 当更新了 Deployment 的 Pod Template 时，Deployment Controller 会创建一个新的 ReplicaSet (nginx-deploy-d47dbbb7c) ，并将其 scale up 到 1 个副本，同时将旧的 ReplicaSet（nginx-deploy-59c9f8dff） scale down 到3个副本。接下来 Deployment Controller 继续 scale up 新的 ReplicaSet 并 scale down 旧的 ReplicaSet，直到新的 ReplicaSet 拥有 replicas 个数的 Pod， 旧的 ReplicaSet Pod 数缩放到0。这个过程称为 rollout（发布更新）。 通过 .spec.strategy 字段，可以指定更新策略，除了上述使用的 RollingUpdate（滚动更新），另一个可取的值为 Recreate（重新创建）。选择重新创建，Deployment 将先删除原有 ReplicaSet 中的所有 Pod，然后再创建新的 ReplicaSet 和新的 Pod，更新过程中将出现一段应用程序不可用的情况。因此，线上环境一般使用 RollingUpdate。 4. 回滚默认情况下，kubernetes 将保存 Deployment 的所有更新（rollout）历史。可以通过设定 revision history limit（.spec.revisionHistoryLimit 配置项）来指定保存的历史版本数量。 当且仅当 Deployment 的 .spec.template 字段被修改时（例如修改容器的镜像），kubernetes 才为其创建一个 Deployment revision（版本）。Deployment 的其他更新（例如：修改 .spec.replicas 字段）将不会创建新的 Deployment revision（版本）。 查看 Deployment 的 revision， 12345[root@kmaster ~]# kubectl rollout history deploy nginx-deploydeployment.apps/nginx-deployREVISION CHANGE-CAUSE1 kubectl apply --filename=nginx-deploy.yaml --record=true2 kubectl set image deploy nginx-deploy nginx=nginx:1.16.1 --record=true 如果前面更新 Deployment 时没有添加 --record=true，则此处 CHANGE-CAUSE 将为空。 我们通过将镜像修改为一个不存在的版本来模拟一次失败的更新，并回滚到前一个版本的场景， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 1. 修改镜像版本到一个不存在的值[root@kmaster ~]# kubectl set image deploy nginx-deploy nginx=nginx:1.161 --recorddeployment.apps/nginx-deploy image updated# 2. 查看 ReplicaSet[root@kmaster ~]# kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deploy-58f69cfc57 2 2 0 2m7snginx-deploy-59c9f8dff 0 0 0 3d7hnginx-deploy-d47dbbb7c 3 3 3 81m# 3. 查看 Pod 状态[root@kmaster ~]# kubect get podNAME READY STATUS RESTARTS AGEnginx-deploy-58f69cfc57-5968g 0/1 ContainerCreating 0 42snginx-deploy-58f69cfc57-tk7c5 0/1 ErrImagePull 0 42snginx-deploy-d47dbbb7c-2chgx 1/1 Running 0 77mnginx-deploy-d47dbbb7c-8fcb9 1/1 Running 0 80mnginx-deploy-d47dbbb7c-gnwjj 1/1 Running 0 78m# 4. 查看 Deployment 详情[root@kmaster ~]# kubectl describe deploy nginx-deploy...Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 3m57s deployment-controller Scaled up replica set nginx-deploy-58f69cfc57 to 1 Normal ScalingReplicaSet 3m57s deployment-controller Scaled down replica set nginx-deploy-d47dbbb7c to 3 Normal ScalingReplicaSet 3m57s deployment-controller Scaled up replica set nginx-deploy-58f69cfc57 to 2# 5. 查看 Deployment 的历史版本[root@kmaster ~]# kubectl rollout history deploy nginx-deploydeployment.apps/nginx-deploy REVISION CHANGE-CAUSE1 kubectl apply --filename=nginx-deploy.yaml --record=true2 kubectl set image deploy nginx-deploy nginx=nginx:1.16.1 --record=true3 kubectl set image deploy nginx-deploy nginx=nginx:1.161 --record=true# 6. 查看某个版本的详情[root@kmaster ~]# kubectl rollout history deploy nginx-deploy --revision=3deployment.apps/nginx-deploy with revision #3Pod Template: Labels: app=nginx pod-template-hash=58f69cfc57 Annotations: kubernetes.io/change-cause: kubectl set image deploy nginx-deploy nginx=nginx:1.161 --record=true Containers: nginx: Image: nginx:1.161 Port: 80/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;# 7. 回滚到前一个版本[root@kmaster ~]# kubectl rollout undo deploy nginx-deploydeployment.apps/nginx-deploy rolled back# 8. 回滚到指定的版本[root@kmaster ~]# kubectl rollout undo deploy nginx-deploy --to-revision=1deployment.apps/nginx-deploy rolled back# 9. 查看历史版本信息[root@kmaster ~]# kubectl rollout history deploy nginx-deploydeployment.apps/nginx-deploy REVISION CHANGE-CAUSE3 kubectl set image deploy nginx-deploy nginx=nginx:1.161 --record=true4 kubectl set image deploy nginx-deploy nginx=nginx:1.16.1 --record=true5 kubectl apply --filename=nginx-deploy.yaml --record=true 通过 kubectl rollout undo 命令可回滚到上一个版本或指定的版本，上述示例也可看出，回滚到历史版本，会将历史版本的序号设置为最新序号。如前所述，我们可以通过设置 Deployment 的 .spec.revisionHistoryLimit 来指定保留多少个旧的 ReplicaSet（或 revision），超出该数字的将在后台进行垃圾回收。如果该字段被设为 0，Kubernetes 将清理掉该 Deployment 的所有历史版本（revision），此时，将无法对该 Deployment 执行回滚操作了。 5. 伸缩可以通过 kubectl scale 命令或 kubectl edit 修改定义的方式来对 Deployment 进行伸缩，增加或减少 Pod 的副本数， 123456789101112131415161718# 将 Pod 数缩放到2个[root@kmaster ~]# kubectl scale deploy nginx-deploy --replicas=2deployment.apps/nginx-deploy scaled# 查看 Pod[root@kmaster ~]# kubectl get podNAME READY STATUS RESTARTS AGEnginx-deploy-59c9f8dff-7bpjp 1/1 Running 0 9m48snginx-deploy-59c9f8dff-tpxzf 0/1 Terminating 0 8m57snginx-deploy-59c9f8dff-v8fgz 0/1 Terminating 0 10mnginx-deploy-59c9f8dff-w8s9z 1/1 Running 0 10m# 查看 ReplicaSet，DESIRED 变为2了[root@kmaster ~]# kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deploy-58f69cfc57 0 0 0 22mnginx-deploy-59c9f8dff 2 2 2 3d8hnginx-deploy-d47dbbb7c 0 0 0 102m 6. 自动伸缩（HPA）如果集群启用了自动伸缩（HPA —— Horizontal Pod Autoscaling），则可以基于 CPU、 内存的使用率在一个最大和最小的区间对 Deployment 实现自动伸缩， 123456789101112# 创建一个 HPA[root@kmaster ~]# kubectl autoscale deploy nginx-deploy --min=2 --max=4 --cpu-percent=80horizontalpodautoscaler.autoscaling/nginx-deploy autoscaled# 查看 HPA[root@kmaster ~]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEnginx-deploy Deployment/nginx-deploy &lt;unknown&gt;/80% 2 4 2 16s# 删除 HPA[root@kmaster ~]# kubectl delete hpa nginx-deployhorizontalpodautoscaler.autoscaling \"nginx-deploy\" deleted 7. 暂停与恢复我们可以将一个 Deployment 暂停（pause），然后在它上面做一个或多个更新，此时 Deployment 并不会触发更新，只有再恢复（resume）该 Deployment，才会执行该时间段内的所有更新。这种做法可以在暂停和恢复中间对 Deployment 做多次更新，而不会触发不必要的滚动更新。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 1. 暂停 Deployment[root@kmaster ~]# kubectl rollout pause deploy nginx-deploydeployment.apps/nginx-deploy paused# 2. 更新容器镜像[root@kmaster ~]# kubectl set image deploy nginx-deploy nginx=nginx:1.9.1 --recorddeployment.apps/nginx-deploy image updated# 3. 查看版本历史， 此时并没有触发更新[root@kmaster ~]# kubectl rollout history deploy nginx-deploydeployment.apps/nginx-deploy REVISION CHANGE-CAUSE3 kubectl set image deploy nginx-deploy nginx=nginx:1.161 --record=true4 kubectl set image deploy nginx-deploy nginx=nginx:1.16.1 --record=true5 kubectl apply --filename=nginx-deploy.yaml --record=true# 4. 更新 Resource 限制，同样并不会触发更新[root@kmaster ~]# kubectl set resources deploy nginx-deploy -c=nginx --limits=memory=512Mi,cpu=500mdeployment.apps/nginx-deploy resource requirements updated# 5. 查看修改，Pod 定义已被更新[root@kmaster ~]# kubectl describe deploy nginx-deployPod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.9.1 Port: 80/TCP Host Port: 0/TCP Limits: cpu: 500m memory: 512Mi# 6. 恢复 Deployment[root@kmaster ~]# kubectl rollout resume deploy nginx-deploydeployment.apps/nginx-deploy resumed# 7. 查看版本历史，可见两次修改只做了一次 rollout[root@kmaster ~]# kubectl rollout history deploy nginx-deploydeployment.apps/nginx-deployREVISION CHANGE-CAUSE3 kubectl set image deploy nginx-deploy nginx=nginx:1.161 --record=true4 kubectl set image deploy nginx-deploy nginx=nginx:1.16.1 --record=true5 kubectl apply --filename=nginx-deploy.yaml --record=true6 kubectl set image deploy nginx-deploy nginx=nginx:1.9.1 --record=true 在更新容器镜像时，因为 Deployment 处于暂停状态，所以并不会生成新的版本（Revision），当 Deployment 恢复时，才将这段时间的更新生效，执行滚动更新，生成新的版本。在暂停中的 Deployment 上做的更新， 因为没有生成版本，因此也不能回滚（rollback）。也不能对处于暂停状态的 Deployment 执行回滚操作，只有在恢复（Resume）之后才能执行回滚操作。 8. 金丝雀发布金丝雀发布也叫灰度发布。当我们需要发布新版本时，可以针对新版本新建一个 Deployment，与旧版本的 Deployment 同时挂在一个 Service 下（通过 label match）， 通过 Service 的负载均衡将用户请求流量分发到新版 Deployment 的 Pod 上，观察新版运行情况，如果没有问题再将旧版 Deployment 的版本更新到新版完成滚动更新，最后删除新建的 Deployment。很明显这种金丝雀发布具有一定的局限性，无法根据用户或地域来分流，如果要更充分地实现金丝雀发布，则可能需要引入 Istio 等。 金丝雀发布名称的由来： 以前，旷工在下矿洞时面临的一个重要危险是矿井中的毒气，他们想到一个办法来辨别矿井中是否有毒气，矿工们随身携带一只金丝雀下矿井，金丝雀对毒气的抵抗能力比人类要弱，在毒气环境下会先挂掉从而起到预警的作用。它背后的原理是：用较小的代价试错，即使出现了严重的错误（出现了毒气），系统总体的损失也是可承受的或者是非常小的（失去了一只金丝雀）。 总结Kubernetes 中最小的调度单元是 Pod， 负载创建 Pod 并控制其按一定的副本数运行的是 ReplicaSet， 而 Deployment 可以以“声明式”的方式来管理 Pod 和 ReplicaSet，并提供滚动更新与版本（revision）回退功能。所以，一般使用 Deployment 来部署应用， 而不直接操作 ReplicaSet 或 Pod。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.jboost.cn/categories/Kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.jboost.cn/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"http://blog.jboost.cn/tags/k8s/"}]},{"title":"在 Kubernetes Ingress 中支持 Websocket/Socket 服务","slug":"k8s-tcp-service","date":"2020-08-05T07:50:08.000Z","updated":"2020-08-26T01:44:01.286Z","comments":true,"path":"k8s-tcp-service.html","link":"","permalink":"http://blog.jboost.cn/k8s-tcp-service.html","excerpt":"Kubernetes Ingress 可将集群内部的 Service 通过 HTTP/HTTPS 的方式暴露供外部访问，并通过路径匹配规则定义服务的路由。但是 Ingress 对 TCP/UDP 的服务却支持的不那么好。如果我们服务中有使用 Websocket 或 Socket， 需要暴露给外部访问，在 Kubernetes 中该如何配置呢？","text":"Kubernetes Ingress 可将集群内部的 Service 通过 HTTP/HTTPS 的方式暴露供外部访问，并通过路径匹配规则定义服务的路由。但是 Ingress 对 TCP/UDP 的服务却支持的不那么好。如果我们服务中有使用 Websocket 或 Socket， 需要暴露给外部访问，在 Kubernetes 中该如何配置呢？ 大致有两种方式[见参考文档1]： 使用 NodePort， 使用节点 IP 与 NodePort 暴露的端口访问 使用 ClusterIp + Ingress + ConfigMap 使用 NodePort 将端口直接暴露，需要节点有外网 IP，且该方式可能绕过现有的 TLS， 存在安全性的问题。 ClusterIp 只能在集群内部访问，由 Ingress 进行代理对外暴露，但对于 TCP/UDP， Ingress 不支持直接代理， 需要借助 ConfigMap 进行映射。 NodePort 的方式比较简单， 本文介绍 ClusterIp + Ingress + ConfigMap 的方式。 创建 ClusterIp 服务假设有一个 Websocket/Socket 服务，暴露端口 8828， 针对该服务定义 ClusterIp 配置如下（不声明 type， 默认即为 ClusterIp）， my-websocket-svc.yaml12345678910111213apiVersion: v1kind: Servicemetadata: name: my-websocket-svc namespace: developspec: ports: - name: socket port: 8828 targetPort: 8828 protocol: TCP selector: app: my-websocket 创建 ClusterIp， 1[root@kmaster k8s-deploy]# kubectl apply -f my-websocket-svc.yaml 创建 ConfigMap在 ingress-nginx-controller 所在的 namespace 下创建 ConfigMap（如果已经有 ConfigMap 了， 则可在已有 ConfigMap 的 data 部分添加下面配置中的 data 条目） tcp-service-configmap.yaml1234567apiVersion: v1kind: ConfigMapmetadata: name: tcp-services namespace: ingress-nginxdata: 8828: \"develop/my-websocket-svc:8828\" data 部分的格式为： &lt;namespace/service name&gt;:&lt;service port&gt;:[PROXY]:[PROXY]， [PROXY]:[PROXY] 部分为可选。 上述配置表示将宿主机的 8828 端口 映射到 develop namespace 下 my-websocket-svc 服务的 8828 端口上。 创建 ConfigMap， 1[root@kmaster k8s-deploy]# kubectl apply -f tcp-service-configmap.yaml 配置 ingress-nginx-controller修改 ingress-nginx-controller 的配置， 1[root@kmaster ~]# kubectl edit deploy ingress-nginx-controller -n ingress-nginx 在 .spec.template.spec.containers[].args[] 部分添加 --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services （或针对 UDP， --udp-services-configmap=$(POD_NAMESPACE)/udp-services）， 如下图所示 在.spec.template.spec.containers[].ports[] 部分添加 port 映射，如图 经验证，不加该部分 port 映射配置也没问题 保存，应用配置更新，nginx-ingress-controller 将会自动重启 Pod，使配置生效。 验证在 nginx-ingress-controller Pod 所在节点上执行如下命令查看是否监听了 TCP 端口， 如上，8828 端口已被 nginx-ingress 监听。 对于 Websocket 应用， 可使用 wscat 进行调试 123C:\\Users\\Administrator&gt;wscat -c ws://域名:8828Connected (press CTRL+C to quit)&gt; wscat 安装： npm install -g wscat 其它 注意 ConfigMap 的 namesapce 与 nginx-ingress-controller 一致，否则将 --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services 中的 $(POD_NAMESPACE) 改为 ConfigMap 具体的 namesapce 如果将 nginx-ingress-controller 绑定了节点，则重启可能导致失败（因为端口分配冲突），可先删除（kubectl delete deploy ingress-nginx-controller -n ingress-nginx），再新建（kubectl apply -f nginx-ingress.yaml），该操作会影响服务可用性，生产环境需慎重 如果配置后未生效，可通过查看 nginx-ingress-controller Pod 的日志定位原因 kubectl logs ingress-nginx-controller-58fdbbc68d-wqtlr -n ingress-nginx 参考文档： https://www.ibm.com/support/knowledgecenter/en/SSSHTQ/omnibus/helms/all_helms/wip/reference/hlm_expose_probe.html https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/exposing-tcp-udp-services.md","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.jboost.cn/categories/Kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.jboost.cn/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"http://blog.jboost.cn/tags/k8s/"}]},{"title":"Kubernetes笔记（五）：了解Pod（容器组）","slug":"k8s5-pod","date":"2020-07-28T09:17:37.000Z","updated":"2020-08-26T01:44:23.358Z","comments":true,"path":"k8s5-pod.html","link":"","permalink":"http://blog.jboost.cn/k8s5-pod.html","excerpt":"Kubernetes 中， 容器总是以 Pod（容器组）的方式进行调度与运行。因此对 Pod 的理解与掌握是学习 Kubernetes 的基础。","text":"Kubernetes 中， 容器总是以 Pod（容器组）的方式进行调度与运行。因此对 Pod 的理解与掌握是学习 Kubernetes 的基础。 理解 PodPod（容器组）是 Kubernetes 中最小的调度单元，每一个Pod都是某个应用程序的一个运行实例。以前我们的 Web 应用都是以 Tomcat 等 Web 容器进程的形式运行在操作系统中，在 Kubernetes 中，我们需要将 Web 应用打成镜像，以容器的方式运行在 Pod 中。 Kubernetes 不会直接管理容器，而是通过 Pod 来管理。一个Pod包含如下内容： 一个或多个容器， 一般是一个，除非多个容器紧密耦合共享资源才放在一个 Pod 中； 共享的存储资源（如数据卷），一个 Pod 中的容器是可以共享存储空间的； 一个共享的 IP 地址，Pod 中容器之间可以通过 localhost:port 彼此访问； 定义容器该如何运行的选项。 Pod 中的容器可包括两种类型： 工作容器：就是我们通常运行服务进程的容器 初始化容器：完成一些初始化操作的容器，初始化容器在工作容器之前运行，所有的初始化容器成功执行后，才开始启动工作容器 管理 Pod创建 Pod在 Kubernetes 中，我们一般不直接创建 Pod，而是通过控制器来调度管理（Deployment，StatefulSet，DaemonSet 等），这里为了便于了解，先通过 yaml 配置文件的方式定义 Pod 来直接创建 Pod。定义配置文件 pod-test.yaml 如下， 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: pod-test # pod 名称 namespace: default # pod 创建的 namespacespec: containers: # pod 中容器定义 - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80 hostPort: 8081 volumeMounts: - name: workdir mountPath: /usr/share/nginx/html restartPolicy: OnFailure # 重启策略 volumes: # 数据卷定义 - name: workdir hostPath: path: /tmp type: Directory 其中 spec 部分的 containers 定义了该 Pod 中运行的容器，从 containers 的复数形式也可以看出一个 Pod 中是可以运行多个容器的。 执行 kubectl create 或 kubectl apply 命令创建 Pod， 12345[root@kmaster test]# kubectl create -f pod-test.yaml或[root@kmaster test]# kubectl apply -f pod-test.yaml 该 Pod 创建后将会拉取一个最新的 nginx 镜像，运行一个 nginx 容器，并将容器的 80 端口映射到宿主机的 8081 端口。 查看 Pod可使用 kubectl get pods 命令查看当前 namesapce 下的所有 Pod，加 Pod 名称查看具体某个 Pod。 如果需要查看 Pod 调度到了哪个节点，可加 -o wide 选项，如果查看 yaml 文件信息则可加 -o yaml 选项， 如下所示 123456789[root@kmaster test]# kubectl get podsNAME READY STATUS RESTARTS AGEpod-test 1/1 Running 0 116s[root@kmaster test]# kubectl get pods pod-test -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-test 1/1 Running 0 2m19s 10.244.1.42 knode2 &lt;none&gt; &lt;none&gt;[root@kmaster test]# kubectl get pods pod-test -o yaml 如果要查看更多的信息，可使用 kubectl describe 命令， 1[root@kmaster test]# kubectl describe pod pod-test 该命令输出内容如下图， 各部分说明： Status: Pending， 表示 Pod 的整体状态，当前处于 Pending 状态； State: Waiting，Pod 中每个容器都有一个自己的状态 State， 当前容器 nginx 处于 Waiting 状态，Reason: ContainerCreating 表示容器还处于创建中，Ready：False 表明容器还未就绪，还不能对外提供服务； Conditions， 这部分聚合了一些状态，第一个 Initialized：True，表明已经完成了初始化；而第二个 Ready：False，表明 Pod 还未就绪；ContainersReady：False，表明容器还未就绪； PodScheduled：True，表明 Pod 已经被调度到某个具体的节点上了； 3中不同的状态之间的转换都会发生相应的事件，事件类型包括 Normal 与 Warning 两种， 从上图可看到一个 Pulling image 的 Normal 事件，表示当前正在拉取 Pod 中容器的镜像。 当 Pod 在调度或运行中出现问题时，我们都可以使用 kubectl describe 命令来进行排查，通过其中的状态及事件来判断问题产生的可能原因。 进入 Pod 容器通过 kubectl exec 命令可进入 Pod， 类似于 docker exec， 如 123456# 如果 Pod 中只有一个容器[root@kmaster test]# kubectl exec -it pod-test bashroot@pod-test:/## 如果 Pod 中有多个容器kubectl exec -it pod-name -c container-name /bin/bash 如果一个 Pod 中有多个容器，则需要通过 -c 指定进入哪个容器。 更新/删除 PodKubernetes 对 Pod 的更新做了限制，除了更改 Pod 中容器（包括工作容器与初始化容器）的镜像，以及 activeDeadlineSeconds （对 Job 类型的 Pod 定义失败重试的最大时间）， tolerations （Pod 对污点的容忍），修改其它部分将不会产生作用，如我们可以尝试在前面 Pod 定义文档 pod-test.yaml 中将宿主机端口 8081 改为 8082，重新执行 kubectl apply， 将提示如下错误， 12[root@kmaster test]# kubectl apply -f pod-test.yamlThe Pod \"pod-test\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations) 通过 kubectl delete 命令可删除一个 Pod 1[root@kmaster test]# kubectl delete pod pod-test 在 Kubernetes 中，一般不直接创建，更新或删除单个 Pod，而是通过 Kubernetes 的 Controller（控制器）来管理 Pod，包括 ReplicSet（一般也不直接用，推荐Deployment方式）， Deployment，StatefulSet，DaemonSet 等。 控制器提供如下功能： 水平伸缩，控制运行 Pod 指定个数的副本 rollout，即版本更新 故障恢复，当一个节点出现故障，或资源不够，或进入维护中，控制器会自动在另一个合适的节点调度一个一样的 Pod，以保障 Pod 以一定的副本数运行 Pod 状态Pod状态并不是容器的状态，容器的状态一般包括： Waiting： 容器的初始状态，处于 Waiting 状态的容器，表示仍然有对应的操作在执行，例如：拉取镜像、应用 Secrets等Running： 容器处于正常运行的状态Terminated： 容器处于结束运行的状态 而Pod的状态一般包括： Pending： Kubernetes 已经创建并确认该 Pod，可能两种情况： 1. Pod 还未完成调度（例如没有合适的节点）；2. 正在从 docker registry 下载镜像 Running： 该 Pod 已经被绑定到一个节点，并且该 Pod 所有的容器都已经成功创建，其中至少有一个容器正在运行，或者正在启动/重启 Succeeded：Pod 中的所有容器都已经成功终止，并且不会再被重启 Failed：Pod 中的所有容器都已经终止，至少一个容器终止于失败状态：容器的进程退出码不是 0，或者被系统 kill Unknown： 因为某些未知原因，不能确定 Pod 的状态，通常的原因是 master 与 Pod 所在节点之间的通信故障 状态之间的变迁关系如图 Pod 刚开始处于 Pending 的状态，接下来可能会转换到 Running，也可能转换到 Unknown，甚至可能转换到 Failed。然后，当 Running 执行了一段时间之后，它可以转换到类似像 Successded 或者是 Failed。 当出现 Unknown 这个状态时，可能由于一些状态的恢复，它会重新恢复到 Running 或者 Successded 或者是 Failed。 重启策略定义 Pod 或工作负载时，可以指定 restartPolicy，可选的值有： Always：默认值，只要退出就重启 OnFailure：失败退出时（exit code 不为 0）才重启 Never： 永远不重启 restartPolicy 作用于 Pod 中的所有容器。kubelete 将在五分钟内，按照递延的时间间隔（10s, 20s, 40s …）尝试重启已退出的容器，并在十分钟后再次启动这个循环，直到容器成功启动，或者 Pod 被删除。在控制器 Deployment/StatefulSet/DaemonSet 中，只支持 Always 这一个选项，不支持 OnFailure 和 Never 选项。 健康检查提高应用服务的可用性与稳定性，一般可从两个方面来进行： 首先是提高应用的可观测性，如对应用的健康状态，资源的使用情况，应用日志等可进行实时的观测 第二是提高应用的可恢复能力，在应用出现故障时，能通过自动重启等方式进行恢复 Kubernetes 中对 Pod 的健康检查提供了两种方式： Readiness probe，就绪探测，用来判断一个 Pod 是否处于就绪状态，是否能对外提供相应服务了。当Pod处于就绪状态时，负载均衡器才会将流量打到这个 Pod，否则将把流量从这个 Pod 上面摘除。 Liveness probe，存活探测，用来判断一个 Pod 是否处于存活状态，如果一个 Pod 被探测到不处于存活状态，则由上层判断机制来处理，如果上层配置重启策略为 restart always 的话，Pod 就会被重启。 Liveness probe 适用场景是支持那些可以重新拉起的应用，而 Readiness probe 主要应对的是启动之后无法立即对外提供服务的应用。 就绪探测、存活探测目前支持三种不同的探测方式： httpGet，通过发送http Get请求来判断，返回状态码在 200-399之间，认为是探测成功 Exec，通过执行容器中的一个命令来判断服务是否正常，如果命令的退出状态码为 0，表示成功 tcpSocket，通过容器的IP，端口来进行TCP连接检查，如果TCP连接能被正常建立，则认为成功 以 httpGet 为例，示例配置文件如下， 12345678910111213141516171819apiVersion: v1kind: Podmetadata: name: pod-testspec: containers: - # ... 与前同 - name: workdir mountPath: /usr/share/nginx/html livenessProbe: httpGet: path: / port: 80 httpHeaders: # 此处header无意义，仅作示例 - name: purpose value: for-test initialDelaySeconds: 2 periodSeconds: 5 # ... 与前同 删除之前的 Pod， 重新创建，使用 kubectl describe 查看，可看到 Events 部分如下图， Http 存活探测失败，状态码返回 403， 导致容器重启。出现这个错误的原因是前面做目录挂载时将 nginx 的 html 目录挂载到了宿主机的 /tmp 目录， 而 /tmp 目录没有 index.html 文件，导致请求返回403， 在 Pod 调度到的宿主机 /tmp 目录下创建 index.html 文件即可。 1echo '&lt;h1&gt;Hello, K8s!&lt;/h1&gt;' &gt; /tmp/index.html 其它 Exec，tcpSocket 探测的配置示例如下（配置在 containers 元素下）， 123456789101112131415# execlivenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5# tcpSocketlivenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 10 periodSeconds: 10 支持的参数说明： initialDelaySeconds：延迟探测时间，表示 Pod 启动延迟多久后进行一次检查，比如某个应用启动时间如果较长的话，可以设置该值为略大于启动时间； periodSeconds：探测频率，表示探测的时间间隔，正常默认的这个值是 10 秒； timeoutSeconds：超时时间，表示探测的超时时间，当超时时间之内没有检测成功，那会认为失败； successThreshold：健康阈值，表示当这个 Pod 从探测失败到再一次判断探测成功，所需要的阈值次数，默认情况下是 1 次。如果之前探测失败，接下来的一次探测成功了，就会认为这个 Pod 是处在一个正常的状态； failureThreshold： 不健康阈值，与 successThreshold 相对，表示认为探测失败需要重试的次数，默认值是 3。意思是当从一个健康的状态连续探测到 3 次失败，就会认为Pod 的状态处在一个失败的状态。 readinessProbe 配置与 livenessProbe 类似。阿里云上配置就绪检查如图所示： 健康检查的结果分为三种： Success，表示 container 通过了健康检查，也就是 Liveness probe 或 Readiness probe 是正常的一个状态； Failure，表示 container 没有通过健康检查。针对 Readiness probe，service 层就会将没有通过 Readiness probe 的 pod 进行摘除，不再分发请求到该 Pod；针对 Liveness probe，就会将这个 pod 进行重新拉起，或者是删除。 Unknown，表示当前的执行机制没有进行完整的一个执行，可能是因为类似像超时或者像一些脚本没有及时返回，此时 Readiness probe 或 Liveness probe 不做任何操作，会等待下一次的机制来进行检查。 健康检查的一些实践建议： 如果容器中的进程在碰到问题时可以自己 crash，就不需要执行存活探测，因为 kubelet 可以自动的根据 Pod 的 restartPolicy（重启策略）来执行对应的动作； 如果希望在容器的进程无响应后，将容器重启，则指定一个存活探测 livenessProbe，并同时指定 restartPolicy（重启策略）为 Always 或者 OnFailure； 如果希望在 Pod 确实就绪之后才向其分发服务请求，就指定一个就绪检查 readinessProbe； 适当调大 exec 探测的超时阈值，因为在容器里面执行一个 shell 脚本，它的执行时长是非常长的，平时在一台虚机上执行可能 3 秒返回的一个脚本在容器里面可能需要 30 秒。可以适当调大超时阈值，来防止由于容器压力比较大的时候出现偶发的超时； 调整失败判断的次数，3 次的默认值有时候可能不一定是最佳实践，适当调整一下判断的次数也是一个比较好的方式； 使用 tcpSocket 方式进行判断的时候，如果遇到了 TLS 的服务，那可能会造成后边 TLS 里面有很多这种未鉴权的 tcp 连接，这时候需要自己针对业务场景判断这种连接是否会对业务造成影响。 总结本文对 Pod 的概念与基本的管理操作，Pod 的状态变迁机制与重启策略进行了介绍，对 Pod 的健康检查进行了详细的了解。但在 Kubernetes 中，我们一般不直接创建 Pod，而是通过控制器，如Deployment，StatefulSet，DaemonSet， 因为控制器能为我们提供水平伸缩，rollout（版本更新），self-healing（故障恢复）等能力。我们将在接下来的文章了解控制器。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.jboost.cn/categories/Kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.jboost.cn/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"http://blog.jboost.cn/tags/k8s/"}]},{"title":"浏览器缓存常用策略及Vue应用的Nginx缓存设置","slug":"browser-cache","date":"2020-07-22T05:59:42.000Z","updated":"2020-08-26T01:40:38.768Z","comments":true,"path":"browser-cache.html","link":"","permalink":"http://blog.jboost.cn/browser-cache.html","excerpt":"最近一次移动端Vue应用的上线，导致某些用户使用某些功能时出现问题，经主动清空缓存后恢复。有时候清空微信应用的存储空间缓存仍不能解决问题，此时安卓机可借助微信TBS调试工具 http://debugx5.qq.com （微信中打开页面，勾选最下面四个选项清除缓存），但该工具目前只支持安卓手机，苹果机就比较麻烦了。为了找到问题的本质，从根本上避免问题，最近浏览了一些文章，其中有一篇对浏览器缓存的分析及在Nginx中对应的处理策略总结的比较好，这里分享给大家。","text":"最近一次移动端Vue应用的上线，导致某些用户使用某些功能时出现问题，经主动清空缓存后恢复。有时候清空微信应用的存储空间缓存仍不能解决问题，此时安卓机可借助微信TBS调试工具 http://debugx5.qq.com （微信中打开页面，勾选最下面四个选项清除缓存），但该工具目前只支持安卓手机，苹果机就比较麻烦了。为了找到问题的本质，从根本上避免问题，最近浏览了一些文章，其中有一篇对浏览器缓存的分析及在Nginx中对应的处理策略总结的比较好，这里分享给大家。 原文标题：http常用缓存策略及vue-cli单页面应用服务器端(nginx)如何设置缓存 原文地址：http://www.shanhuxueyuan.com/news/detail/125.html 以下为原文。 关于http或者是浏览器缓存策略，我认为可以分为这三种： 不使用缓存 强制使用缓存 协商使用缓存 不使用缓存有时，我们希望浏览器永远都不要使用缓存，全部到服务器拉取数据，此时即为不使用缓存，我们可以在服务端通过Cache-Control为 no-store实现。 服务器端针对上面文件设置了no-store，可以看到在请求的时候，无论怎么刷新，都是返回200，不会显示304，也不会显示“memory cache”或“disk cache”，说明真的都是从服务器重新拉取数据。 比如我们想设置html文件不缓存，可以在域名的解析配置中如下设置，当文件后缀为html或htm时add_header Cache-Control “no-store” 1234567891011121314server &#123;listen 80;server_name yourdomain.com;location &#x2F; &#123; try_files $uri $uri&#x2F; &#x2F;index.html; root &#x2F;yourdir&#x2F;; index index.html index.htm; if ($request_filename ~* .*\\.(?:htm|html)$) &#123; add_header Cache-Control &quot;no-store&quot;; &#x2F;&#x2F;对html文件设置永远不缓存 &#125; &#125;&#125; 这种方式缺点就是每次都要去服务端拉取文件，即使文件没有更新，很明显这样增加了不必要的带宽消耗。 如果文件没有更新，我们就使用缓存，只有更新了才去拉取最新文件，这样多好，这就是协商缓存。 协商缓存协商缓存就是浏览器携带文件缓存标识（如Last-Modified或ETag），向服务器发送请求，由服务器根据文件缓存标识来决定是否使用缓存，如果文件没有更新，则告诉浏览器使用本地缓存，如果文件更新了，则直接返回新文件内容。 可以看出，相比不使用缓存，协商缓存是会大大减少带宽消耗的。 协商缓存生效，返回304 和 Not Modified 协商缓存无效，返回200和请求文件 我们在浏览器调试页面，可以看到有304的，即是使用了协商缓存 服务器返回的header中会有Last-Modified和ETag标识，而浏览器请求header中会包含If-Modified-Since和If-None-Match Last-Modified和If-Modified-Since在 http 1.0 版本中，第一次请求资源时服务器通过 Last-Modified 来设置响应头的缓存标识，并且把资源最后修改的时间作为值填入，然后将资源返回给浏览器。在第二次请求时，浏览器会首先带上 If-Modified-Since 请求头去访问服务器，服务器会将 If-Modified-Since 中携带的时间与资源修改的时间匹配，如果时间不一致，服务器会返回新的资源，并且将 Last-Modified 值更新，作为响应头返回给浏览器。如果时间一致，表示资源没有更新，服务器返回 304 状态码，浏览器拿到响应状态码后从本地缓存数据库中读取缓存资源。 这种方式有2个弊端，第一个就是当服务器中的资源增加了一个字符，后来又把这个字符删掉，本身资源文件并没有发生变化，但修改时间发生了变化。当下次请求过来时，服务器也会把这个本来没有变化的资源重新返回给浏览器；第二个就是修改时间的单位为秒，所以存在1s的间隙，即使更新了，也会认为没有更新。 ETag和If-None-Match在 http 1.1 版本中，服务器通过 Etag 来设置响应头缓存标识。Etag 的值由服务端生成，可以认为是文件内容的hash值。在第一次请求时，服务器会将资源和 Etag 一并返回给浏览器，浏览器将两者缓存到本地缓存数据库。在第二次请求时，浏览器会将 Etag 信息放到 If-None-Match 请求头去访问服务器，服务器收到请求后，会将服务器中的文件标识与浏览器发来的标识进行对比，如果不相同，服务器返回更新的资源和新的 Etag ，如果相同，服务器返回 304 状态码，浏览器读取缓存。 两者对比 首先在精确度上，Etag要优于Last-Modified。Last-Modified的时间单位是秒，如果某个文件在1秒内改变了多次，那么他们的Last-Modified其实并没有体现出来修改，但是Etag每次都会改变确保了精度；如果是负载均衡的服务器，各个服务器生成的Last-Modified也有可能不一致。 第二在性能上，Etag要逊于Last-Modified，毕竟Last-Modified只需要记录时间，而Etag需要服务器通过算法来计算出一个hash值。 第三在优先级上，服务器校验优先考虑Etag 协商缓存服务端配置可以在服务端通过设置Cache-Control为 no-cache或者max-age=0来实现 强制缓存有时我们希望文件强制使用缓存，比如通过vue-cli产生的js和css，文件名上带有hash值，所以如果文件名没有变的时候，我们希望文件永久缓存，这样可以减少网络请求。 强制缓存整体流程比较简单，就是在第一次访问服务器取到数据之后，在过期时间之内不会再去重复请求。实现这个流程的核心就是如何知道当前时间是否超过了过期时间。 强制缓存的过期时间通过第一次访问服务器时返回的响应头获取。在 http 1.0 和 http 1.1 版本中通过不同的响应头字段实现。 在 http 1.0 版本中，强制缓存通过 Expires 响应头来实现。 expires 表示未来资源会过期的时间。也就是说，当发起请求的时间超过了 expires 设定的时间，即表示资源缓存时间到期，会发送请求到服务器重新获取资源。而如果发起请求的时间在 expires 限定的时间之内，浏览器会直接读取本地缓存数据库中的信息（from memory or from disk），两种方式根据浏览器的策略随机获取。 在 http 1.1 版本中，可以设置Cache-Control中的 max-age=xxx ，来表示缓存的资源将在 xxx 秒后过期。一般来说，为了兼容，两个版本的强制缓存都会被实现。 为什么有了Expires，后来又增加了max-age呢，这是因为Expires是一个绝对时间，有可能客户端的时间和服务器不一致，导致缓存不能按照预期进行，而max-age则是个相对时间，比如3600s，自浏览器请求后3600s之内，都使用本地缓存，和客户端的时间没关系。 vue-cli缓存策略由于打包后的js、css和图片，一般名称都带有hash值，名称中的hash变了，自然会拉取新文件，所以我们可以将这类文件设置为强制缓存，只要文件名不变，就一直缓存，比如缓存100天或者一年。 而html文件则不能设为强制缓存，一般html名称是没法带hash值的，所以html如果设置了强制缓存，则永远也没法更新，html不更新，其引用的js、css等名称也不会更新，则整个服务都没有更新，只能让用户清除缓存了。所以针对html文件，我们可以设置协商缓存或者直接不使用缓存，本身html文件都比较小，我是直接使用了不缓存，nginx配置如下。 1234567891011121314151617181920server &#123;listen 80;server_name yourdomain.com;location &#x2F; &#123; try_files $uri $uri&#x2F; &#x2F;index.html; root &#x2F;yourdir&#x2F;; index index.html index.htm; if ($request_filename ~* .*\\.(js|css|woff|png|jpg|jpeg)$) &#123; expires 100d; &#x2F;&#x2F;js、css、图片缓存100天 #add_header Cache-Control &quot;max-age &#x3D; 8640000&quot;; &#x2F;&#x2F;或者设置max-age &#125; if ($request_filename ~* .*\\.(?:htm|html)$) &#123; add_header Cache-Control &quot;no-store&quot;; &#x2F;&#x2F;html不缓存 &#125; &#125;&#125;","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.jboost.cn/categories/DevOps/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://blog.jboost.cn/tags/nginx/"},{"name":"vue","slug":"vue","permalink":"http://blog.jboost.cn/tags/vue/"}]},{"title":"一个轻量级的基于RateLimiter的分布式限流实现","slug":"distributedratelimiter","date":"2020-07-13T03:21:43.000Z","updated":"2020-08-26T01:39:49.568Z","comments":true,"path":"distributedratelimiter.html","link":"","permalink":"http://blog.jboost.cn/distributedratelimiter.html","excerpt":"上篇文章（限流算法与Guava RateLimiter解析）对常用的限流算法及Google Guava基于令牌桶算法的实现RateLimiter进行了介绍。RateLimiter通过线程锁控制同步，只适用于单机应用，在分布式环境下，虽然有像阿里Sentinel的限流开源框架，但对于一些小型应用来说未免过重，但限流的需求在小型项目中也是存在的，比如获取手机验证码的控制，对资源消耗较大操作的访问频率控制等。本文介绍最近写的一个基于RateLimiter，适用于分布式环境下的限流实现，并使用spring-boot-starter的形式发布，比较轻量级且“开箱即用”。","text":"上篇文章（限流算法与Guava RateLimiter解析）对常用的限流算法及Google Guava基于令牌桶算法的实现RateLimiter进行了介绍。RateLimiter通过线程锁控制同步，只适用于单机应用，在分布式环境下，虽然有像阿里Sentinel的限流开源框架，但对于一些小型应用来说未免过重，但限流的需求在小型项目中也是存在的，比如获取手机验证码的控制，对资源消耗较大操作的访问频率控制等。本文介绍最近写的一个基于RateLimiter，适用于分布式环境下的限流实现，并使用spring-boot-starter的形式发布，比较轻量级且“开箱即用”。 本文限流实现包括两种形式： 基于RateLimiter令牌桶算法的限速控制（严格限制访问速度） 基于Lua脚本的限量控制（限制一个时间窗口内的访问量，对访问速度没有严格限制） 限速控制1. 令牌桶模型首先定义令牌桶模型，与RateLimiter中类似，包括几个关键属性与关键方法。其中关键属性定义如下， 123456789101112131415161718192021@Datapublic class RedisPermits &#123; /** * 最大存储令牌数 */ private double maxPermits; /** * 当前存储令牌数 */ private double storedPermits; /** * 添加令牌的时间间隔/毫秒 */ private double intervalMillis; /** * 下次请求可以获取令牌的时间，可以是过去（令牌积累）也可以是将来的时间（令牌预消费） */ private long nextFreeTicketMillis; //... 关键方法定义与RateLimiter也大同小异，方法注释基本已描述各方法用途，不再赘述。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * 构建Redis令牌数据模型 * * @param permitsPerSecond 每秒放入的令牌数 * @param maxBurstSeconds maxPermits由此字段计算，最大存储maxBurstSeconds秒生成的令牌 * @param nextFreeTicketMillis 下次请求可以获取令牌的起始时间，默认当前系统时间 */public RedisPermits(double permitsPerSecond, double maxBurstSeconds, Long nextFreeTicketMillis) &#123; this.maxPermits = permitsPerSecond * maxBurstSeconds; this.storedPermits = maxPermits; this.intervalMillis = TimeUnit.SECONDS.toMillis(1) / permitsPerSecond; this.nextFreeTicketMillis = nextFreeTicketMillis;&#125;/** * 基于当前时间，若当前时间晚于nextFreeTicketMicros，则计算该段时间内可以生成多少令牌，将生成的令牌加入令牌桶中并更新数据 */public void resync(long nowMillis) &#123; if (nowMillis &gt; nextFreeTicketMillis) &#123; double newPermits = (nowMillis - nextFreeTicketMillis) / intervalMillis; storedPermits = Math.min(maxPermits, storedPermits + newPermits); nextFreeTicketMillis = nowMillis; &#125;&#125;/*** 保留指定数量令牌，并返回需要等待的时间*/public long reserveAndGetWaitLength(long nowMillis, int permits) &#123; resync(nowMillis); double storedPermitsToSpend = Math.min(permits, storedPermits); // 可以消耗的令牌数 double freshPermits = permits - storedPermitsToSpend; // 需要等待的令牌数 long waitMillis = (long) (freshPermits * intervalMillis); // 需要等待的时间 nextFreeTicketMillis = LongMath.saturatedAdd(nextFreeTicketMillis, waitMillis); storedPermits -= storedPermitsToSpend; return waitMillis;&#125;/*** 在超时时间内，是否有指定数量的令牌可用*/public boolean canAcquire(long nowMillis, int permits, long timeoutMillis) &#123; return queryEarliestAvailable(nowMillis, permits) &lt;= timeoutMillis;&#125;/** * 指定数量令牌数可用需等待的时间 * * @param permits 需保留的令牌数 * @return 指定数量令牌可用的等待时间，如果为0或负数，表示当前可用 */private long queryEarliestAvailable(long nowMillis, int permits) &#123; resync(nowMillis); double storedPermitsToSpend = Math.min(permits, storedPermits); // 可以消耗的令牌数 double freshPermits = permits - storedPermitsToSpend; // 需要等待的令牌数 long waitMillis = (long) (freshPermits * intervalMillis); // 需要等待的时间 return LongMath.saturatedAdd(nextFreeTicketMillis - nowMillis, waitMillis);&#125; 2. 令牌桶控制类Guava RateLimiter中的控制都在RateLimiter及其子类中（如SmoothBursty），本处涉及到分布式环境下的同步，因此将其解耦，令牌桶模型存储于Redis中，对其同步操作的控制放置在如下控制类，其中同步控制使用到了前面介绍的分布式锁（参考基于Redis分布式锁的正确打开方式） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133@Slf4jpublic class RedisRateLimiter &#123; /** * 获取一个令牌，阻塞一直到获取令牌，返回阻塞等待时间 * * @return time 阻塞等待时间/毫秒 */ public long acquire(String key) throws IllegalArgumentException &#123; return acquire(key, 1); &#125; /** * 获取指定数量的令牌，如果令牌数不够，则一直阻塞，返回阻塞等待的时间 * * @param permits 需要获取的令牌数 * @return time 等待的时间/毫秒 * @throws IllegalArgumentException tokens值不能为负数或零 */ public long acquire(String key, int permits) throws IllegalArgumentException &#123; long millisToWait = reserve(key, permits); log.info(\"acquire &#123;&#125; permits for key[&#123;&#125;], waiting for &#123;&#125;ms\", permits, key, millisToWait); try &#123; Thread.sleep(millisToWait); &#125; catch (InterruptedException e) &#123; log.error(\"Interrupted when trying to acquire &#123;&#125; permits for key[&#123;&#125;]\", permits, key, e); &#125; return millisToWait; &#125; /** * 在指定时间内获取一个令牌，如果获取不到则一直阻塞，直到超时 * * @param timeout 最大等待时间（超时时间），为0则不等待立即返回 * @param unit 时间单元 * @return 获取到令牌则true，否则false * @throws IllegalArgumentException */ public boolean tryAcquire(String key, long timeout, TimeUnit unit) throws IllegalArgumentException &#123; return tryAcquire(key, 1, timeout, unit); &#125; /** * 在指定时间内获取指定数量的令牌，如果在指定时间内获取不到指定数量的令牌，则直接返回false， * 否则阻塞直到能获取到指定数量的令牌 * * @param permits 需要获取的令牌数 * @param timeout 最大等待时间（超时时间） * @param unit 时间单元 * @return 如果在指定时间内能获取到指定令牌数，则true,否则false * @throws IllegalArgumentException tokens为负数或零，抛出异常 */ public boolean tryAcquire(String key, int permits, long timeout, TimeUnit unit) throws IllegalArgumentException &#123; long timeoutMillis = Math.max(unit.toMillis(timeout), 0); checkPermits(permits); long millisToWait; boolean locked = false; try &#123; locked = lock.lock(key + LOCK_KEY_SUFFIX, WebUtil.getRequestId(), 60, 2, TimeUnit.SECONDS); if (locked) &#123; long nowMillis = getNowMillis(); RedisPermits permit = getPermits(key, nowMillis); if (!permit.canAcquire(nowMillis, permits, timeoutMillis)) &#123; return false; &#125; else &#123; millisToWait = permit.reserveAndGetWaitLength(nowMillis, permits); permitsRedisTemplate.opsForValue().set(key, permit, expire, TimeUnit.SECONDS); &#125; &#125; else &#123; return false; //超时获取不到锁，也返回false &#125; &#125; finally &#123; if (locked) &#123; lock.unLock(key + LOCK_KEY_SUFFIX, WebUtil.getRequestId()); &#125; &#125; if (millisToWait &gt; 0) &#123; try &#123; Thread.sleep(millisToWait); &#125; catch (InterruptedException e) &#123; &#125; &#125; return true; &#125; /** * 保留指定的令牌数待用 * * @param permits 需保留的令牌数 * @return time 令牌可用的等待时间 * @throws IllegalArgumentException tokens不能为负数或零 */ private long reserve(String key, int permits) throws IllegalArgumentException &#123; checkPermits(permits); try &#123; lock.lock(key + LOCK_KEY_SUFFIX, WebUtil.getRequestId(), 60, 2, TimeUnit.SECONDS); long nowMillis = getNowMillis(); RedisPermits permit = getPermits(key, nowMillis); long waitMillis = permit.reserveAndGetWaitLength(nowMillis, permits); permitsRedisTemplate.opsForValue().set(key, permit, expire, TimeUnit.SECONDS); return waitMillis; &#125; finally &#123; lock.unLock(key + LOCK_KEY_SUFFIX, WebUtil.getRequestId()); &#125; &#125; /** * 获取令牌桶 * * @return */ private RedisPermits getPermits(String key, long nowMillis) &#123; RedisPermits permit = permitsRedisTemplate.opsForValue().get(key); if (permit == null) &#123; permit = new RedisPermits(permitsPerSecond, maxBurstSeconds, nowMillis); &#125; return permit; &#125; /** * 获取redis服务器时间 */ private long getNowMillis() &#123; String luaScript = \"return redis.call('time')\"; DefaultRedisScript&lt;List&gt; redisScript = new DefaultRedisScript&lt;&gt;(luaScript, List.class); List&lt;String&gt; now = (List&lt;String&gt;)stringRedisTemplate.execute(redisScript, null); return now == null ? System.currentTimeMillis() : Long.valueOf(now.get(0))*1000+Long.valueOf(now.get(1))/1000; &#125; //...&#125; 其中： acquire 是阻塞方法，如果没有可用的令牌，则一直阻塞直到获取到令牌。 tryAcquire 则是非阻塞方法，如果在指定超时时间内获取不到指定数量的令牌，则直接返回false，不阻塞等待。 getNowMillis 获取Redis服务器时间，避免业务服务器时间不一致导致的问题，如果业务服务器能保障时间同步，则可从本地获取提高效率。 3. 令牌桶控制工厂类工厂类负责管理令牌桶控制类，将其缓存在本地，这里使用了Guava中的Cache，一方面避免每次都新建控制类提高效率，另一方面通过控制缓存的最大容量来避免像用户粒度的限流占用过多的内存。 12345678910111213141516171819202122232425262728293031323334353637383940public class RedisRateLimiterFactory &#123; private PermitsRedisTemplate permitsRedisTemplate; private StringRedisTemplate stringRedisTemplate; private DistributedLock distributedLock; private Cache&lt;String, RedisRateLimiter&gt; cache = CacheBuilder.newBuilder() .initialCapacity(100) //初始大小 .maximumSize(10000) // 缓存的最大容量 .expireAfterAccess(5, TimeUnit.MINUTES) // 缓存在最后一次访问多久之后失效 .concurrencyLevel(Runtime.getRuntime().availableProcessors()) // 设置并发级别 .build(); public RedisRateLimiterFactory(PermitsRedisTemplate permitsRedisTemplate, StringRedisTemplate stringRedisTemplate, DistributedLock distributedLock) &#123; this.permitsRedisTemplate = permitsRedisTemplate; this.stringRedisTemplate = stringRedisTemplate; this.distributedLock = distributedLock; &#125; /** * 创建RateLimiter * * @param key RedisRateLimiter本地缓存key * @param permitsPerSecond 每秒放入的令牌数 * @param maxBurstSeconds 最大存储maxBurstSeconds秒生成的令牌 * @param expire 该令牌桶的redis tty/秒 * @return RateLimiter */ public RedisRateLimiter build(String key, double permitsPerSecond, double maxBurstSeconds, int expire) &#123; if (cache.getIfPresent(key) == null) &#123; synchronized (this) &#123; if (cache.getIfPresent(key) == null) &#123; cache.put(key, new RedisRateLimiter(permitsRedisTemplate, stringRedisTemplate, distributedLock, permitsPerSecond, maxBurstSeconds, expire)); &#125; &#125; &#125; return cache.getIfPresent(key); &#125;&#125; 4. 注解支持定义注解 @RateLimit 如下，表示以每秒rate的速率放置令牌，最多保留burst秒的令牌，取令牌的超时时间为timeout，limitType用于控制key类型，目前支持： IP, 根据客户端IP限流 USER, 根据用户限流，对于Spring Security可从SecurityContextHolder中获取当前用户信息，如userId METHOD, 根据方法名全局限流，className.methodName，注意避免同时对同一个类中的同名方法做限流控制，否则需要修改获取key的逻辑 CUSTOM，自定义，支持表达式解析，如#{id}, #{user.id} 1234567891011@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface RateLimit &#123; String key() default \"\"; String prefix() default \"rateLimit:\"; //key前缀 int expire() default 60; // 表示令牌桶模型RedisPermits redis key的过期时间/秒 double rate() default 1.0; // permitsPerSecond值 double burst() default 1.0; // maxBurstSeconds值 int timeout() default 0; // 超时时间/秒 LimitType limitType() default LimitType.METHOD;&#125; 通过切面的前置增强来为添加了 @RateLimit 注解的方法提供限流控制，如下 123456789101112131415@Aspect@Slf4jpublic class RedisLimitAspect &#123; //... @Before(value = \"@annotation(rateLimit)\") public void rateLimit(JoinPoint point, RateLimit rateLimit) throws Throwable &#123; String key = getKey(point, rateLimit.limitType(), rateLimit.key(), rateLimit.prefix()); RedisRateLimiter redisRateLimiter = redisRateLimiterFactory.build(key, rateLimit.rate(), rateLimit.burst(), rateLimit.expire()); if(!redisRateLimiter.tryAcquire(key, rateLimit.timeout(), TimeUnit.SECONDS))&#123; ExceptionUtil.rethrowClientSideException(LIMIT_MESSAGE); &#125; &#125; //... 限量控制1. 限量控制类限制一个时间窗口内的访问量，可使用计数器算法，借助Lua脚本执行的原子性来实现。 Lua脚本逻辑： 以需要控制的对象为key（如方法，用户ID，或IP等），当前访问次数为Value，时间窗口值为缓存的过期时间 如果key存在则将其增1，判断当前值是否大于访问量限制值，如果大于则返回0，表示该时间窗口内已达访问量上限，如果小于则返回1表示允许访问 如果key不存在，则将其初始化为1，并设置过期时间，返回1表示允许访问 123456789101112131415161718192021222324252627282930313233public class RedisCountLimiter &#123; private StringRedisTemplate stringRedisTemplate; private static final String LUA_SCRIPT = \"local c \\nc = redis.call('get',KEYS[1]) \\nif c and redis.call('incr',KEYS[1]) &gt; tonumber(ARGV[1]) then return 0 end\" + \" \\nif c then return 1 else \\nredis.call('set', KEYS[1], 1) \\nredis.call('expire', KEYS[1], tonumber(ARGV[2])) \\nreturn 1 end\"; private static final int SUCCESS_RESULT = 1; private static final int FAIL_RESULT = 0; public RedisCountLimiter(StringRedisTemplate stringRedisTemplate) &#123; this.stringRedisTemplate = stringRedisTemplate; &#125; /** * 是否允许访问 * * @param key redis key * @param limit 限制次数 * @param expire 时间段/秒 * @return 获取成功true，否则false * @throws IllegalArgumentException */ public boolean tryAcquire(String key, int limit, int expire) throws IllegalArgumentException &#123; RedisScript&lt;Number&gt; redisScript = new DefaultRedisScript&lt;&gt;(LUA_SCRIPT, Number.class); Number result = stringRedisTemplate.execute(redisScript, Collections.singletonList(key), String.valueOf(limit), String.valueOf(expire)); if(result != null &amp;&amp; result.intValue() == SUCCESS_RESULT) &#123; return true; &#125; return false; &#125;&#125; 2. 注解支持定义注解 @CountLimit 如下，表示在period时间窗口内，最多允许访问limit次，limitType用于控制key类型，取值与 @RateLimit 同。 123456789@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface CountLimit &#123; String key() default \"\"; String prefix() default \"countLimit:\"; //key前缀 int limit() default 1; // expire时间段内限制访问次数 int period() default 1; // 表示时间段/秒 LimitType limitType() default LimitType.METHOD;&#125; 同样采用前值增强来为添加了 @CountLimit 注解的方法提供限流控制，如下 1234567@Before(value = \"@annotation(countLimit)\")public void countLimit(JoinPoint point, CountLimit countLimit) throws Throwable &#123; String key = getKey(point, countLimit.limitType(), countLimit.key(), countLimit.prefix()); if (!redisCountLimiter.tryAcquire(key, countLimit.limit(), countLimit.period())) &#123; ExceptionUtil.rethrowClientSideException(LIMIT_MESSAGE); &#125;&#125; 使用示例1.添加依赖 123456789101112131415&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;cn.jboost.springboot&lt;/groupId&gt; &lt;artifactId&gt;limiter-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.配置redis相关参数 1234567891011spring: application: name: limiter-demo redis: #数据库索引 database: 0 host: 192.168.40.92 port: 6379 password: password #连接超时时间 timeout: 2000 3.测试类 12345678910111213141516171819202122232425262728293031323334353637383940414243@RestController@RequestMapping(\"limiter\")public class LimiterController &#123; /** * 注解形式 * @param key * @return */ @GetMapping(\"/count\") @CountLimit(key = \"#&#123;key&#125;\", limit = 2, period = 10, limitType = LimitType.CUSTOM) public String testCountLimit(@RequestParam(\"key\") String key)&#123; return \"test count limiter...\"; &#125; /** * 注解形式 * @param key * @return */ @GetMapping(\"/rate\") @RateLimit(rate = 1.0/5, burst = 5.0, expire = 120, timeout = 0) public String testRateLimit(@RequestParam(\"key\") String key)&#123; return \"test rate limiter...\"; &#125; @Autowired private RedisRateLimiterFactory redisRateLimiterFactory; /** * 代码段形式 * @param * @return */ @GetMapping(\"/rate2\") public String testRateLimit()&#123; RedisRateLimiter limiter = redisRateLimiterFactory.build(\"LimiterController.testRateLimit\", 1.0/30, 30, 120); if(!limiter.tryAcquire(\"app.limiter\", 0, TimeUnit.SECONDS)) &#123; System.out.println(LocalDateTime.now()); ExceptionUtil.rethrowClientSideException(\"您的访问过于频繁，请稍后重试\"); &#125; return \"test rate limiter 2...\"; &#125;&#125; 4.验证 启动测试项目，浏览器中访问 http://localhost:8080/limiter/rate?key=test ，第一次访问成功，如图 持续刷新，将返回如下错误，直到5s之后再返回成功，限制5秒1次的访问速度 注解的使用 限流类型LimitType支持IP（客户端IP）、用户（userId）、方法（className.methodName）、自定义（CUSTOM）几种形式，默认为METHOD LimitType为CUSTOM时，需要手动指定key（其它key自动为ip，userid，或methodname），key支持表达式形式，如#{id}, #{user.id} 针对某个时间窗口内限制访问一次的场景，既可以使用 @CountLimit， 也可以使用 @RateLimit，比如验证码一分钟内只允许获取一次，以下两种形式都能达到目的 1234//同一个手机号码60s内最多访问一次@CountLimit(key = \"#&#123;params.phone&#125;\", limit = 1, period = 60, limitType = LimitType.CUSTOM)//以1/60的速度放置令牌，最多保存60s的令牌（也就是最多保存一个），控制访问速度为1/60个每秒（1个每分钟）@RateLimit(key = \"#&#123;params.phone&#125;\", rate = 1.0/60, burst = 60, expire = 120, limitType = LimitType.CUSTOM) 总结本文介绍了适用于分布式环境的基于RateLimiter令牌桶算法的限速控制与基于计数器算法的限量控制，可应用于中小型项目中有相关需求的场景（注：本实现未做压力测试，如果用户并发量较大需验证效果）。 本文完整代码见：https://github.com/ronwxy/base-spring-boot ，目录 spring-boot-autoconfigure/src/main/java/cn/jboost/springboot/autoconfig/limiter 示例项目代码地址：https://github.com/ronwxy/springboot-demos/tree/master/springboot-limiter 如果觉得有帮助，别忘了给个star ^_^。作者公众号：半路雨歌，欢迎关注查看更多干货文章。","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.jboost.cn/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"redis","slug":"redis","permalink":"http://blog.jboost.cn/tags/redis/"},{"name":"RateLimiter","slug":"RateLimiter","permalink":"http://blog.jboost.cn/tags/RateLimiter/"}]},{"title":"限流算法与Guava RateLimiter解析","slug":"ratelimiter","date":"2020-07-08T03:06:17.000Z","updated":"2020-08-26T01:36:12.561Z","comments":true,"path":"ratelimiter.html","link":"","permalink":"http://blog.jboost.cn/ratelimiter.html","excerpt":"在分布式系统中，应对高并发访问时，缓存、限流、降级是保护系统正常运行的常用方法。当请求量突发暴涨时，如果不加以限制访问，则可能导致整个系统崩溃，服务不可用。同时有一些业务场景，比如短信验证码，或者其它第三方API调用，也需要提供必要的访问限制支持。还有一些资源消耗过大的请求，比如数据导出等（参考 记一次线上Java服务CPU 100%处理过程 ），也有限制访问频率的需求。","text":"在分布式系统中，应对高并发访问时，缓存、限流、降级是保护系统正常运行的常用方法。当请求量突发暴涨时，如果不加以限制访问，则可能导致整个系统崩溃，服务不可用。同时有一些业务场景，比如短信验证码，或者其它第三方API调用，也需要提供必要的访问限制支持。还有一些资源消耗过大的请求，比如数据导出等（参考 记一次线上Java服务CPU 100%处理过程 ），也有限制访问频率的需求。 常见的限流算法有令牌桶算法，漏桶算法，与计数器算法。本文主要对三个算法的基本原理及Google Guava包中令牌桶算法的实现RateLimiter进行介绍，下一篇文章介绍最近写的一个以RateLimiter为参考的分布式限流实现及计数器限流实现。 令牌桶算法令牌桶算法的原理就是以一个恒定的速度往桶里放入令牌，每一个请求的处理都需要从桶里先获取一个令牌，当桶里没有令牌时，则请求不会被处理，要么排队等待，要么降级处理，要么直接拒绝服务。当桶里令牌满时，新添加的令牌会被丢弃或拒绝。 令牌桶算法的处理示意图如下（图片来自网络） 令牌桶算法主要是可以控制请求的平均处理速率，它允许预消费，即可以提前消费令牌，以应对突发请求，但是后面的请求需要为预消费买单（等待更长的时间），以满足请求处理的平均速率是一定的。 漏桶算法漏桶算法的原理是水（请求）先进入漏桶中，漏桶以一定的速度出水（处理请求），当水流入速度大于流出速度导致水在桶内逐渐堆积直到桶满时，水会溢出（请求被拒绝）。 漏桶算法的处理示意图如下（图片来自网络） 漏桶算法主要是控制请求的处理速率，平滑网络上的突发流量，请求可以以任意速度进入漏桶中，但请求的处理则以恒定的速度进行。 计数器算法计数器算法是限流算法中最简单的一种算法，限制在一个时间窗口内，至多处理多少个请求。比如每分钟最多处理10个请求，则从第一个请求进来的时间为起点，60s的时间窗口内只允许最多处理10个请求。下一个时间窗口又以前一时间窗口过后第一个请求进来的时间为起点。常见的比如一分钟内只能获取一次短信验证码的功能可以通过计数器算法来实现。 Guava RateLimiter解析Guava是Google开源的一个工具包，其中的RateLimiter是实现了令牌桶算法的一个限流工具类。在pom.xml中添加guava依赖，即可使用RateLimiter 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;29.0-jre&lt;/version&gt;&lt;/dependency&gt; 如下测试代码示例了RateLimiter的用法, 1234567public static void main(String[] args) &#123; RateLimiter rateLimiter = RateLimiter.create(1); //创建一个每秒产生一个令牌的令牌桶 for(int i=1;i&lt;=5;i++) &#123; double waitTime = rateLimiter.acquire(i); //一次获取i个令牌 System.out.println(\"acquire:\" + i + \" waitTime:\" + waitTime); &#125;&#125; 运行后，输出如下， 12345acquire:1 waitTime:0.0acquire:2 waitTime:0.997729acquire:3 waitTime:1.998076acquire:4 waitTime:3.000303acquire:5 waitTime:4.000223 第一次获取一个令牌时，等待0s立即可获取到（这里之所以不需要等待是因为令牌桶的预消费特性），第二次获取两个令牌，等待时间1s，这个1s就是前面获取一个令牌时因为预消费没有等待延到这次来等待的时间，这次获取两个又是预消费，所以下一次获取（取3个时）就要等待这次预消费需要的2s了，依此类推。可见预消费不需要等待的时间都由下一次来买单，以保障一定的平均处理速率（上例为1s一次）。 RateLimiter有两种实现： SmoothBursty： 令牌的生成速度恒定。使用 RateLimiter.create(double permitsPerSecond) 创建的是 SmoothBursty 实例。 SmoothWarmingUp：令牌的生成速度持续提升，直到达到一个稳定的值。WarmingUp，顾名思义就是有一个热身的过程。使用 RateLimiter.create(double permitsPerSecond, long warmupPeriod, TimeUnit unit) 时创建就是 SmoothWarmingUp 实例，其中 warmupPeriod 就是热身达到稳定速度的时间。 类结构如下 关键属性及方法解析（以 SmoothBursty 为例） 1.关键属性 1234567891011121314/** 桶中当前拥有的令牌数. */double storedPermits;/** 桶中最多可以保存多少秒存入的令牌数 */double maxBurstSeconds;/** 桶中能存储的最大令牌数，等于storedPermits*maxBurstSeconds. */double maxPermits;/** 放入令牌的时间间隔*/double stableIntervalMicros;/** 下次可获取令牌的时间点，可以是过去也可以是将来的时间点*/private long nextFreeTicketMicros = 0L; 2.关键方法 调用 RateLimiter.create(double permitsPerSecond) 方法时，创建的是 SmoothBursty 实例，默认设置 maxBurstSeconds 为1s。SleepingStopwatch 是guava中的一个时钟类实现。 1234567891011@VisibleForTestingstatic RateLimiter create(double permitsPerSecond, SleepingStopwatch stopwatch) &#123; RateLimiter rateLimiter = new SmoothBursty(stopwatch, 1.0 /* maxBurstSeconds */); rateLimiter.setRate(permitsPerSecond); return rateLimiter;&#125;SmoothBursty(SleepingStopwatch stopwatch, double maxBurstSeconds) &#123; super(stopwatch); this.maxBurstSeconds = maxBurstSeconds;&#125; 并通过调用 SmoothBursty.doSetRate(double, long) 方法进行初始化，该方法中: 调用 resync(nowMicros) 对 storedPermits 与 nextFreeTicketMicros 进行了调整——如果当前时间晚于 nextFreeTicketMicros，则计算这段时间内产生的令牌数，累加到 storedPermits 上，并更新下次可获取令牌时间 nextFreeTicketMicros 为当前时间。 计算 stableIntervalMicros 的值，1/permitsPerSecond。 调用 doSetRate(double, double) 方法计算 maxPermits 值（maxBurstSeconds*permitsPerSecond），并根据旧的 maxPermits 值对 storedPermits 进行调整。 源码如下所示 1234567891011121314151617181920212223242526272829303132@Overridefinal void doSetRate(double permitsPerSecond, long nowMicros) &#123; resync(nowMicros); double stableIntervalMicros = SECONDS.toMicros(1L) / permitsPerSecond; this.stableIntervalMicros = stableIntervalMicros; doSetRate(permitsPerSecond, stableIntervalMicros);&#125;/** Updates &#123;@code storedPermits&#125; and &#123;@code nextFreeTicketMicros&#125; based on the current time. */void resync(long nowMicros) &#123; // if nextFreeTicket is in the past, resync to now if (nowMicros &gt; nextFreeTicketMicros) &#123; double newPermits = (nowMicros - nextFreeTicketMicros) / coolDownIntervalMicros(); storedPermits = min(maxPermits, storedPermits + newPermits); nextFreeTicketMicros = nowMicros; &#125;&#125;@Overridevoid doSetRate(double permitsPerSecond, double stableIntervalMicros) &#123; double oldMaxPermits = this.maxPermits; maxPermits = maxBurstSeconds * permitsPerSecond; if (oldMaxPermits == Double.POSITIVE_INFINITY) &#123; // if we don't special-case this, we would get storedPermits == NaN, below storedPermits = maxPermits; &#125; else &#123; storedPermits = (oldMaxPermits == 0.0) ? 0.0 // initial state : storedPermits * maxPermits / oldMaxPermits; &#125;&#125; 调用 acquire(int) 方法获取指定数量的令牌时， 调用 reserve(int) 方法，该方法最终调用 reserveEarliestAvailable(int, long) 来更新下次可取令牌时间点与当前存储的令牌数，并返回本次可取令牌的时间点，根据该时间点计算需要等待的时间 阻塞等待1中返回的等待时间 返回等待的时间（秒） 源码如下所示 123456789101112131415161718192021222324252627282930313233343536/** 获取指定数量（permits）的令牌，阻塞直到获取到令牌，返回等待的时间*/@CanIgnoreReturnValuepublic double acquire(int permits) &#123; long microsToWait = reserve(permits); stopwatch.sleepMicrosUninterruptibly(microsToWait); return 1.0 * microsToWait / SECONDS.toMicros(1L);&#125;final long reserve(int permits) &#123; checkPermits(permits); synchronized (mutex()) &#123; return reserveAndGetWaitLength(permits, stopwatch.readMicros()); &#125;&#125;/** 返回需要等待的时间*/final long reserveAndGetWaitLength(int permits, long nowMicros) &#123; long momentAvailable = reserveEarliestAvailable(permits, nowMicros); return max(momentAvailable - nowMicros, 0);&#125;/** 针对此次需要获取的令牌数更新下次可取令牌时间点与存储的令牌数，返回本次可取令牌的时间点*/@Overridefinal long reserveEarliestAvailable(int requiredPermits, long nowMicros) &#123; resync(nowMicros); // 更新当前数据 long returnValue = nextFreeTicketMicros; double storedPermitsToSpend = min(requiredPermits, this.storedPermits); // 本次可消费的令牌数 double freshPermits = requiredPermits - storedPermitsToSpend; // 需要新增的令牌数 long waitMicros = storedPermitsToWaitTime(this.storedPermits, storedPermitsToSpend) + (long) (freshPermits * stableIntervalMicros); // 需要等待的时间 this.nextFreeTicketMicros = LongMath.saturatedAdd(nextFreeTicketMicros, waitMicros); // 更新下次可取令牌的时间点 this.storedPermits -= storedPermitsToSpend; // 更新当前存储的令牌数 return returnValue;&#125; acquire(int) 方法是获取不到令牌时一直阻塞，直到获取到令牌，tryAcquire(int,long,TimeUnit) 方法则是在指定超时时间内尝试获取令牌，如果获取到或超时时间到则返回是否获取成功 先判断是否能在指定超时时间内获取到令牌，通过 nextFreeTicketMicros &lt;= timeoutMicros + nowMicros 是否为true来判断，即可取令牌时间早于当前时间加超时时间则可取（预消费的特性），否则不可获取。 如果不可获取，立即返回false。 如果可获取，则调用 reserveAndGetWaitLength(permits, nowMicros) 来更新下次可取令牌时间点与当前存储的令牌数，返回等待时间（逻辑与前面相同），并阻塞等待相应的时间，返回true。 源码如下所示 123456789101112131415161718192021222324public boolean tryAcquire(int permits, long timeout, TimeUnit unit) &#123; long timeoutMicros = max(unit.toMicros(timeout), 0); checkPermits(permits); long microsToWait; synchronized (mutex()) &#123; long nowMicros = stopwatch.readMicros(); if (!canAcquire(nowMicros, timeoutMicros)) &#123; //判断是否能在超时时间内获取指定数量的令牌 return false; &#125; else &#123; microsToWait = reserveAndGetWaitLength(permits, nowMicros); &#125; &#125; stopwatch.sleepMicrosUninterruptibly(microsToWait); return true;&#125;private boolean canAcquire(long nowMicros, long timeoutMicros) &#123; return queryEarliestAvailable(nowMicros) - timeoutMicros &lt;= nowMicros; //只要可取时间小于当前时间+超时时间，则可获取（可预消费的特性！）&#125;@Overridefinal long queryEarliestAvailable(long nowMicros) &#123; return nextFreeTicketMicros;&#125; 以上就是 SmoothBursty 实现的基本处理流程。注意两点： RateLimiter 通过限制后面请求的等待时间，来支持一定程度的突发请求——预消费的特性。 RateLimiter 令牌桶的实现并不是起一个线程不断往桶里放令牌，而是以一种延迟计算的方式（参考resync函数），在每次获取令牌之前计算该段时间内可以产生多少令牌，将产生的令牌加入令牌桶中并更新数据来实现，比起一个线程来不断往桶里放令牌高效得多。（想想如果需要针对每个用户限制某个接口的访问，则针对每个用户都得创建一个RateLimiter，并起一个线程来控制令牌存放的话，如果在线用户数有几十上百万，起线程来控制是一件多么恐怖的事情） 总结本文介绍了限流的三种基本算法，其中令牌桶算法与漏桶算法主要用来限制请求处理的速度，可将其归为限速，计数器算法则是用来限制一个时间窗口内请求处理的数量，可将其归为限量（对速度不限制）。Guava 的 RateLimiter 是令牌桶算法的一种实现，但 RateLimiter 只适用于单机应用，在分布式环境下就不适用了。虽然已有一些开源项目可用于分布式环境下的限流管理，如阿里的Sentinel，但对于小型项目来说，引入Sentinel可能显得有点过重，但限流的需求在小型项目中也是存在的，下一篇文章就介绍下基于 RateLimiter 的分布式下的限流实现。","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.jboost.cn/categories/Java/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://blog.jboost.cn/tags/redis/"},{"name":"限流","slug":"限流","permalink":"http://blog.jboost.cn/tags/%E9%99%90%E6%B5%81/"}]},{"title":"基于Redis分布式锁的正确打开方式","slug":"distributedlock","date":"2020-07-01T06:18:15.000Z","updated":"2020-08-26T01:39:56.844Z","comments":true,"path":"distributedlock.html","link":"","permalink":"http://blog.jboost.cn/distributedlock.html","excerpt":"分布式锁是在分布式环境下（多个JVM进程）控制多个客户端对某一资源的同步访问的一种实现，与之相对应的是线程锁，线程锁控制的是同一个JVM进程内多个线程之间的同步。分布式锁的一般实现方法是在应用服务器之外通过一个共享的存储服务器存储锁资源，同一时刻只有一个客户端能占有锁资源来完成。通常有基于Zookeeper，Redis，或数据库三种实现形式。本文介绍基于Redis的实现方案。","text":"分布式锁是在分布式环境下（多个JVM进程）控制多个客户端对某一资源的同步访问的一种实现，与之相对应的是线程锁，线程锁控制的是同一个JVM进程内多个线程之间的同步。分布式锁的一般实现方法是在应用服务器之外通过一个共享的存储服务器存储锁资源，同一时刻只有一个客户端能占有锁资源来完成。通常有基于Zookeeper，Redis，或数据库三种实现形式。本文介绍基于Redis的实现方案。 要求基于Redis实现分布式锁需要满足如下几点要求： 在分布式集群中，被分布式锁控制的方法或代码段同一时刻只能被一个客户端上面的一个线程执行，也就是互斥 锁信息需要设置过期时间，避免一个线程长期占有（比如在做解锁操作前异常退出）而导致死锁 加锁与解锁必须一致，谁加的锁，就由谁来解（或过期超时），一个客户端不能解开另一个客户端加的锁 加锁与解锁的过程必须保证原子性 实现1. 加锁实现基于Redis的分布式锁加锁操作一般使用 SETNX 命令，其含义是“将 key 的值设为 value ，当且仅当 key 不存在。若给定的 key 已经存在，则 SETNX 不做任何动作”。在 Spring Boot 中，可以使用 StringRedisTemplate 来实现，如下，一行代码即可实现加锁过程。（下列代码给出两种调用形式——立即返回加锁结果与给定超时时间获取加锁结果） 12345678910111213141516171819202122232425262728293031323334353637383940/** * 尝试获取锁（立即返回） * @param key 锁的redis key * @param value 锁的value * @param expire 过期时间/秒 * @return 是否获取成功 */public boolean lock(String key, String value, long expire) &#123; return stringRedisTemplate.opsForValue().setIfAbsent(key, value, expire, TimeUnit.SECONDS);&#125;/** * 尝试获取锁，并至多等待timeout时长 * * @param key 锁的redis key * @param value 锁的value * @param expire 过期时间/秒 * @param timeout 超时时长 * @param unit 时间单位 * @return 是否获取成功 */public boolean lock(String key, String value, long expire, long timeout, TimeUnit unit) &#123; long waitMillis = unit.toMillis(timeout); long waitAlready = 0; while (!stringRedisTemplate.opsForValue().setIfAbsent(key, value, expire, TimeUnit.SECONDS) &amp;&amp; waitAlready &lt; waitMillis) &#123; try &#123; Thread.sleep(waitMillisPer); &#125; catch (InterruptedException e) &#123; log.error(\"Interrupted when trying to get a lock. key: &#123;&#125;\", key, e); &#125; waitAlready += waitMillisPer; &#125; if (waitAlready &lt; waitMillis) &#123; return true; &#125; log.warn(\"&lt;====== lock &#123;&#125; failed after waiting for &#123;&#125; ms\", key, waitAlready); return false;&#125; 上述实现如何满足前面提到的几点要求： 客户端互斥： 可以将expire过期时间设置为大于同步代码的执行时间，比如同步代码块执行时间为1s，则可将expire设置为3s或5s。避免同步代码执行过程中expire时间到，其它客户端又可以获取锁执行同步代码块。 通过设置过期时间expire来避免某个客户端长期占有锁。 通过value来控制谁加的锁，由谁解的逻辑，比如可以使用requestId作为value，requestId唯一标记一次请求。 setIfAbsent方法 底层通过调用 Redis 的 SETNX 命令，操作具备原子性。 错误示例： 网上有如下实现， 1234567public boolean lock(String key, String value, long expire) &#123; boolean result = stringRedisTemplate.opsForValue().setIfAbsent(key, value); if(result) &#123; stringRedisTemplate.expire(key, expire, TimeUnit.SECONDS); &#125; return result;&#125; 该实现的问题是如果在result为true，但还没成功设置expire时，程序异常退出了，将导致该锁一直被占用而导致死锁，不满足第二点要求。 2. 解锁实现解锁也需要满足前面所述的四个要求，实现代码如下： 123456789101112private static final String RELEASE_LOCK_LUA_SCRIPT = \"if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end\";private static final Long RELEASE_LOCK_SUCCESS_RESULT = 1L;/** * 释放锁 * @param key 锁的redis key * @param value 锁的value */public boolean unLock(String key, String value) &#123; DefaultRedisScript&lt;Long&gt; redisScript = new DefaultRedisScript&lt;&gt;(RELEASE_LOCK_LUA_SCRIPT, Long.class); long result = stringRedisTemplate.execute(redisScript, Collections.singletonList(key), value); return Objects.equals(result, RELEASE_LOCK_SUCCESS_RESULT);&#125; 这段实现使用一个Lua脚本来实现解锁操作，保证操作的原子性。传入的value值需与该线程加锁时的value一致，可以使用requestId（具体实现下面给出）。 错误示例： 123456 public boolean unLock(String key, String value) &#123; String oldValue = stringRedisTemplate.opsForValue().get(key); if(value.equals(oldValue)) &#123; stringRedisTemplate.delete(key); &#125;&#125; 该实现先获取锁的当前值，判断两值相等则删除。考虑一种极端情况，如果在判断为true时，刚好该锁过期时间到，另一个客户端加锁成功，则接下来的delete将不管三七二十一将别人加的锁直接删掉了，不满足第三点要求。该示例主要是因为没有保证解锁操作的原子性导致。 3. 注解支持为了方便使用，添加一个注解，可以放于方法上控制方法在分布式环境中的同步执行。 12345678910/*** 标注在方法上的分布式锁注解*/@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface DistributedLockable &#123; String key(); String prefix() default \"disLock:\"; long expire() default 10L; // 默认10s过期&#125; 添加一个切面来解析注解的处理， 123456789101112131415161718192021222324252627282930313233343536373839/*** 分布式锁注解处理切面*/@Aspect@Slf4jpublic class DistributedLockAspect &#123; private DistributedLock lock; public DistributedLockAspect(DistributedLock lock) &#123; this.lock = lock; &#125; /** * 在方法上执行同步锁 */ @Around(value = \"@annotation(lockable)\") public Object distLock(ProceedingJoinPoint point, DistributedLockable lockable) throws Throwable &#123; boolean locked = false; String key = lockable.prefix() + lockable.key(); try &#123; locked = lock.lock(key, WebUtil.getRequestId(), lockable.expire()); if(locked) &#123; return point.proceed(); &#125; else &#123; log.info(\"Did not get a lock for key &#123;&#125;\", key); return null; &#125; &#125; catch (Exception e) &#123; throw e; &#125; finally &#123; if(locked) &#123; if(!lock.unLock(key, WebUtil.getRequestId()))&#123; log.warn(\"Unlock &#123;&#125; failed, maybe locked by another client already. \", lockable.key()); &#125; &#125; &#125; &#125;&#125; RequestId 的实现如下，通过注册一个Filter，在请求开始时生成一个uuid存于ThreadLocal中，在请求返回时清除。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class WebUtil &#123; public static final String REQ_ID_HEADER = \"Req-Id\"; private static final ThreadLocal&lt;String&gt; reqIdThreadLocal = new ThreadLocal&lt;&gt;(); public static void setRequestId(String requestId) &#123; reqIdThreadLocal.set(requestId); &#125; public static String getRequestId()&#123; String requestId = reqIdThreadLocal.get(); if(requestId == null) &#123; requestId = ObjectId.next(); reqIdThreadLocal.set(requestId); &#125; return requestId; &#125; public static void removeRequestId() &#123; reqIdThreadLocal.remove(); &#125;&#125;public class RequestIdFilter implements Filter &#123; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; HttpServletRequest httpServletRequest = (HttpServletRequest) servletRequest; String reqId = httpServletRequest.getHeader(WebUtil.REQ_ID_HEADER); //没有则生成一个 if (StringUtils.isEmpty(reqId)) &#123; reqId = ObjectId.next(); &#125; WebUtil.setRequestId(reqId); try &#123; filterChain.doFilter(servletRequest, servletResponse); &#125; finally &#123; WebUtil.removeRequestId(); &#125; &#125;&#125;//在配置类中注册Filter/*** 添加RequestId* @return*/@Beanpublic FilterRegistrationBean requestIdFilter() &#123; RequestIdFilter reqestIdFilter = new RequestIdFilter(); FilterRegistrationBean registrationBean = new FilterRegistrationBean(); registrationBean.setFilter(reqestIdFilter); List&lt;String&gt; urlPatterns = Collections.singletonList(\"/*\"); registrationBean.setUrlPatterns(urlPatterns); registrationBean.setOrder(Ordered.HIGHEST_PRECEDENCE + 1); return registrationBean;&#125; 4. 使用注解12345678910@DistributedLockable(key = \"test\", expire = 10)public void test()&#123; System.out.println(\"线程-\"+Thread.currentThread().getName()+\"开始执行...\" + LocalDateTime.now()); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"线程-\"+Thread.currentThread().getName()+\"结束执行...\" + LocalDateTime.now());&#125; 总结本文给出了基于Redis的分布式锁的实现方案与常见的错误示例。要保障分布式锁的正确运行，需满足本文所提的四个要求，尤其注意保证加锁解锁操作的原子性，设置过期时间，及对同一个锁的加锁解锁线程一致。","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.jboost.cn/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"redis","slug":"redis","permalink":"http://blog.jboost.cn/tags/redis/"},{"name":"RateLimiter","slug":"RateLimiter","permalink":"http://blog.jboost.cn/tags/RateLimiter/"}]},{"title":"记一次线上Java服务CPU 100%处理过程","slug":"issue-cpu-high","date":"2020-06-16T05:56:05.000Z","updated":"2020-08-26T01:39:15.612Z","comments":true,"path":"issue-cpu-high.html","link":"","permalink":"http://blog.jboost.cn/issue-cpu-high.html","excerpt":"告警正在开会，突然钉钉告警声响个不停，同时市场人员反馈客户在投诉系统登不进了，报504错误。查看钉钉上的告警信息，几台业务服务器节点全部报CPU超过告警阈值，达100%。 赶紧从会上下来，SSH登录服务器，使用 top 命令查看，几个Java进程CPU占用达到180%，190%，这几个Java进程对应同一个业务服务的几个Pod（或容器）。","text":"告警正在开会，突然钉钉告警声响个不停，同时市场人员反馈客户在投诉系统登不进了，报504错误。查看钉钉上的告警信息，几台业务服务器节点全部报CPU超过告警阈值，达100%。 赶紧从会上下来，SSH登录服务器，使用 top 命令查看，几个Java进程CPU占用达到180%，190%，这几个Java进程对应同一个业务服务的几个Pod（或容器）。 定位 使用 docker stats 命令查看本节点容器资源使用情况，对占用CPU很高的容器使用 docker exec -it &lt;容器ID&gt; bash 进入。 在容器内部执行 top 命令查看，定位到占用CPU高的进程ID，使用 top -Hp &lt;进程ID&gt; 定位到占用CPU高的线程ID。 使用 jstack &lt;进程ID&gt; &gt; jstack.txt 将进程的线程栈打印输出。 退出容器， 使用 docker cp &lt;容器ID&gt;:/usr/local/tomcat/jstack.txt ./ 命令将jstack文件复制到宿主机，便于查看。获取到jstack信息后，赶紧重启服务让服务恢复可用。 将2中占用CPU高的线程ID使用 pringf &#39;%x\\n&#39; &lt;线程ID&gt; 命令将线程ID转换为十六进制形式。假设线程ID为133，则得到十六进制85。在jstack.txt文件中定位到 nid=0x85的位置，该位置即为占用CPU高线程的执行栈信息。如下图所示， 与同事确认，该处为使用一个框架的excel导出功能，并且，导出excel时没有分页，没有限制！！！查看SQL查询记录，该导出功能一次导出50w条数据，并且每条数据都需要做转换计算，更为糟糕的是，操作者因为导出时久久没有响应，于是连续点击，几分钟内发起了10多次的导出请求。。。于是，CPU被打满，服务崩溃了，我也崩溃了。。 解决对于此类耗资源的操作，一定要做好相应的限制。比如可以限制请求量，控制最大分页大小，同时可以限制访问频率，比如同一用户一分钟内最多请求多少次。 再发服务重启后恢复。到了下午，又一台服务器节点CPU告警，依前面步骤定位到占用CPU高的线程，如下 123\"GC task thread#0 (ParallelGC)\" os_prio=0 tid=0x00007fa114020800 nid=0x10 runnable \"GC task thread#1 (ParallelGC)\" os_prio=0 tid=0x00007fa114022000 nid=0x11 runnable 使用命令 jstat -gcutil &lt;进程ID&gt; 2000 10 查看GC情况，如图 发现Full GC次数达到1000多次，且还在不断增长，同时Eden区，Old区已经被占满（也可使用jmap -heap &lt;进程ID&gt;查看堆内存各区的占用情况），使用jmap将内存使用情况dump出来， 1jmap -dump:format=b,file=./jmap.dump 13 退出容器，使用 docker cp &lt;容器ID&gt;:/usr/local/tomcat/jmap.dump ./ 将dump文件复制到宿主机目录，下载到本地，使用 MemoryAnalyzer（下载地址：https://www.eclipse.org/mat/downloads.php ）打开，如图 如果dump文件比较大，需要增大MemoryAnalyzer.ini配置文件中的-Xmx值 发现占用内存最多的是char[], String对象，通过右键可以查看引用对象，但点开貌似也看不出所以然来，进入内存泄露报告页面，如图 该页面统计了堆内存的占用情况，并且给出疑似泄露点，在上图中点开“see stacktrace”链接，进入线程栈页面， 似曾熟悉的画面，还是跟excel导出有关，数据太多，导致内存溢出。。。于是GC频繁，于是CPU爆了。根源还是同一个。 总结本文以处理一次线上服务CPU 100%的实战过程示例了在遇到Java服务造成服务器CPU消耗过高或内存溢出的一般处理方法，希望对大家定位线上类似问题提供参考。同时，开发实现功能时需要考虑的更深远一些，不能停留在解决当前的场景，需要考虑数据量不断增大时，你的实现是否还能适用。俗话说，初级程序员解决当前问题，中级程序员解决两年后的问题，高级程序员解决五年后的问题，^_^。","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.jboost.cn/categories/Java/"}],"tags":[{"name":"cpu100%","slug":"cpu100","permalink":"http://blog.jboost.cn/tags/cpu100/"}]},{"title":"nginx反向代理导致session失效的问题处理","slug":"nginx-proxy","date":"2020-06-02T01:50:00.000Z","updated":"2020-08-26T01:36:55.493Z","comments":true,"path":"nginx-proxy.html","link":"","permalink":"http://blog.jboost.cn/nginx-proxy.html","excerpt":"一同事求援：后台系统的登录成功了，但不能成功登进系统，仍然跳转到登录页，但同一套代码另一个环境却没有问题。","text":"一同事求援：后台系统的登录成功了，但不能成功登进系统，仍然跳转到登录页，但同一套代码另一个环境却没有问题。 背景经了解，他对同一个项目使用tomcat部署了两个环境，一个在开发服务器上，一个在他本机，两个环境代码配置完全相同。两边通过同一个nginx进行反向代理，nginx配置大致如下， 1234567location &#x2F;health&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;192.168.40.159:8081&#x2F;health&#x2F;; #无问题的配置 &#125;location &#x2F;health-dev&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;192.168.40.202:8080&#x2F;health&#x2F;; #有问题的配置&#125; 一个反向代理到开发环境，一个反向代理到本机服务。 定位既然代码配置完全相同，那么问题很大可能就出现在nginx的反向代理上。 因为两边location路径不同（即浏览器路径不同），但是反向代理的服务端路径却相同，结合session的基本原理，如下图， 当浏览器第一次打开页面时，服务端会为这次会话创建一个session，并将session id通过response的header传递给浏览器，header一般为 Set-Cookie: JSESSIONID=xxxxx; Path=xxxx 浏览器接收到响应后，如果header Set-Cookie 中path的值与浏览器地址路径匹配，则将该header值存于浏览器的Cookie中 浏览器在下次请求服务器时，将Cookie中的JSESSIONID值通过request的header上报给服务端，header一般为 Cookie: JSESSIONID=xxxx; 服务端可通过该JSESSIONID来定位到对应的session nginx反向代理按这种方式配置时 123location &#x2F;health-dev&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;192.168.40.202:8080&#x2F;health&#x2F;;&#125; 浏览器访问 http://www.domian.com/health-dev 时，服务端返回的 Set-Cookie 的 Path 值为 /health（因为中间有反向代理，服务端并不知道代理前的路径是啥，是按最终请求服务端的路径设置），如图 因为浏览器访问地址的路径 /health-dev 与 Set-Cookie 的 Path /health 不匹配，所以浏览器并不会将其值存入Cookie中，如图 因此在下次请求服务器时，浏览器无法设置request Cookie header的 JSESSIONID 值，服务器无法定位到对应的session，因此会将其当做第一次请求，创建一个新的session，如此反复，因此就算你登录认证通过了，但服务器返回的登录凭证（JSESSIONID）浏览器不会保存，并在下次请求时携带，导致服务器认为你是一个新的请求，当然就会又跳到登录页面了。 解决nginx有一个命令 proxy_cookie_path（参考： proxy_cookie_path）可将服务器返回的 Set-Cookie 中的path进行修改，格式为 proxy_cookie_path 原路径 目标路径，我们在配置中添加 proxy_cookie_path 如下。 1234location &#x2F;health-dev&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;192.168.40.202:8080&#x2F;health&#x2F;; proxy_cookie_path &#x2F;health &#x2F;health-dev;&#125; 重启nginx，问题解决。","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.jboost.cn/categories/DevOps/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://blog.jboost.cn/tags/nginx/"}]},{"title":"Kubernetes笔记（四）：详解Namespace与资源限制ResourceQuota，LimitRange","slug":"k8s4-namespace","date":"2020-05-27T05:20:25.000Z","updated":"2020-08-26T01:44:31.062Z","comments":true,"path":"k8s4-namespace.html","link":"","permalink":"http://blog.jboost.cn/k8s4-namespace.html","excerpt":"前面我们对K8s的基本组件与概念有了个大致的印象，并且基于K8s实现了一个初步的CI/CD流程，但对里面涉及的各个对象（如Namespace, Pod, Deployment, Service, Ingress, PVC等）及各对象的管理可能还缺乏深入的理解与实践，接下来的文章就让我们一起深入K8s的各组件内部来一探究竟吧。下图是基于个人的理解梳理的一个K8s结构图，示例了各个组件（只包含了主要组件）如何协同。","text":"前面我们对K8s的基本组件与概念有了个大致的印象，并且基于K8s实现了一个初步的CI/CD流程，但对里面涉及的各个对象（如Namespace, Pod, Deployment, Service, Ingress, PVC等）及各对象的管理可能还缺乏深入的理解与实践，接下来的文章就让我们一起深入K8s的各组件内部来一探究竟吧。下图是基于个人的理解梳理的一个K8s结构图，示例了各个组件（只包含了主要组件）如何协同。 后续几篇文章围绕该图涉及组件进行整理介绍，本文主要探究Namespace及与Namespace管理相关的资源限制ResourceQuota/LimitRange部分。 Namespace理解Namespace即命名空间，主要有两个方面的作用： 资源隔离：可为不同的团队/用户（或项目）提供虚拟的集群空间，共享同一个Kubernetes集群的资源。比如可以为团队A创建一个Namespace ns-a，团队A的项目都部署运行在 ns-a 中，团队B创建另一个Namespace ns-b，其项目都部署运行在 ns-b 中，或者为开发、测试、生产环境创建不同的Namespace，以做到彼此之间相互隔离，互不影响。我们可以使用 ResourceQuota 与 Resource LimitRange 来指定与限制 各个namesapce的资源分配与使用 权限控制：可以指定某个namespace哪些用户可以访问，哪些用户不能访问 Kubernetes 安装成功后，默认会创建三个namespace： default：默认的namespace，如果创建Kubernetes对象时不指定 metadata.namespace，该对象将在default namespace下创建 kube-system：Kubernetes系统创建的对象放在此namespace下，我们前面说的kube-apiserver，etcd，kube-proxy等都在该namespace下 kube-public：顾名思义，共享的namespace，所有用户对该namespace都是可读的。主要是为集群做预留，一般都不在该namespace下创建对象 实践1.查看namesapce 1234kubectl get namespaceskubectl get namesapcekubectl get ns # 三个操作等效kubectl get ns --show-labels # 显示namespace的label 使用namesapces,namesapce,ns都是可以的。如下列出了当前集群中的所有namespace 12345678910[root@kmaster ~]# kubectl get nsNAME STATUS AGEdefault Active 34ddevelop Active 17dingress-nginx Active 33dkube-node-lease Active 34dkube-public Active 34dkube-system Active 34dkubernetes-dashboard Active 31dpre-release Active 17d 可以使用 kubectl describe 命令来查看某个namespace的概要信息，如 123456789[root@kmaster ~]# kubectl describe ns defaultName: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Status: ActiveNo resource quota.No resource limits. 2.创建namespace 有两种方式：通过yaml定义文件创建或直接使用命令创建。 123456789101112# 方式1. 通过yaml定义文件创建[root@kmaster ~]# vim test-namespace.yamlapiVersion: v1kind: Namespacemetadata: name: test # namespace的名称 labels: name: ns-test[root@kmaster ~]# kubectl create -f ./test-namespace.yaml # 方式2. 直接使用命令创建[root@kmaster ~]# kubectl create ns test 3.在namesapce中创建对象 12345678910111213# 1. 在yaml中通过metadata.namesapce 指定[root@kmaster ~]# kubectl get deploy my-nginx -o yamlapiVersion: apps/v1kind: Deploymentmetadata: labels: run: my-nginx name: my-nginx namespace: test # 指定namespacespec: ...# 2. 在命令中通过 -n 或 --namesapce 指定[root@kmaster ~]# kubectl run dev-nginx --image=nginx:latest --replicas=3 -n test 4.设定kubectl namesapce上下文 kubectl上下文即集群、namespace、用户的组合，设定kubectl上下文，即可以以上下文指定的用户，在上下文指定的集群与namespace中进行操作管理。查看当前集群kubectl上下文 123456789101112131415161718192021# 查看当前kubectl上下文[root@kmaster ~]# kubectl config viewapiVersion: v1clusters:- cluster: certificate-authority-data: DATA+OMITTED server: https://192.168.40.111:6443 name: kubernetescontexts:- context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetescurrent-context: kubernetes-admin@kuberneteskind: Configpreferences: &#123;&#125;users:- name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED 可见当前上下文为kubernetes-admin@kubernetes (current-context: kubernetes-admin@kubernetes)。 创建一个kubectl上下文 12[root@kmaster ~]# kubectl config set-context test --namespace=test --cluster=kubernetes --user=kubernetes-adminContext \"test\" created. 再次执行 kubectl config view 将可以看到上面创建的test上下文。 切换上下文 123456# 设置当前上下文[root@kmaster ~]# kubectl config use-context testSwitched to context \"test\".# 查看当前所在的上下文[root@kmaster ~]# kubectl config current-contexttest 指定了上下文，后续操作都在该上下文对应的namespace中进行，不需要再显式指定namespace。在上下文中创建对象 123456789101112# 在当前上下文中创建对象[root@kmaster ~]# kubectl run my-nginx --image=nginx:latest --replicas=2kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.deployment.apps/my-nginx created# 查看创建的对象，不需要指定namespace[root@kmaster ~]# kubectl get deployNAME READY UP-TO-DATE AVAILABLE AGEmy-nginx 2/2 2 2 25m[root@kmaster ~]# kubectl get podNAME READY STATUS RESTARTS AGEmy-nginx-667764d77b-ldb78 1/1 Running 0 24mmy-nginx-667764d77b-wpgxw 1/1 Running 0 24m 删除上下文 12[root@kmaster ~]# kubectl config delete-context testdeleted context test from /root/.kube/config 也可以使用如下命令直接切换默认的namespace 12# 将默认namespace设置为test[root@kmaster ~]# kubectl config set-context --current --namespace=test 5.删除namesapce 可以使用 kubectl delete ns &lt;namespace名称&gt; 来删除一个namesapce，该操作会删除namespace中的所有内容。 1[root@kmaster ~]# kubectl delete ns test Resource QuotaResource Quota即资源配额，限定单个namespace中可使用集群资源的总量，包括两个维度： 限定某个对象类型（如Pod）可创建对象的总数； 限定某个对象类型可消耗的计算资源（CPU、内存）与存储资源（存储卷声明）总数 如果在 namespace 中为计算资源 CPU 和内存设定了 ResourceQuota，用户在创建对象（Pod、Service等）时，必须指定 requests 和 limits；如果在创建或更新对象时申请的资源与 namespace 的 ResourceQuota 冲突，则 apiserver 会返回 HTTP 状态码 403，以及对应的错误提示信息。当集群中总的容量小于各个 namespace 资源配额的总和时，可能会发生资源争夺，此时 Kubernetes 将按照先到先得的方式分配资源。 对象数量限制声明格式为： count/&lt;resource&gt;.&lt;group&gt;， 如下列出各类对象的声明格式 1234567891011count&#x2F;persistentvolumeclaims count&#x2F;servicescount&#x2F;secretscount&#x2F;configmapscount&#x2F;replicationcontrollerscount&#x2F;deployments.appscount&#x2F;replicasets.appscount&#x2F;statefulsets.appscount&#x2F;jobs.batchcount&#x2F;cronjobs.batchcount&#x2F;deployments.extensions 计算资源限制定义CPU、内存请求（requests）、限制（limits）使用的总量，包括 limits.cpu：namespace中，所有非终止状态的 Pod 的 CPU 限制 resources.limits.cpu 总和不能超过该值 limits.memory：namespace中，所有非终止状态的 Pod 的内存限制 resources.limits.memory 总和不能超过该值 requests.cpu：namespace中，所有非终止状态的 Pod 的 CPU 请求 resources.requrest.cpu 总和不能超过该值 requests.memory：namespace中，所有非终止状态的 Pod 的 CPU 请求 resources.requests.memory 总和不能超过该值 存储资源限制定义存储卷声明请求的存储总量或创建存储卷声明数量的限制，包括 requests.storage：namespace中，所有存储卷声明（PersistentVolumeClaim）请求的存储总量不能超过该值 persistentvolumeclaims：namespace中，可以创建的存储卷声明的总数不能超过该值 &lt;storage-class-name&gt;.storageclass.storage.k8s.io/requests.storage：namespace中，所有与指定存储类（StorageClass）关联的存储卷声明请求的存储总量不能超过该值 &lt;storage-class-name&gt;.storageclass.storage.k8s.io/persistentvolumeclaims：namespace中，所有与指定存储类关联的存储卷声明的总数不能超过该值 除此之外，还可以对本地临时存储资源进行限制定义 requests.ephemeral-storage：namespace中，所有 Pod 的本地临时存储（local ephemeral storage）请求的总和不能超过该值 limits.ephemeral-storage：namespace中，所有 Pod 的本地临时存储限定的总和不能超过此值 实践查看是否开启 Resource Quota 支持，默认一般是开启的。如果没有，可在启动 apiserver 时为参数 –enable-admission-plugins 添加 ResourceQuota 配置项。 1.创建ResourceQuota 123456789101112131415161718192021222324252627282930313233343536# 创建namespace[root@kmaster ~]# kubectl create namespace test# 编辑ResourceQuota定义文档[root@kmaster ~]# vim quota-test.yamlapiVersion: v1kind: ResourceQuotametadata: name: quota-test namespace: testspec: hard: requests.cpu: \"2\" requests.memory: 2Gi limits.cpu: \"4\" limits.memory: 4Gi requests.nvidia.com/gpu: 4 pods: \"3\" services: \"6\"# 创建ResourceQuota[root@kmaster ~]# kubectl apply -f quota-test.yaml# 查看[root@kmaster ~]# kubectl get quota -n testNAME CREATED ATquota-test 2020-05-26T10:31:10Z[root@kmaster ~]# kubectl describe quota quota-test -n testName: quota-testNamespace: testResource Used Hard-------- ---- ----limits.cpu 0 4limits.memory 0 4Gipods 0 3requests.cpu 0 2requests.memory 0 2Girequests.nvidia.com/gpu 0 4services 0 6 或者使用kubectl命令，如 1[root@kmaster ~]# kubectl create quota quota-test --hard=count/deployments.extensions=2,count/replicasets.extensions=4,count/pods=3,count/secrets=4 --namespace=test 我们在namespace test中创建了一个ResourceQuota，限制CPU、内存请求为2、2GB，限制CPU、内存限定使用为4、4GB，限制Pod个数为3 等。 我们来尝试创建一个如下定义的Deployment来测试一下， 12345678910111213141516171819202122232425262728293031323334353637# 创建一个测试deploy[root@kmaster ~]# vim quota-test-deploy.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: quota-test-deployspec: selector: matchLabels: purpose: quota-test replicas: 3 template: metadata: labels: purpose: quota-test spec: containers: - name: quota-test image: nginx resources: limits: memory: \"2Gi\" cpu: \"1\" requests: memory: \"500Mi\" cpu: \"500m\"[root@kmaster ~]# kubectl apply -f quota-test-deploy.yaml -n test# 查看pod[root@kmaster ~]# kubectl get pod -n testNAME READY STATUS RESTARTS AGEquota-test-deploy-6b89fdc686-2dthq 1/1 Running 0 3m54squota-test-deploy-6b89fdc686-9m2qw 1/1 Running 0 3m54s# 查看deploy状态[root@kmaster ~]# kubectl get deploy quota-test-deploy -n test -o yaml message: 'pods \"quota-test-deploy-6b89fdc686-rmktq\" is forbidden: exceeded quota: quota-test, requested: limits.memory=2Gi, used: limits.memory=4Gi, limited: limits.memory=4Gi' replicas: 3定义创建三个Pod副本，但只成功创建了两个Pod，在deploy的status部分（最后一条命令结果），我们可以看到message提示第三个Pod创建时被拒绝，因为内存已达到限定。我们也可以将limits.memory调整为1Gi，将replicas调整为4，来验证对Pod个数的限制。可看到最终只起了三个Pod，status部分message提示 pods &quot;quota-test-deploy-9dc54f95c-gzqw7&quot; is forbidden: exceeded quota:quota-test, requested: pods=1, used: pods=3, limited: pods=3。 Resource Limit Range理解Resource Quota 是对namespace中总体的资源使用进行限制，Resource Limit Range 则是对具体某个Pod或容器的资源使用进行限制。默认情况下，namespace中Pod或容器的资源消耗是不受限制的，这就可能导致某个容器应用内存泄露耗尽资源影响其它应用的情况。Limit Range可以用来限定namespace内Pod（或容器）可以消耗资源的数量。 使用LimitRange对象，我们可以： 限制namespace中每个Pod或容器的最小与最大计算资源 限制namespace中每个Pod或容器计算资源request、limit之间的比例 限制namespace中每个存储卷声明（PersistentVolumeClaim）可使用的最小与最大存储空间 设置namespace中容器默认计算资源的request、limit，并在运行时自动注入到容器中 如果创建或更新对象（Pod、容器、PersistentVolumeClaim）对资源的请求与LimitRange相冲突，apiserver会返回HTTP状态码403，以及相应的错误提示信息；如果namespace中定义了LimitRange 来限定CPU与内存等计算资源的使用，则用户创建Pod、容器时，必须指定CPU或内存的request与limit，否则将被系统拒绝；当namespace总的limit小于其中Pod、容器的limit之和时，将发生资源争夺，Pod或者容器将不能创建，但不影响已经创建的Pod或容器。 实践创建一个测试namespace test-limitrange， 1234# 创建测试namespace[root@kmaster ~]# kubectl create namespace test-limitrange# 切换默认的namespace[root@kmaster ~]# kubectl config set-context --current --namespace=test-limitrange 创建LimitRange定义文件 lr-test.yaml 12345678910111213141516171819202122232425262728293031apiVersion: v1kind: LimitRangemetadata: name: lr-testspec: limits: - type: Container #资源类型 max: cpu: \"1\" #限定最大CPU memory: \"1Gi\" #限定最大内存 min: cpu: \"100m\" #限定最小CPU memory: \"100Mi\" #限定最小内存 default: cpu: \"900m\" #默认CPU限定 memory: \"800Mi\" #默认内存限定 defaultRequest: cpu: \"200m\" #默认CPU请求 memory: \"200Mi\" #默认内存请求 maxLimitRequestRatio: cpu: 2 #限定CPU limit/request比值最大为2 memory: 1.5 #限定内存limit/request比值最大为1.5 - type: Pod max: cpu: \"2\" #限定Pod最大CPU memory: \"2Gi\" #限定Pod最大内存 - type: PersistentVolumeClaim max: storage: 2Gi #限定PVC最大的requests.storage min: storage: 1Gi #限定PVC最小的requests.storage 该文件定义了在namespace test-limitrange 中，容器、Pod、PVC的资源限制，在该namesapce中，只有满足如下条件，对象才能创建成功 容器的resources.limits部分CPU必须在100m-1之间，内存必须在100Mi-1Gi之间，否则创建失败 容器的resources.limits部分CPU与resources.requests部分CPU的比值最大为2，memory比值最大为1.5，否则创建失败 Pod内所有容器的resources.limits部分CPU总和最大为2，内存总和最大为2Gi，否则创建失败 PVC的resources.requests.storage最大为2Gi，最小为1Gi，否则创建失败 如果容器定义了resources.requests没有定义resources.limits，则LimitRange中的default部分将作为limit注入到容器中；如果容器定义了resources.limits却没有定义resources.requests，则将requests值也设置为limits的值；如果容器两者都没有定义，则使用LimitRange中default作为limits，defaultRequest作为requests值 创建与查看LimitRange， 12345678910111213# 创建LimitRange[root@kmaster ~]# kubectl apply -f lr-test.yaml# 查看[root@kmaster ~]# kubectl describe limits lr-testName: lr-testNamespace: test-limitrangeType Resource Min Max Default Request Default Limit Max Limit/Request Ratio---- -------- --- --- --------------- ------------- -----------------------Container cpu 100m 1 200m 900m 2Container memory 100Mi 1Gi 200Mi 800Mi 1500mPod cpu - 2 - - -Pod memory - 2Gi - - -PersistentVolumeClaim storage 1Gi 2Gi - - - 我们可以创建不同配置的容器或Pod对象来验证，出于篇幅不再列出验证步骤。 总结本文对K8s的Namespace及针对Namespace的资源限制管理ResourceQuota，LimitRange进行了较为深入的探索，其中ResourceQuota对整个Namespace的资源使用情况进行限制，LimitRange则对单个的Pod或容器的资源使用进行限制。Namespace的权限控制可基于RBAC来实现，后续再单独进行梳理介绍。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.jboost.cn/categories/Kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.jboost.cn/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"http://blog.jboost.cn/tags/k8s/"}]},{"title":"解决K8s “kernel:unregister_netdevice:waiting for vethxxx to become free. Usage count=1” 的问题（Kernel升级）","slug":"k8s-netdevice-issue","date":"2020-05-22T03:35:05.000Z","updated":"2020-08-26T01:43:56.751Z","comments":true,"path":"k8s-netdevice-issue.html","link":"","permalink":"http://blog.jboost.cn/k8s-netdevice-issue.html","excerpt":"k8s集群运行过程中，经常出现节点上报出类似 “kernel:unregister_netdevice:waiting for vethxxx to become free. Usage count=1” 的错误信息，一方面影响交互，另一方面，对于有些操作比如 docker stop，半天没有响应，处于hang住的状态。","text":"k8s集群运行过程中，经常出现节点上报出类似 “kernel:unregister_netdevice:waiting for vethxxx to become free. Usage count=1” 的错误信息，一方面影响交互，另一方面，对于有些操作比如 docker stop，半天没有响应，处于hang住的状态。 问题背景操作系统及内核版本为 12345[root@dev-server-2 ~]# cat /etc/redhat-release CentOS Linux release 7.6.1810 (Core) [root@dev-server-2 ~]# uname -aLinux dev-server-2 3.10.0-957.el7.x86_64 #1 SMP Thu Nov 8 23:39:32 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux 经常有节点报出类似如下错误 查看一些网上资料，一般都说是Kernel的Bug，并且虽然有些地方说在先前版本已经修复，但是在较高版本上仍有出现。 有一些解决办法是停止syslog服务，让其不将错误信息显示到控制台， 1[root@dev-server-2 ~]# systemctl stop rsyslog 但这只能解决影响交互的问题，对于容器引擎被hang住的问题并没有解决，治标不治本。 也有人对此Bug的原因进行了追踪分析，判断可能是net_device引用计数器泄露的原因（参考：https://zhuanlan.zhihu.com/p/66895097 ），并给出了修复补丁，尝试着安装给出的补丁，但没有成功。 查看了目前elrepo的kernel的ml（mainline stable，稳定的主线版本）版本已经到了 5.6.14-1.el7.elrepo， 于是尝试着对kernel进行升级。 升级Kernel 更新yum仓库 1[root@dev-server-2 ~]# yum -y update 导入elrepo仓库 123456#导入elrepo仓库的公钥[root@dev-server-2 ~]# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org#安装ELRepo仓库的yum源[root@dev-server-2 ~]# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm#查看可用的kerner版本[root@dev-server-2 ~]# yum --disablerepo=\"*\" --enablerepo=\"elrepo-kernel\" list available 升级内核 12#安装最新版的内核[root@dev-server-2 ~]# yum --enablerepo=elrepo-kernel install kernel-ml 设置默认启动内核 123456789#查看系统已有内核[root@dev-server-2 ~]# sudo awk -F\\' '$1==\"menuentry \" &#123;print i++ \" : \" $2&#125;' /etc/grub2.cfg0 : CentOS Linux (5.6.14-1.el7.elrepo.x86_64) 7 (Core)1 : CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core)2 : CentOS Linux (0-rescue-f638aa03cdcf42998b077254bde50b53) 7 (Core)#设置默认启动内核，其中0为上述查出内核的索引grub2-set-default 0#生成grub配置文件grub2-mkconfig -o /boot/grub2/grub.cfg 重启验证 12345#重启[root@dev-server-2 devuser]# reboot#查看当前内核版本[root@dev-server-2 devuser]# uname -aLinux dev-server-2 5.6.14-1.el7.elrepo.x86_64 #1 SMP Tue May 19 12:17:13 EDT 2020 x86_64 x86_64 x86_64 GNU/Linux 删除旧内核 1234567891011#查看现有内核[root@dev-server-2 devuser]# rpm -qa | grep kernelkernel-tools-3.10.0-957.el7.x86_64kernel-headers-3.10.0-1062.1.1.el7.x86_64kernel-debuginfo-3.10.0-957.el7.x86_64kernel-tools-libs-3.10.0-957.el7.x86_64kernel-debuginfo-common-x86_64-3.10.0-957.el7.x86_64kernel-3.10.0-957.el7.x86_64kernel-ml-5.6.14-1.el7.elrepo.x86_64#删除旧的内核[root@dev-server-2 devuser]# yum remove ernel-tools-3.10.0-957.el7.x86_64 kernel-headers-3.10.0-1062.1.1.el7.x86_64 kernel-debuginfo-3.10.0-957.el7.x86_64 kernel-tools-libs-3.10.0-957.el7.x86_64 kernel-debuginfo-common-x86_64-3.10.0-957.el7.x86_64 kernel-3.10.0-957.el7.x86_64 后续通过升级内核重启后，问题目前没有再复现。但是否彻底解决该Bug待进一步跟进（至少不需要重装系统了）。 参考： 诊断修复 TiDB Operator 在 K8s 测试中遇到的 Linux 内核问题 Centos7 升级内核版本","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.jboost.cn/categories/Kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.jboost.cn/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"http://blog.jboost.cn/tags/k8s/"}]},{"title":"面试官：请写一个你认为比较“完美”的单例","slug":"patterns-singleton","date":"2020-05-12T03:50:21.000Z","updated":"2020-08-26T01:36:48.632Z","comments":true,"path":"patterns-singleton.html","link":"","permalink":"http://blog.jboost.cn/patterns-singleton.html","excerpt":"单例模式是保证一个类的实例有且只有一个，在需要控制资源（如数据库连接池），或资源共享（如有状态的工具类）的场景中比较适用。如果让我们写一个单例实现，估计绝大部分人都觉得自己没问题，但如果需要实现一个比较完美的单例，可能并没有你想象中简单。本文以主人公小雨的一次面试为背景，循序渐进地讨论如何实现一个较为“完美”的单例。本文人物与场景皆为虚构，如有雷同，纯属捏造。","text":"单例模式是保证一个类的实例有且只有一个，在需要控制资源（如数据库连接池），或资源共享（如有状态的工具类）的场景中比较适用。如果让我们写一个单例实现，估计绝大部分人都觉得自己没问题，但如果需要实现一个比较完美的单例，可能并没有你想象中简单。本文以主人公小雨的一次面试为背景，循序渐进地讨论如何实现一个较为“完美”的单例。本文人物与场景皆为虚构，如有雷同，纯属捏造。 小雨计算机专业毕业三年，对设计模式略有涉猎，能写一些简单的实现，掌握一些基本的JVM知识。在某次面试中，面试官要求现场写代码：请写一个你认为比较“完美”的单例。 简单的单例实现凭借着对单例的理解与印象，小雨写出了下面的代码 123456789101112public class Singleton &#123; private static Singleton instance; private Singleton()&#123;&#125; public static final Singleton getInstance()&#123; if(instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 写完后小雨审视了一遍，总觉得有点太简单了，离“完美”貌似还相差甚远。对，在多线程并发环境下，这个实现就玩不转了，如果两个线程同时调用 getInstance() 方法，同时执行到了 if 判断，则两边都认为 instance 实例为空，都会实例化一个 Singleton 对象，就会导致至少产生两个实例了，小雨心想。嗯，需要解决多线程并发环境下的同步问题，保证单例的线程安全。 线程安全的单例一提到并发同步问题，小雨就想到了锁。加个锁还不简单，synchronized 搞起， 123456789101112public class Singleton &#123; private static Singleton instance; private Singleton()&#123;&#125; public synchronized static final Singleton getInstance()&#123; if(instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 小雨再次审视了一遍，发现貌似每次 getInstance() 被调用时，其它线程必须等待这个线程调用完才能执行（因为有锁锁住了嘛），但是加锁其实是想避免多个线程同时执行实例化操作导致产生多个实例，在单例被实例化后，后续调用 getInstance() 直接返回就行了，每次都加锁释放锁造成了不必要的开销。 经过一阵思索与回想之后，小雨记起了曾经看过一个叫 Double-Checked Locking 的东东，双重检查锁，嗯，再优化一下, 12345678910111213141516public class Singleton &#123; private static volatile Singleton instance; private Singleton()&#123;&#125; public static final Singleton getInstance()&#123; if(instance == null) &#123; synchronized (Singleton.class)&#123; if(instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 单例在完成第一次实例化，后续再调用 getInstance() 先判空，如果不为空则直接返回，如果为空，就算两个线程同时判断为空，在同步块中还做了一次双重检查，可以确保只会实例化一次，省去了不必要的加锁开销，同时也保证了线程安全。并且令小雨感到自我满足的是他基于对JVM的一些了解加上了 volatile 关键字来避免实例化时由于指令重排序优化可能导致的问题，真是画龙点睛之笔啊。 简直——完美！ Tips: volatile关键字的语义保证变量对所有线程的可见性。对变量写值的时候JMM（Java内存模型）会将当前线程的工作内存值刷新到主内存，读的时候JMM会从主内存读取变量的值而不是从工作内存读取，确保一个变量值被一个线程更新后，另一个线程能立即读取到更新后的值。禁止指令重排序优化。JVM在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序，使用 volatile 可以禁止进行指令重排序优化。JVM创建一个新的实例时，主要需三步：分配内存初始化构造器将对象引用指向分配的内存地址如果一个线程在实例化时JVM做了指令重排，比如先执行了1，再执行3，最后执行2，则另一个线程可能获取到一个还没有完成初始化的对象引用，调用时可能导致问题，使用volatile可以禁止指令重排，避免这种问题。 小雨将答案交给面试官，面试官瞄了一眼说道：“基本可用了，但如果我用反射直接调用这个类的构造函数，是不是就不能保证单例了。” 小雨挠挠头，对哦，如果使用反射就可以在运行时改变单例构造器的可见性，直接调用构造器来创建一个新的实例了，比如通过下面这段代码 123Constructor&lt;Singleton&gt; constructor = Singleton.class.getDeclaredConstructor();constructor.setAccessible(true);Singleton singleton = constructor.newInstance(); 小雨再次陷入了思考。 反射安全的单例怎么避免反射破坏单例呢，或许可以加一个静态变量来控制,让构造器只有从 getInstance() 内部调用才有效，不通过 getInstance() 直接调用则抛出异常，小雨按这个思路做了一番改造， 123456789101112131415161718192021222324252627public class Singleton &#123; private static volatile Singleton instance; private static boolean flag = false; private Singleton()&#123; synchronized (Singleton.class) &#123; if (flag) &#123; flag = false; &#125; else &#123; throw new RuntimeException(\"Please use getInstance() method to get the single instance.\"); &#125; &#125; &#125; public static final Singleton getInstance()&#123; if(instance == null) &#123; synchronized (Singleton.class)&#123; if(instance == null) &#123; flag = true; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 使用静态变量 flag 来控制，只有从 getInstance() 调用构造器才能正常实例化，否则抛出异常。但马上小雨就发现了存在的问题：既然可以通过反射来调用构造器，那么也可以通过反射来改变 flag 的值，这样苦心设置的 flag 控制逻辑不就被打破了吗。看来也没那么“完美”。虽然并不那么完美，但也一定程度上规避了使用反射直接调用构造器的场景，并且貌似也想不出更好的办法了，于是小雨提交了答案。 面试官露出迷之微笑：“想法挺好，反射的问题基本解决了，但如果我序列化这个单例对象，然后再反序列化出来一个对象，这两个对象还一样吗，还能保证单例吗。如果不能，怎么解决这个问题？” 12345678910SerializationSafeSingleton s1 = SerializationSafeSingleton.getInstance();ByteArrayOutputStream bos = new ByteArrayOutputStream();ObjectOutputStream oos = new ObjectOutputStream(bos);oos.writeObject(s1);oos.close();ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray());ObjectInputStream ois = new ObjectInputStream(bis);SerializationSafeSingleton s2 = (SerializationSafeSingleton) ois.readObject();ois.close(); s1 == s2 吗？ 答案是否，如何解决呢。 序列化安全的单例小雨思考了一会，想起了曾经学习序列化知识时接触的 readResolve() 方法，该方法在ObjectInputStream已经读取一个对象并在准备返回前调用，可以用来控制反序列化时直接返回一个对象，替换从流中读取的对象，于是在前面实现的基础上，小雨添加了一个 readResolve() 方法， 1234567891011121314151617181920212223242526272829303132333435public class Singleton &#123; private static volatile Singleton instance; private static boolean flag = false; private Singleton()&#123; synchronized (Singleton.class) &#123; if (flag) &#123; flag = false; &#125; else &#123; throw new RuntimeException(\"Please use getInstance() method to get the single instance.\"); &#125; &#125; &#125; public static final Singleton getInstance()&#123; if(instance == null) &#123; synchronized (Singleton.class)&#123; if(instance == null) &#123; flag = true; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; /** * 该方法代替了从流中读取对象 * @return */ private Object readResolve()&#123; return getInstance(); &#125;&#125; 通过几个步骤的逐步改造优化，小雨完成了一个基本具备线程安全、反射安全、序列化安全的单例实现，心想这下应该足够完美了吧。面试官脸上继续保持着迷之微笑：“这个实现看起来还是显得有点复杂，并且也不能完全解决反射安全的问题，想想看还有其它实现方案吗。” 其它方案小雨反复思考，前面的实现是通过加锁来实现线程安全，除此之外，还可以通过类的加载机制来实现线程安全——类的静态属性只会在第一次加载类时初始化，并且在初始化的过程中，JVM是不允许其它线程来访问的，于是又写出了下面两个版本 静态初始化版本 123456789public class Singleton &#123; private static final Singleton instance = new Singleton(); private Singleton()&#123;&#125; public static final Singleton getInstance() &#123; return instance; &#125;&#125; 该版本借助JVM的类加载机制，本身线程安全，但只要 Singleton 类的某个静态对象（方法或属性）被访问，就会造成实例的初始化，而该实例可能根本不会被用到，造成资源浪费，另一方面也存在反射与序列化的安全性问题，也需要进行相应的处理。 静态内部类版本 1234567891011public class Singleton &#123; private Singleton()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.instance; &#125; private static class SingletonHolder &#123; private static final Singleton instance = new Singleton(); &#125;&#125; 该版本只有在调用 getInstance() 才会进行实例化，即延迟加载，避免资源浪费的问题，同时也能保障线程安全，但是同样存在反射与序列化的安全性问题，需要相应处理。 这貌似跟前面版本的复杂性差不多啊，依然都需要解决反射与安全性的问题，小雨心想，有没有一种既简单又能避免这些问题的方案呢。 “完美”方案一阵苦思冥想之后，小雨突然脑中灵光闪现，枚举！（这也是《Effective Java》的作者推荐的方式啊） 1234567public enum Singleton &#123; INSTANCE; public void func()&#123; ... &#125;&#125; 可以直接通过 Singleton.INSTANCE 来引用单例，非常简单的实现，并且既是线程安全的，同时也能应对反射与序列化的问题，面试官想要的估计就是它了吧。小雨再次提交了答案，这一次，面试官脸上的迷之微笑逐渐消失了…… Tips：为什么枚举是线程、反射、序列化安全的？枚举实际是通过一个继承自Enum的final类来实现（通过反编译class文件可看到具体实现），在static代码块中对其成员进行初始化，因此借助类加载机制来保障其线程安全枚举是不支持通过反射实例化的，在Constructor类的newInstance方法中可看到12if ((clazz.getModifiers() &amp; Modifier.ENUM) != 0) throw new IllegalArgumentException(\"Cannot reflectively create enum objects\");3. 枚举在序列化的时候仅仅是将枚举对象的name属性输出到结果中，反序列化的时候则是通过java.lang.Enum的valueOf方法来根据名字查找枚举对象。并且，编译器是不允许任何对这种序列化机制的定制的，禁用了writeObject、readObject、readObjectNoData、writeReplace和readResolve等方法。枚举通过这种机制保障了序列化安全。 总结枚举方案近乎“完美”，但实际中，大部分情况下，我们使用双重检查锁方案或静态内部类方案基本都能满足我们的场景并能很好地运行。并且方案从来没有“完美”，只有更好或更合适。本文只是从单例实现的不断演进的过程中，了解或回顾如反射、序列化、线程安全、Java内存模型（volatile语义）、JVM类加载机制、JVM指令重排序优化等方面的知识，同时也是启示我们在设计或实现的过程中，多从各个角度思考，尽可能全面地考虑问题。或者，在相关面试中能更好地迎合面试官的“完美”期望。","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.jboost.cn/categories/Java/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.jboost.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"Kubernetes笔记（三）：Gitlab+Jenkins Pipeline+Docker+k8s+Helm自动化部署实践（干货！）","slug":"k8s3-cd","date":"2020-05-09T03:53:23.000Z","updated":"2020-08-26T01:44:17.155Z","comments":true,"path":"k8s3-cd.html","link":"","permalink":"http://blog.jboost.cn/k8s3-cd.html","excerpt":"通过前面两篇文章，我们已经有了一个“嗷嗷待哺”的K8s集群环境，也对相关的概念与组件有了一个基本了解（前期对概念有个印象即可，因为只有实践了才能对其有深入理解，所谓“纸上得来终觉浅，绝知此事要躬行”），本文从实践角度介绍如何结合我们常用的Gitlab与Jenkins，通过K8s来实现项目的自动化部署，示例将包括基于SpringBoot的服务端项目与基于Vue.js的Web项目。","text":"通过前面两篇文章，我们已经有了一个“嗷嗷待哺”的K8s集群环境，也对相关的概念与组件有了一个基本了解（前期对概念有个印象即可，因为只有实践了才能对其有深入理解，所谓“纸上得来终觉浅，绝知此事要躬行”），本文从实践角度介绍如何结合我们常用的Gitlab与Jenkins，通过K8s来实现项目的自动化部署，示例将包括基于SpringBoot的服务端项目与基于Vue.js的Web项目。 本文涉及到的工具与技术包括： Gitlab —— 常用的源代码管理系统 Jenkins, Jenkins Pipeline —— 常用的自动化构建、部署工具，Pipeline以流水线的方式将构建、部署的各个步骤组织起来 Docker，Dockerfile —— 容器引擎，所有应用最终都要以Docker容器运行，Dockerfile是Docker镜像定义文件 Kubernetes —— Google开源的容器编排管理系统 Helm —— Kubernetes的包管理工具，类似Linux的yum，apt，或Node的npm等包管理工具，能将Kubernetes中的应用及相关依赖服务以包（Chart）的形式组织管理 环境背景： 已使用Gitlab做源码管理，源码按不同的环境建立了develop（对应开发环境），pre-release（对应测试环境），master（对应生产环境）分支 已搭建了Jenkins服务 已有Docker Registry服务，用于Docker镜像存储（基于Docker Registry或Harbor自建，或使用云服务，本文使用阿里云容器镜像服务） 已搭建了K8s集群 预期效果： 分环境部署应用，开发环境、测试环境、生产环境分开来，部署在同一集群的不同namespace，或不同集群中（比如开发测试部署在本地集群的不同namespace中，生产环境部署在云端集群） 配置尽可能通用化，只需要通过修改少量配置文件的少量配置属性，就能完成新项目的自动化部署配置 开发测试环境在push代码时自动触发构建与部署，生产环境在master分支上添加版本tag并且push tag后触发自动部署 整体交互流程如下图 项目配置文件首先我们需要在项目的根路径中添加一些必要的配置文件，如下图所示 包括： Dockerfile文件，用于构建Docker镜像的文件（参考 Docker笔记（十一）：Dockerfile详解与最佳实践） Helm相关配置文件，Helm是Kubernetes的包管理工具，可以将应用部署相关的Deployment，Service，Ingress等打包进行发布与管理（Helm的具体介绍我们后面再补充） Jenkinsfile文件，Jenkins的pipeline定义文件，定义了各个阶段需执行的任务 Dockerfile在项目根目录中添加一个Dockerfile文件（文件名就叫Dockerfile），定义如何构建Docker镜像，以Spring Boot项目为例， 123456789101112131415161718FROM frolvlad&#x2F;alpine-java:jdk8-slim#在build镜像时可以通过 --build-args profile&#x3D;xxx 进行修改ARG profileENV SPRING_PROFILES_ACTIVE&#x3D;$&#123;profile&#125;#项目的端口EXPOSE 8000 WORKDIR &#x2F;mnt#修改时区RUN sed -i &#39;s&#x2F;dl-cdn.alpinelinux.org&#x2F;mirrors.ustc.edu.cn&#x2F;g&#39; &#x2F;etc&#x2F;apk&#x2F;repositories \\ &amp;&amp; apk add --no-cache tzdata \\ &amp;&amp; ln -sf &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai &#x2F;etc&#x2F;localtime \\ &amp;&amp; echo &quot;Asia&#x2F;Shanghai&quot; &gt; &#x2F;etc&#x2F;timezone \\ &amp;&amp; apk del tzdata \\ &amp;&amp; rm -rf &#x2F;var&#x2F;cache&#x2F;apk&#x2F;* &#x2F;tmp&#x2F;* &#x2F;var&#x2F;tmp&#x2F;* $HOME&#x2F;.cacheCOPY .&#x2F;target&#x2F;your-project-name-1.0-SNAPSHOT.jar .&#x2F;app.jarENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;&#x2F;mnt&#x2F;app.jar&quot;] 将SPRING_PROFILES_ACTIVE通过参数profile暴露出来，在构建的时候可以通过 –build-args profile=xxx 来进行动态设定，以满足不同环境的镜像构建要求。 SPRING_PROFILES_ACTIVE本可以在Docker容器启动时通过docker run -e SPRING_PROFILES_ACTIVE=xxx来设定，因这里使用Helm进行部署不直接通过docker run运行，因此通过ARG在镜像构建时指定 Helm配置文件Helm是Kubernetes的包管理工具，将应用部署相关的Deployment，Service，Ingress等打包进行发布与管理（可以像Docker镜像一样存储于仓库中）。如上图中Helm的配置文件包括： 12345678910helm - chart包的目录名├── templates - k8s配置模版目录│ ├── deployment.yaml - Deployment配置模板，定义如何部署Pod│ ├── _helpers.tpl - 以下划线开头的文件，helm视为公共库定义文件，用于定义通用的子模版、函数、变量等│ ├── ingress.yaml - Ingress配置模板，定义外部如何访问Pod提供的服务，类似于Nginx的域名路径配置│ ├── NOTES.txt - chart包的帮助信息文件，执行helm install命令成功后会输出这个文件的内容│ └── service.yaml - Service配置模板，配置访问Pod的服务抽象，有NodePort与ClusterIp等|── values.yaml - chart包的参数配置文件，各模版文件可以引用这里的参数├── Chart.yaml - chart定义，可以定义chart的名字，版本号等信息├── charts - 依赖的子包目录，里面可以包含多个依赖的chart包，一般不存在依赖，我这里将其删除了 我们可以在Chart.yaml中定义每个项目的chart名称（类似安装包名），如 1234567apiVersion: v2name: your-chart-namedescription: A Helm chart for Kubernetestype: applicationversion: 1.0.0appVersion: 1.16.0 在values.yaml中定义模板文件中需要用到的变量，如 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#部署Pod的副本数，即运行多少个容器replicaCount: 1#容器镜像配置image: repository: registry.cn-hangzhou.aliyuncs.com/demo/demo pullPolicy: Always # Overrides the image tag whose default is the chart version. tag: \"dev\"#镜像仓库访问凭证imagePullSecrets: - name: aliyun-registry-secret#覆盖启动容器名称nameOverride: \"\"fullnameOverride: \"\"#容器的端口暴露及环境变量配置container: port: 8000 env: []#ServiceAccount，默认不创建serviceAccount: # Specifies whether a service account should be created create: false # Annotations to add to the service account annotations: &#123;&#125; name: \"\"podAnnotations: &#123;&#125;podSecurityContext: &#123;&#125; # fsGroup: 2000securityContext: &#123;&#125; # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsUser: 1000#使用NodePort的service，默认为ClusterIpservice: type: NodePort port: 8000#外部访问Ingress配置，需要配置hosts部分ingress: enabled: true annotations: &#123;&#125; # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" hosts: - host: demo.com paths: [\"/demo\"] tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local #.... 省略了其它默认参数配置 这里在默认生成的基础上添加了container部分，可以在这里指定容器的端口号而不用去改模板文件（让模板文件在各个项目通用，通常不需要做更改），同时添加env的配置，可以在helm部署时往容器里传入环境变量。将Service type从默认的ClusterIp改为了NodePort。部署同类型的不同项目时，只需要根据项目情况配置Chart.yaml与values.yaml两个文件的少量配置项，templates目录下的模板文件可直接复用。 部署时需要在K8s环境中从Docker镜像仓库拉取镜像，因此需要在K8s中创建镜像仓库访问凭证（imagePullSecrets） 123456# 登录Docker Registry生成/root/.docker/config.json文件sudo docker login --username=your-username registry.cn-shenzhen.aliyuncs.com# 创建namespace develop（我这里是根据项目的环境分支名称建立namespace）kubectl create namespace develop# 在namespace develop中创建一个secretkubectl create secret generic aliyun-registry-secret --from-file=.dockerconfigjson=/root/.docker/config.json --type=kubernetes.io/dockerconfigjson --namespace=develop JenkinsfileJenkinsfile是Jenkins pipeline配置文件，遵循Groovy语法，对于Spring Boot项目的构建部署， 编写Jenkinsfile脚本文件如下， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596image_tag = \"default\" //定一个全局变量，存储Docker镜像的tag（版本）pipeline &#123; agent any environment &#123; GIT_REPO = \"$&#123;env.gitlabSourceRepoName&#125;\" //从Jenkins Gitlab插件中获取Git项目的名称 GIT_BRANCH = \"$&#123;env.gitlabTargetBranch&#125;\" //项目的分支 GIT_TAG = sh(returnStdout: true,script: 'git describe --tags --always').trim() //commit id或tag名称 DOCKER_REGISTER_CREDS = credentials('aliyun-docker-repo-creds') //docker registry凭证 KUBE_CONFIG_LOCAL = credentials('local-k8s-kube-config') //开发测试环境的kube凭证 KUBE_CONFIG_PROD = \"\" //credentials('prod-k8s-kube-config') //生产环境的kube凭证 DOCKER_REGISTRY = \"registry.cn-hangzhou.aliyuncs.com\" //Docker仓库地址 DOCKER_NAMESPACE = \"your-namespace\" //命名空间 DOCKER_IMAGE = \"$&#123;DOCKER_REGISTRY&#125;/$&#123;DOCKER_NAMESPACE&#125;/$&#123;GIT_REPO&#125;\" //Docker镜像地址 INGRESS_HOST_DEV = \"dev.your-site.com\" //开发环境的域名 INGRESS_HOST_TEST = \"test.your-site.com\" //测试环境的域名 INGRESS_HOST_PROD = \"prod.your-site.com\" //生产环境的域名 &#125; parameters &#123; string(name: 'ingress_path', defaultValue: '/your-path', description: '服务上下文路径') string(name: 'replica_count', defaultValue: '1', description: '容器副本数量') &#125; stages &#123; stage('Code Analyze') &#123; agent any steps &#123; echo \"1. 代码静态检查\" &#125; &#125; stage('Maven Build') &#123; agent &#123; docker &#123; image 'maven:3-jdk-8-alpine' args '-v $HOME/.m2:/root/.m2' &#125; &#125; steps &#123; echo \"2. 代码编译打包\" sh 'mvn clean package -Dfile.encoding=UTF-8 -DskipTests=true' &#125; &#125; stage('Docker Build') &#123; agent any steps &#123; echo \"3. 构建Docker镜像\" echo \"镜像地址： $&#123;DOCKER_IMAGE&#125;\" //登录Docker仓库 sh \"sudo docker login -u $&#123;DOCKER_REGISTER_CREDS_USR&#125; -p $&#123;DOCKER_REGISTER_CREDS_PSW&#125; $&#123;DOCKER_REGISTRY&#125;\" script &#123; def profile = \"dev\" if (env.gitlabTargetBranch == \"develop\") &#123; image_tag = \"dev.\" + env.GIT_TAG &#125; else if (env.gitlabTargetBranch == \"pre-release\") &#123; image_tag = \"test.\" + env.GIT_TAG profile = \"test\" &#125; else if (env.gitlabTargetBranch == \"master\")&#123; // master分支则直接使用Tag image_tag = env.GIT_TAG profile = \"prod\" &#125; //通过--build-arg将profile进行设置，以区分不同环境进行镜像构建 sh \"docker build --build-arg profile=$&#123;profile&#125; -t $&#123;DOCKER_IMAGE&#125;:$&#123;image_tag&#125; .\" sh \"sudo docker push $&#123;DOCKER_IMAGE&#125;:$&#123;image_tag&#125;\" sh \"docker rmi $&#123;DOCKER_IMAGE&#125;:$&#123;image_tag&#125;\" &#125; &#125; &#125; stage('Helm Deploy') &#123; agent &#123; docker &#123; image 'lwolf/helm-kubectl-docker' args '-u root:root' &#125; &#125; steps &#123; echo \"4. 部署到K8s\" sh \"mkdir -p /root/.kube\" script &#123; def kube_config = env.KUBE_CONFIG_LOCAL def ingress_host = env.INGRESS_HOST_DEV if (env.gitlabTargetBranch == \"pre-release\") &#123; ingress_host = env.INGRESS_HOST_TEST &#125; else if (env.gitlabTargetBranch == \"master\")&#123; ingress_host = env.INGRESS_HOST_PROD kube_config = env.KUBE_CONFIG_PROD &#125; sh \"echo $&#123;kube_config&#125; | base64 -d &gt; /root/.kube/config\" //根据不同环境将服务部署到不同的namespace下，这里使用分支名称 sh \"helm upgrade -i --namespace=$&#123;env.gitlabTargetBranch&#125; --set replicaCount=$&#123;params.replica_count&#125; --set image.repository=$&#123;DOCKER_IMAGE&#125; --set image.tag=$&#123;image_tag&#125; --set nameOverride=$&#123;GIT_REPO&#125; --set ingress.hosts[0].host=$&#123;ingress_host&#125; --set ingress.hosts[0].paths=&#123;$&#123;params.ingress_path&#125;&#125; $&#123;GIT_REPO&#125; ./helm/\" &#125; &#125; &#125; &#125;&#125; Jenkinsfile定义了整个自动化构建部署的流程： Code Analyze，可以使用SonarQube之类的静态代码分析工具完成代码检查，这里先忽略 Maven Build，启动一个Maven的Docker容器来完成项目的maven构建打包，挂载maven本地仓库目录到宿主机，避免每次都需要重新下载依赖包 Docker Build，构建Docker镜像，并推送到镜像仓库，不同环境的镜像通过tag区分，开发环境使用dev.commitId的形式，如dev.88f5822，测试环境使用test.commitId，生产环境可以将webhook事件设置为tag push event，直接使用tag名称 Helm Deploy，使用helm完成新项目的部署，或已有项目的升级，不同环境使用不同的参数配置，如访问域名，K8s集群的访问凭证kube_config等 Jenkins配置Jenkins任务配置在Jenkins中创建一个pipeline的任务，如图 配置构建触发器，将目标分支设置为develop分支，生成一个token，如图 记下这里的“GitLab webhook URL”及token值，在Gitlab配置中使用。 配置流水线，选择“Pipeline script from SCM”从项目源码中获取pipeline脚本文件，配置项目Git地址，拉取源码凭证等，如图 保存即完成了项目开发环境的Jenkins配置。测试环境只需将对应的分支修改为pre-release即可 Jenkins凭据配置在Jenkinsfile文件中，我们使用到了两个访问凭证——Docker Registry凭证与本地K8s的kube凭证， 12DOCKER_REGISTER_CREDS = credentials('aliyun-docker-repo-creds') //docker registry凭证KUBE_CONFIG_LOCAL = credentials('local-k8s-kube-config') //开发测试环境的kube凭证 这两个凭证需要在Jenkins中创建。 添加Docker Registry登录凭证,在Jenkins 凭据页面，添加一个用户名密码类型的凭据，如图 添加K8s集群的访问凭证，在master节点上将/root/.kube/config文件内容进行base64编码， 12base64 /root/.kube/config &gt; kube-config-base64.txtcat kube-config-base64.txt 使用编码后的内容在Jenkins中创建一个Secret text类型的凭据，如图 在Secret文本框中输入base64编码后的内容。 Gitlab配置在Gitlab项目的 Settings - Integrations 页面配置一个webhook，在URL与Secret Token中填入前面Jenkins触发器部分的“GitLab webhook URL”及token值，选中“Push events”作为触发事件，如图 开发、测试环境选择“Push events”则在开发人员push代码，或merge代码到develop，pre-release分支时，就会触发开发或测试环境的Jenkins pipeline任务完成自动化构建；生产环境选择“Tag push events”，在往master分支push tag时触发自动化构建。如图为pipeline构建视图 总结本文介绍使用Gitlab+Jenkins Pipeline+Docker+Kubernetes+Helm来实现Spring Boot项目的自动化部署，只要稍加修改即可应用于其它基于Spring Boot的项目（具体修改的地方在源码的Readme文件中说明）。 本文涉及的所有配置文件（包括基于Spring Boot的服务端项目与基于Vue.js的Web项目）可在源码项目中获取（源码地址获取办法：关注公众号“半路雨歌”，首页输入“k8sops”即可）。 原文地址：http://blog.jboost.cn/k8s3-cd.html","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.jboost.cn/categories/Kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.jboost.cn/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"http://blog.jboost.cn/tags/k8s/"},{"name":"helm","slug":"helm","permalink":"http://blog.jboost.cn/tags/helm/"},{"name":"jenkins","slug":"jenkins","permalink":"http://blog.jboost.cn/tags/jenkins/"}]},{"title":"Kubernetes笔记（二）：了解k8s的基本组件与概念","slug":"k8s2-concept","date":"2020-05-06T13:13:53.000Z","updated":"2020-08-26T01:44:14.116Z","comments":true,"path":"k8s2-concept.html","link":"","permalink":"http://blog.jboost.cn/k8s2-concept.html","excerpt":"前文 Kubernetes笔记（一）：十分钟部署一套K8s环境 介绍了如何快速搭建一个k8s系统。为了继续使用k8s来部署我们的应用，需要先对k8s中的一些基本组件与概念有个了解。","text":"前文 Kubernetes笔记（一）：十分钟部署一套K8s环境 介绍了如何快速搭建一个k8s系统。为了继续使用k8s来部署我们的应用，需要先对k8s中的一些基本组件与概念有个了解。 Kubernetes是什么Kubernetes是Google于2014年基于其内部Brog系统开源的一个容器编排管理系统，可使用声明式的配置（以yaml文件的形式）自动地执行容器化应用程序的管理，包括部署、伸缩、负载均衡、回滚等。 kubernetes提供的功能： 自动发布与伸缩：可以通过声明式的配置文件定义想要部署的容器，Kubernetes将自动进行容器的部署，达到期望的结果；通过指定容器副本数，或者设置根据资源负载情况（如CPU、内存使用率），自动对容器组进行快速的伸缩——增大或缩小容器数量 滚动升级与灰度发布：采用逐步替换的策略实现滚动升级，使用Kubernetes也可以很轻易地管理系统的灰度发布 服务发现与负载均衡：Kubernetes通过DNS名称或IP地址暴露容器的访问方式，并且可在同一容器组内实现负载分发与均衡 存储编排：Kubernetes可以自动挂载指定的存储系统，如local storage/nfs/云存储等 故障恢复：Kubernetes自动重启已经停机的容器，替换不满足健康检查的容器 密钥与配置管理：Kubernetes可以存储与管理敏感信息，如Docker Registry的登录凭证，密码，ssh密钥等 Kubernetes架构我们先来看一张Kubernetes的架构图 Kubernetes是一套分布式系统， 与大多数分布式系统类似，包含控制节点（master node）与工作节点（worker node）。 master node控制节点就是指挥官，负责发号施令的，其上运行一些管理服务来对整个系统进行管理与控制，包括 apiserver：作为整个系统的对外接口，提供一套Restful API供客户端调用，任何的资源请求/调用操作都是通过kube-apiserver提供的接口进行,如kubectl、kubernetes dashboard等管理工具就是通过apiserver来实现对集群的管理 kube-scheduler：资源调度器，负责将容器组分配到哪些节点上 kube-controller-manager：管理控制器，集群中处理常规任务的后台线程，包括节点控制器（负责监听节点停机的事件并作出对应响应）、endpoint-controller（刷新服务与容器组的关联信息）、replication-controller（维护容器组的副本数为指定的数值）、Service Account &amp; Token控制器（负责为新的命名空间创建默认的 Service Account 以及 API Access Token） etcd：数据存储，存储集群所有的配置信息 coredns：实现集群内部通过服务名称进行容器组访问的功能 worker node工作节点就是具体干活的小兵，其上也运行一些服务来执行指挥官分派的任务，包括 kubelet：是工作节点上执行操作的代理程序，负责容器的生命周期管理，定期执行容器健康检查，并上报容器的运行状态 kube-proxy：是一个具有负载均衡能力的简单的网络访问代理，负责将访问某个服务的请求分配到工作节点的具体某个容器上（kube-proxy也运行于master node上） Docker Daemon：这个不难理解，所有服务或容器组都要以Docker容器的形式来运行（但Kubernetes其实不局限于Docker，它支持任何实现了Kubernetes容器引擎接口的容器引擎，如containerd、rktlet） 另外还有既在master node上也在worker node上运行的网络通信组件 kube-flannel。这些服务组件一般运行在kube-system的命名空间中，如图 Kubernetes基本概念我们再来看第二张图 功能组件在上面已经做了介绍。Kubernetes的操作对象主要包括容器组（Pod），服务（Service），副本控制器（replication-controller），及围绕这些的其它辅助对象 PodPod是Kubernetes创建或部署的最小基本单元。一个Pod封装一个或多个应用容器、存储资源、一个独立的网络IP以及管理控制容器运行方式的策略选项。Pod中的每个容器共享网络命名空间（包括IP与端口），Pod内的容器可以使用localhost相互通信。Pod可以指定一组共享存储卷Volumes，Pod中所有容器都可以访问共享的Volumes，Volumes用于数据持久化，防止容器重启丢失数据。 VolumeKubernetes使用Volume来解决Pod中容器重启数据丢失的问题，以及Pod中多个容器间数据共享的问题。Kubernetes支持的Volume类型包括： emptyDir：当Pod分配到Node上时，将会创建emptyDir，只要Node上的Pod一直运行，Volume就会一直存在。当Pod（不管任何原因）从Node上被删除时，emptyDir也同时会删除，存储的数据也将永久删除，但删除容器不影响emptyDir hostPath：hostPath允许挂载Node上的文件系统到Pod里面去。如果Pod需要使用Node上的文件，可以使用hostPath nfs: 使用nfs网络文件系统提供的共享目录 ReplicationControllerReplicationController确保在任何时候都有按配置的Pod副本数在运行。现在推荐使用配置ReplicaSet（下一代ReplicationController）的Deployment来建立副本管理机制。 ReplicaSetReplicaSet是下一代ReplicationController，两者的唯一区别是ReplicaSet支持新的基于集合的选择器，而ReplicationController仅支持基于相等选择器的需求。 DeploymentDeployment为Pod与ReplicaSet提供了声明式的定义，描述你想要的目标状态是什么，Deployment controller就会帮你将Pod与ReplicaSet的实际状态改变到你想要的目标状态。 Service一个Service可以看做一组提供相同服务的Pod的对外访问接口。Kubernetes提供两种类型的Service： NodePort： 集群外部可以通过Node IP与Node Port来访问具体某个Pod ClusterIP：指通过集群的内部IP暴露服务，服务只能够在集群内部可以访问，这也是默认的 ServiceType LabelLabel就是一对key/value，可以附加到各种资源对象上，如Node、Pod、Service等，一个资源对象可以定义任意数量的Label。可以通过Label选择器来选择具备某个（些）Label的资源。 PV &amp; PVCPersistentVolume（PV） 为用户提供了一个存储抽象，由管理员设置，它是集群的一部分。就像节点是集群中的资源一样，PV也是集群中的资源。 PV是Volume之类的卷插件，但具有独立于Pod的生命周期。 PersistentVolumeClaim（PVC）是用户存储的请求。它与Pod相似。Pod消耗节点资源，PVC消耗PV资源。Pod可以请求特定级别的资源（CPU和内存）。PVC可以请求特定大小和访问模式的存储资源（例如，可以以读/写或只读模式挂载）。 SecretSecret解决了密码、token、密钥等敏感数据的存储问题，Secret的三种类型： Service Account ：用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的/run/secrets/kubernetes.io/serviceaccount目录中 Opaque ：Base64编码格式的Secret，用来存储密码、密钥等 kubernetes.io/dockerconfigjson ：用来存储docker registry的认证信息 ConfigMapConfigMap用来保存key/value对的配置数据，这个数据可以在Pods里使用，或者被用来为像controller一样的系统组件存储配置数据。ConfigMap可以方便的处理不含敏感信息的字符串（敏感信息可使用Secret）。 NamespaceNamespace类似于Kubernetes中的虚拟集群，便于不同的分组在共享使用整个集群的资源的同时还能被分别管理。比如我们如果开发测试共用一个Kubernetes集群，则可以将开发环境的服务部署到dev的namespace，测试环境的部署到test的namespace。 Ingress为集群服务提供外部访问，包括基于Nginx与Traefik两个版本，为服务提供域名绑定访问与路径路由功能。也可以基于Ingress实现服务的灰度发布。 总结本文对Kubernetes中涉及的基本组件与概念进行了整理，对其基本构成有了一个大致的理解与印象。下一篇将从一个实践出发，实现一个基于Gitlab+Jenkins+K8s的CI/CD流程，以对涉及的各个组件进行深入了解与学习。 相关阅读： Kubernetes笔记（一）：十分钟部署一套K8s环境","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.jboost.cn/categories/Kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.jboost.cn/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"http://blog.jboost.cn/tags/k8s/"}]},{"title":"Kubernetes笔记（一）：十分钟部署一套K8s环境","slug":"k8s1-installation","date":"2020-04-28T00:21:03.000Z","updated":"2020-08-26T01:44:10.469Z","comments":true,"path":"k8s1-installation.html","link":"","permalink":"http://blog.jboost.cn/k8s1-installation.html","excerpt":"Kubernetes是Goole开源的一个容器编排引擎，它支持自动化部署、大规模可伸缩、应用容器化管理 —— 百度百科。 接触K8s也有半年多了，也基于阿里云平台搭建了包含多级服务、目前运行较为稳定的K8s集群（感兴趣的可参考 k8s云集群混搭模式，可能帮你节省50%以上的服务成本， k8s云集群混搭模式落地分享），但一直没来得及对其进行系统的学习，本系列文章还像以前Docker系列一样，以笔记的形式进行记录与分享，会包括理论与实践，感兴趣的同学可以关注，一起探索下目前较为流行的容器化及服务编排解决方案。","text":"Kubernetes是Goole开源的一个容器编排引擎，它支持自动化部署、大规模可伸缩、应用容器化管理 —— 百度百科。 接触K8s也有半年多了，也基于阿里云平台搭建了包含多级服务、目前运行较为稳定的K8s集群（感兴趣的可参考 k8s云集群混搭模式，可能帮你节省50%以上的服务成本， k8s云集群混搭模式落地分享），但一直没来得及对其进行系统的学习，本系列文章还像以前Docker系列一样，以笔记的形式进行记录与分享，会包括理论与实践，感兴趣的同学可以关注，一起探索下目前较为流行的容器化及服务编排解决方案。 工欲善其事，必先利其器，本文先介绍如何在本地自行搭建一套k8s集群，并且采用我们前面介绍过的Ansible来提高效率（参考 Ansible简明教程） 本文所涉及的所有配置文件可在这里找到 github 一. 准备服务器节点如果还没有服务器，可以参考 ubuntu18.04上搭建KVM虚拟机环境超完整过程 创建虚拟服务器。 服务器节点IP（hostname）： 192.168.40.111 (kmaster) 192.168.40.112 (knode1) 192.168.40.113 (knode2) 192.168.40.114 (knode3) 操作系统版本： cat /etc/redhat-release : CentOS Linux release 7.6.1810 (Core) uname -a : 3.10.0-957.el7.x86_64 二. 配置Ansible如果还没有Ansible环境，可以参考 [Ansible简明教程]https://mp.weixin.qq.com/s/JIZE1RvN7Yop5dsOHJvStw) 搭建。 1.在Ansible服务器上的/etc/hosts文件中添加k8s服务器节点信息(参考 hosts) 1234192.168.40.111 kmaster192.168.40.112 knode1192.168.40.113 knode2192.168.40.114 knode3 2.在Ansible服务器上的/etc/ansible/hosts文件中添加k8s服务器节点（参考 ansible_hosts） 12345678910111213[k8s-all]kmasterknode1knode2knode3[k8s-master]kmaster[k8s-nodes]knode1knode2knode3 三. 修改k8s集群各节点/etc/hosts（非必须）修改所有主机/etc/hosts文件，添加IP/主机名映射，方便通过主机名ssh访问 1.创建playbook文件（参考 set_hosts_playbook.yml） 1234567891011vim set_hosts_playbook.yml---- hosts: k8s-all remote_user: root tasks: - name: backup /etc/hosts shell: mv /etc/hosts /etc/hosts_bak - name: copy local hosts file to remote copy: src=/etc/hosts dest=/etc/ owner=root group=root mode=0644 2.执行ansible-playbook 1ansible-playbook set_hosts_playbook.yml 四. 安装Docker在所有主机上安装Docker 1.创建playbook文件（参考 install_docker_playbook.yml） 1234567891011121314151617181920212223vim install_docker_playbook.yml- hosts: k8s-all remote_user: root vars: docker_version: 18.09.2 tasks: - name: install dependencies #shell: yum install -y yum-utils device-mapper-persistent-data lvm2 yum: name=&#123;&#123;item&#125;&#125; state=present with_items: - yum-utils - device-mapper-persistent-data - lvm2 - name: config yum repo shell: yum-config-manager --add-repo https://mirrors.ustc.edu.cn/docker-ce/linux/centos/docker-ce.repo - name: install docker yum: name=docker-ce-&#123;&#123;docker_version&#125;&#125; state=present - name: start docker shell: systemctl enable docker &amp;&amp; systemctl start docker 2.执行ansible-playbook 1ansible-playbook install_docker_playbook.yml 五. 部署k8s master1.开始部署之前，需要做一些初始化处理：关闭防火墙、关闭selinux、禁用swap、配置k8s阿里云yum源等，所有操作放在脚本 pre-setup.sh 中，并在2中playbook中通过script模块执行 2.创建playbook文件 deploy_master_playbook.yml，只针对master节点，安装kubectl，kubeadm，kubelet，以及flannel（将kube-flannel.yml文件里镜像地址的quay.io改为quay-mirror.qiniu.com避免超时，参考 kube-flannel.yml） 123456789101112131415161718192021222324252627282930313233343536vim deploy_master_playbook.yml- hosts: k8s-master remote_user: root：q vars: kube_version: 1.16.0-0 k8s_version: v1.16.0 k8s_master: 192.168.40.111 tasks: - name: prepare env script: ./pre-setup.sh - name: install kubectl,kubeadm,kubelet yum: name=&#123;&#123;item&#125;&#125; state=present with_items: - kubectl-&#123;&#123;kube_version&#125;&#125; - kubeadm-&#123;&#123;kube_version&#125;&#125; - kubelet-&#123;&#123;kube_version&#125;&#125; - name: init k8s shell: kubeadm init --image-repository registry.aliyuncs.com/google_containers --kubernetes-version &#123;&#123;k8s_version&#125;&#125; --apiserver-advertise-address &#123;&#123;k8s_master&#125;&#125; --pod-network-cidr=10.244.0.0/16 --token-ttl 0 - name: config kube shell: mkdir -p $HOME/.kube &amp;&amp; cp -i /etc/kubernetes/admin.conf $HOME/.kube/config &amp;&amp; chown $(id -u):$(id -g) $HOME/.kube/config - name: copy flannel yaml file copy: src=./kube-flannel.yml dest=/tmp/ owner=root group=root mode=0644 - name: install flannel shell: kubectl apply -f /tmp/kube-flannel.yml - name: get join command shell: kubeadm token create --print-join-command register: join_command - name: show join command debug: var=join_command verbosity=0 3.执行ansible-playbook 1ansible-playbook deploy_master_playbook.yml 4.上述命令执行完成会输出节点加入k8s集群的命令，如下图。记下该命令，后面部署node时会用到 六. 部署k8s node1.同master一样，开始部署之前，需要做一些初始化处理：关闭防火墙、关闭selinux、禁用swap、配置k8s阿里云yum源等，所有操作放在脚本 pre-setup.sh 中，并在2中playbook中通过script模块执行 2.创建playbook文件 deploy_nodes_playbook.yml，针对除master外的其它集群节点，安装kubeadm，kubelet，并将节点加入到k8s集群中，使用的是前面部署master时输出的加入集群命令 123456789101112131415161718192021vim deploy_nodes_playbook.yml- hosts: k8s-nodes remote_user: root vars: kube_version: 1.16.0-0 tasks: - name: prepare env script: ./pre-setup.sh - name: install kubeadm,kubelet yum: name=&#123;&#123;item&#125;&#125; state=present with_items: - kubeadm-&#123;&#123;kube_version&#125;&#125; - kubelet-&#123;&#123;kube_version&#125;&#125; - name: start kubelt shell: systemctl enable kubelet &amp;&amp; systemctl start kubelet - name: join cluster shell: kubeadm join 192.168.40.111:6443 --token zgx3ov.zlq3jh12atw1zh8r --discovery-token-ca-cert-hash sha256:60b7c62687974ec5803e0b69cfc7ccc2c4a8236e59c8e8b8a67f726358863fa7 3.执行ansible-playbook 1ansible-playbook deploy_nodes_playbook.yml 4.稍等片刻，即可在master节点上通过kubectl get nodes看到加入到集群中的节点，并且status为Ready状态，如下 123456[root@kmaster ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONkmaster Ready master 37m v1.16.0knode1 Ready &lt;none&gt; 7m1s v1.16.0knode2 Ready &lt;none&gt; 7m1s v1.16.0knode3 Ready &lt;none&gt; 4m12s v1.16.0 至此，k8s集群基本部署完成。接下来可安装Ingress与Dashboard。 七. 安装IngressIngress为集群内服务提供外网访问，包括基于Nginx与Traefik两个版本，这里使用比较熟悉的Nginx版本。安装Ingress的操作在master节点进行（因为前面在master节点安装并配置了kubectl，也可在其它安装并配置好了kubectl的节点进行） 1.下载yaml文件（此目录已包含 nginx-ingress.yaml，并修改了镜像地址，可直接进入第3步） 1wget -O nginx-ingress.yaml https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/deploy.yaml 2.将里面的quay.io修改为quay-mirror.qiniu.com，避免镜像拉取超时。同时在nginx-ingress-controller的Deployment上添加hostNetwork为true及nginx-ingress的标签，以使用宿主机网络与控制Ingress部署的节点 12345678910vim nginx-ingress.yaml:s/quay.io/quay-mirror.qiniu.com/gvim nginx-ingress.yaml spec: hostNetwork: true nodeSelector: nginx-ingress: \"true\" 3.部署Ingress 首先在knode1节点上打标签nginx-ingress=true，控制Ingress部署到knode1上，保持IP固定。 12[root@kmaster k8s-deploy]# kubectl label node knode1 nginx-ingress=truenode/knode1 labeled 然后完成nginx-ingress的部署 1kubectl apply -f nginx-ingress.yaml 4.部署完成，稍等片刻等Pod创建完成，可通过如下命令查看ingress相关Pod情况 12345[root@kmaster k8s-deploy]# kubectl get pods -n ingress-nginx -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESingress-nginx-admission-create-drpg5 0/1 Completed 0 79m 10.244.2.2 knode1 &lt;none&gt; &lt;none&gt;ingress-nginx-admission-patch-db2rt 0/1 Completed 1 79m 10.244.3.2 knode3 &lt;none&gt; &lt;none&gt;ingress-nginx-controller-575cffb49c-4xm55 1/1 Running 0 79m 192.168.40.112 knode1 &lt;none&gt; &lt;none&gt; 八. 安装Kubernetes Dashboard1.下载yaml文件（此目录已包含 kubernetes-dashboard.yaml 文件，可直接进入第3步） 1wget -O kubernetes-dashboard.yaml https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta5/aio/deploy/recommended.yaml 2.修改kubernetes-dashboard.yaml 将Service type改为NodePort，使得可通过IP访问Dashboard。注释掉默认的Secret（默认的secret权限很有限，看不到多少数据） 123456789101112131415kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 30443 selector: k8s-app: kubernetes-dashboard 3.部署Dashboard，并创建绑定cluster-admin角色的ServiceAccount —— admin-user (参考 auth.yaml) 12kubectl apply -f kubernetes-dashboard.yamlkubectl apply -f kubernetes-dashboard-auth.yaml 4.访问Dashboard 访问 https://集群任意节点IP:30443，打开Dashboard登录页面，执行如下命令获取登录token 1kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '&#123;print $1&#125;') 使用token完成登录，如图 九. 解决证书无效问题安装完后，默认的证书可能无效，在Chrome浏览中无法打开Dashboard，可通过重新生成证书解决。 1.创建自定义证书 123456789[root@kmaster ~]# cd /etc/kubernetes/pki/#生成私钥[root@kmaster pki]# openssl genrsa -out dashboard.key 2048#生成证书[root@kmaster pki]# openssl req -new -key dashboard.key -out dashboard.csr -subj \"/O=JBST/CN=kubernetes-dashboard\"#使用集群的CA来签署证书[root@kmaster pki]# openssl x509 -req -in dashboard.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out dashboard.crt -days 3650#查看自创证书[root@kmaster pki]# openssl x509 -in dashboard.crt -noout -text 2.注释 kubernetes-dashboard.yaml 中默认的Secret， 12345678910#---##apiVersion: v1#kind: Secret#metadata:# labels:# k8s-app: kubernetes-dashboard# name: kubernetes-dashboard-certs# namespace: kubernetes-dashboard#type: Opaque 3.重新部署Dashboard，并通过自定义证书创建新的Secret 123[root@kmaster k8s-deploy]# kubectl delete -f kubernetes-dashboard.yaml[root@kmaster k8s-deploy]# kubectl apply -f kubernetes-dashboard.yaml [root@kmaster k8s-deploy]# kubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.crt=/etc/kubernetes/pki/dashboard.crt --from-file=dashboard.key=/etc/kubernetes/pki/dashboard.key -n kubernetes-dashboard 十. 在本地（win10）管理k8s集群1.下载kubectl windows版本： https://storage.googleapis.com/kubernetes-release/release/v1.16.0/bin/windows/amd64/kubectl.exe 2.将kubectl.exe文件所在目录加入系统环境变量的Path中 3.将master节点上 /etc/kubernetes/admin.conf 的内容拷贝到本地用户目录的 .kube/config 文件中，如 C:\\Users\\Administrator\\.kube\\config 4.验证 123456C:\\Users\\Administrator&gt;kubectl get nodesNAME STATUS ROLES AGE VERSIONkmaster Ready master 4d19h v1.16.0knode1 Ready &lt;none&gt; 4d19h v1.16.0knode2 Ready &lt;none&gt; 4d19h v1.16.0knode3 Ready &lt;none&gt; 4d19h v1.16.0 本文所涉及的所有配置文件可在这里找到 github 相关阅读： k8s云集群混搭模式，可能帮你节省50%以上的服务成本 k8s云集群混搭模式落地分享 Ansible简明教程","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.jboost.cn/categories/Kubernetes/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://blog.jboost.cn/tags/k8s/"},{"name":"ansible","slug":"ansible","permalink":"http://blog.jboost.cn/tags/ansible/"}]},{"title":"Ansible简明教程","slug":"ansible","date":"2020-04-17T00:39:52.000Z","updated":"2020-04-18T06:28:32.952Z","comments":true,"path":"ansible.html","link":"","permalink":"http://blog.jboost.cn/ansible.html","excerpt":"Ansible是当下比较流行的自动化运维工具，可通过SSH协议对远程服务器进行集中化的配置管理、应用部署等，常结合Jenkins来实现自动化部署。","text":"Ansible是当下比较流行的自动化运维工具，可通过SSH协议对远程服务器进行集中化的配置管理、应用部署等，常结合Jenkins来实现自动化部署。 除了Ansible，还有像SaltStack、Fabric（曾经管理100多台服务器上的应用时也曾受益于它）、Puppet等自动化工具。相比之下，Ansible最大的优势就是无需在被管理主机端部署任何客户端代理程序，通过SSH通道就可以进行远程命令的执行或配置的下发，足够轻量级，但同时功能非常强大，且各项功能通过模块来实现，具备良好的扩展性。不足之处是Ansible只支持在Linux系统上安装，不支持Windows。 如果你需要在多于一台服务器上做相同的操作，那么建议你使用Ansible之类的自动化工具，这将极大提高你的操作效率。 环境搭建1.找一台主机用于做管理服务器，在其上安装Ansible 1yum -y install ansible Ansible基于Python实现，一般Linux系统都自带Python，所以可以直接使用yum安装或pip安装。 安装完后，在/etc/ansible/目录下生成三个主要的文件或目录， 12345[root@tool-server ~]# ll /etc/ansible/total 24-rw-r--r--. 1 root root 19179 Jan 30 2018 ansible.cfg-rw-r--r--. 1 root root 1136 Apr 17 15:17 hostsdrwxr-xr-x. 2 root root 6 Jan 30 2018 roles ansible.cfg： Ansible的配置文件 hosts：登记被管理的主机 roles：角色项目定义目录，主要用于代码复用 2.在/etc/ansible/hosts文件中添加需要被管理的服务器节点 123456[root@tool-server ~]# vim /etc/ansible/hosts[k8s]192.168.40.201192.168.40.202192.168.40.205192.168.40.206 [k8s]表示将下面的服务器节点分到k8s的组中，后面执行命令时可指定针对某个组执行。 3.生成SSH KEY，并copy到被管理节点上，实现免密SSH访问 在管理节点执行 ssh-keygen 生成SSH KEY，然后copy到各被管理节点上 1ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.40.201 上面命令将~/.ssh/id_rsa.pub文件内容添加到被管理节点的/root/.ssh/authorized_keys文件中，实现管理节点到被管理节点的免密SSH访问。 4.调试Ansible 针对k8s服务器组执行ping，验证Ansible到各被管理节点的连通性 1234567891011121314151617[root@tool-server ~]# ansible k8s -m ping192.168.40.201 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125;192.168.40.205 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125;192.168.40.202 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125;192.168.40.206 | SUCCESS =&gt; &#123; \"changed\": false, \"ping\": \"pong\"&#125; Ansible只需要在管理主机上安装，然后打通管理主机到各被管理主机的SSH免密访问即可进行集中化的管理控制，不需在被管理主机安装任何代理程序。 Ansible命令Ansible的命令格式为， ansible 主机群组名 -m 命令模块名 -a &quot;批量执行的操作&quot; 其中-m不是必须的，默认为command模块，-a也不是必须的，表示命令模块的参数，比如前面的ping模块就没有参数。 可以使用 ansible-doc -l 列出所有可用的命令模块， ansible-doc -s 模块名 查看指定模块的参数信息 常用命令模块 1.commandcommand是Ansible的默认模块，不指定-m参数时默认使用command。command可以运行远程主机权限范围内的所有shell命令，但不支持管道操作 12# 查看k8s分组主机内存使用情况ansible k8s -m command -a \"free -g\" 2.shellshell基本与command相同，但shell支持管道操作 12#shell支持管道操作 |grep Memansible k8s -m shell -a \"free -g|grep Mem\" 3.scriptscript就是在远程主机上执行管理端存储的shell脚本文件，相当于scp+shell 12# /root/echo.sh为管理端本地shell脚本ansible k8s -m script -a \"/root/echo.sh\" 4.copycopy实现管理端到远程主机的文件拷贝，相当于scp 12#拷贝本地echo.sh文件到k8s组中远程主机的/tmp目录下，所属用户、组为 root ，权限为 0755ansible k8s -m copy -a \"src=/root/echo.sh dest=/tmp/ owner=root group=root mode=0755\" 5.yum软件包安装或删除 1ansible k8s -m yum -a \"name=wget state=latest\" 其中state有如下取值： 针对安装，可取值“present，installed，latest”，present，installed即普通安装，两者无区别，latest是使用yum mirror上最新的版本进行安装 针对删除，可取值“absent，removed”，两者无差别 6.service对远程主机的服务进行管理 1ansible k8s -m service -a \"name=nginx state=stoped\" state可取值“started/stopped/restarted/reloaded”。 7.get_url在远程主机上下载指定URL到本地 1ansible k8s -m get_url -a \"url=http://www.baidu.com dest=/tmp/index.html mode=0440 force=yes\" 8.setup获取远程主机的信息 1ansible k8s -m setup 9.file管理远程主机的文件或目录 1ansible k8s -m file -a \"dest=/opt/test state=touch\" state可取值 directory：创建目录 file：如果文件不存在，则创建 link：创建symbolic link absent：删除文件或目录 touch： 创建一个不存在的空文件 10.cron管理远程主机的crontab定时任务 1ansible k8s -m cron -a \"name='backup servcie' minute=*/5 job='/usr/sbin/ntpdate time.nist.gov &gt;/dev/null 2&gt;&amp;1'\" 支持的参数 state： 取值present表示创建定时任务，absent表示删除定时任务 disabled： yes表示注释掉定时任务，no表示接触注释 Ansible playbookAnsible的playbook由一个或多个play组成，play的功能就是为归为一组的主机编排要执行的一系列task，其中每一个task就是调用Ansible的一个命令模块。 playbook的核心元素包括： hosts：执行任务的远程主机组或列表 tasks：要执行的任务列表 variables：内置变量或自定义的变量 templates：使用模板语法的文件，通常为配置文件 handlers：和notify结合使用，由特定条件触发，一般用于配置文件变更触发服务重启 tags：标签，可在运行时通过标签指定运行playbook中的部分任务 roles： playbook文件遵循yaml的语法格式，运行命令的格式为 ansible-playbook &lt;filename.yml&gt; ... [options]， 常用options包括 –syntax 检查playbook文件语法是否正确 –check 或 -C 只检测可能会发生的改变，但不真正执行操作 –list-hosts 列出运行任务的主机 –list-tags 列出playbook文件中定义所有的tags –list-tasks 列出playbook文件中定义的所有任务集 –limit 只针对主机列表中的某个主机或者某个组执行 -f 指定并发数，默认为5个 -t 指定某个或多个tags运行（前提playbook中有定义tags） -v 显示过程 -vv -vvv更详细 下面以批量安装Nginx为例，尽可能介绍playbook各核心元素的用法。 定义palybook yaml文件nginx_playbook.yml 12345678910111213141516171819202122232425262728293031323334353637---- hosts: 192.168.40.201,192.168.40.205 # 主机列表，也可以是/etc/ansible/hosts中定义的主机分组名 remote_user: root # 远程用户 vars: # 自定义变量 version: 1.16.1 vars_files: - ./templates/nginx_locations_vars.yml tasks: - name: install dependencies # 定义任务的名称 yum: name=&#123;&#123;item&#125;&#125; state=installed # 调用模块，具体要做的事情，这里使用with_items迭代多个yum任务安装必要的依赖 with_items: - gcc - gcc-c++ - pcre - pcre-devel - zlib - zlib-devel - openssl - openssl-devel - name: download nginx # 通过get_url模块下载nginx get_url: url=http://nginx.org/download/nginx-&#123;&#123;version&#125;&#125;.tar.gz dest=/tmp/ mode=0755 force=no - name: unarchive # 通过unarchive模块解压nginx unarchive: src=/tmp/nginx-&#123;&#123;version&#125;&#125;.tar.gz dest=/tmp/ mode=0755 copy=no - name: configure,make and install # 通过shell模块执行shell命令编译安装 shell: cd /tmp/nginx-&#123;&#123;version&#125;&#125; &amp;&amp; ./configure --prefix=/usr/local/nginx &amp;&amp; make &amp;&amp; make install - name: start nginx # 通过shell模块执行shell命令启动nginx shell: /usr/local/nginx/sbin/nginx - name: update config # 通过template模块动态生成配置文件下发到远程主机目录 template: src=nginx.conf.j2 dest=/usr/local/nginx/conf/nginx.conf notify: reload nginx # 在结束时触发一个操作，具体操作通过handlers来定义 tags: reload # 对任务定义一个标签，运行时通过-t执行带指定标签的任务 handlers: - name: reload nginx # 与notify定义的内容对应 shell: /usr/local/nginx/sbin/nginx -s reload 1. 变量在上面的示例中使用vars定义了变量version，在tasks中通过进行引用。Ansible支持如下几种定义变量的方式 1.在playbook文件中定义前面示例已经说明 2.命令行指定在执行playbook时通过-e指定，如ansible-playbook -e &quot;version=1.17.9&quot; nginx_playbook.yml， 这里指定的变量将覆盖playbook中定义的同名变量的值 3.hosts文件中定义变量在/etc/ansible/hosts文件中也可以定义针对单个主机或主机组的变量，如 12345[nginx]192.168.40.201 version=1.17.9 # 定义单个主机的变量192.168.40.205 [nginx:vars] # 定义整个组的统一变量version=1.16.1 4.在独立的yaml文件中定义变量专门定义一个yaml变量文件，然后在playbook文件中通过var_files引用，如 1234567891011# 定义存放变量的文件[root@ansible ]# cat var.ymlversion: 1.16.1# 编写playbook[root@ansible ]# cat nginx_playbook.yml---- hosts: nginx remote_user: root vars_files: # 引用变量文件 - ./var.yml # 指定变量文件的path（这里可以是绝对路径，也可以是相对路径） 5.使用setup模块获取到的变量前面介绍setup模块可获取远程主机的信息，可在playbook中直接引用setup模块获取到的属性，比如系统版本： ansible_distribution_major_version 2. 模板playbook模板为我们提供了动态的配置服务，使用jinja2语言，支持多种条件判断、循环、逻辑运算、比较操作等。应用场景就是定义一个模板配置文件，然后在执行的时候动态生成最终的配置文件下发到远程主机。一般将模板文件放在playbook文件同级的templates目录下，这样在playbook文件中可以直接引用，否则需要通过绝对路径指定，模板文件后缀名一般为 .j2。 本例中，我们将nginx.conf配置文件作为模板文件，添加需要动态配置的内容，并定义一个变量文件，通过vars_files引入：vars_files: ./templates/nginx_locations_vars.yml 12345678910111213141516171819202122232425262728# 模板文件[root@tool-server nginx-deploy]# vim templates/nginx.conf.j2 ... server &#123; listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; # 这里的内容动态生成 &#123;% for location in nginx_locations %&#125; location &#123;&#123;location.path&#125;&#125; &#123; proxy_pass &#123;&#123;location.proxy&#125;&#125;; &#125; &#123;% endfor %&#125; location / &#123; root html; index index.html index.htm; &#125; ...# 独立的自定义变量文件，用于填充模板文件中的变量[root@tool-server nginx-deploy]# vim templates/nginx_locations_vars.ymlnginx_locations: - &#123;\"path\": \"/cns\", \"proxy\": \"http://192.168.40.202/cns\"&#125; - &#123;\"path\": \"/admin\", \"proxy\": \"http://192.168.40.202/admin\"&#125; 3. handlershandlers和notify结合使用，由特定条件触发，一般用于配置文件变更触发服务重启。在本例中我们在配置文件变更时，通过notify定义了一个“reload nginx”的操作，然后在handlers部分定义“reload nginx”操作——通过shell模块调用nginx的reload来重新加载配置。 4. 标签playbook文件中，如果只想执行某一个或几个任务，则可以给任务打标签，在运行的时候通过 -t 选择带指定标签的任务执行，也可以通过 –skip-tags 选择不带指定标签的任务执行。比如在本例中，我们在“update config”的task上加了“reload”的标签，如果后面再修改配置，我们只需要执行“update config”的task并触发reload nginx就行了，可以这么执行playbook 1[root@tool-server nginx-deploy]# ansible-playbook -t reload nginx_playbook.yml 5. when可以在task上添加when表示当某个条件达到了该任务才执行，如 123456tasks: - name: install nginx yum: name=nginx state=installed - name: update config for system6 template: src=nginx.conf.j2 dest=/usr/local/nginx/conf/nginx.conf when: ansible_distribution_major_version == \"6\" # 判断系统版本，为6才执行上面的template配置的文件 6. rolesroles就是将变量、文件、任务、模板及处理器放置在单独的目录中，并可以在playbook中include的一种机制，一般用于主机构建服务的场景中，但也可以是用于构建守护进程等场景。 roles的目录结构，默认的roles目录为/etc/ansible/roles 1234567891011121314roles: # 所有的角色项目必须放在roles目录下 project: # 具体的角色项目名称，比如nginx、tomcat files： # 用来存放由copy或script模块调用的文件 templates： # 用来存放jinjia2模板，template模块会自动在此目录中寻找jinjia2模板文件 tasks： # 此目录应当包含一个main.yml文件，用于定义此角色的任务列表，此文件可以使用include包含其它的位于此目录的task文件。 main.yml handlers： # 此目录应当包含一个main.yml文件，用于定义此角色中触发条件时执行的动作 main.yml vars： # 此目录应当包含一个main.yml文件，用于定义此角色用到的变量 main.yml defaults： # 此目录应当包含一个main.yml文件，用于为当前角色设定默认变量 main.yml meta： # 此目录应当包含一个main.yml文件，用于定义此角色的特殊设定及其依赖关系 main.yml 我们将上面的例子通过roles改造一下 123456789101112131415161718[root@tool-server ~]# cd /etc/ansible/roles/[root@tool-server roles]# mkdir -p nginx/&#123;tasks,vars,templates,handlers&#125;...#创建各目录的mian.yml文件，并将对应的内容加入文件中#最终目录结构[root@tool-server roles]# tree ..└── nginx ├── handlers │ └── main.yml # 上例handlers部分的内容，直接 -name开头，不需要再加 `handlers：` ├── tasks │ └── main.yml # tasks部分内容，直接-name开头，不需要加tasks，可以将各个task拆分为多个文件，然后在main.yml中通过 `- include: install.yml` 形式的列表引入 ├── templates │ └── main.yml # templates/nginx.conf.j2的内容 └── vars └── main.yml # templates/nginx_locations_vars.yml的内容5 directories, 4 files 最后，在playbook中通过roles引入， 123456[root@ansible roles]# vim nginx_playbook.yml---- hosts: nginx remote_user: root roles: - role: nginx # 指定角色名称 roles将playbook的各个部分进行拆分组织，主要用于代码复用度较高的场景。 总结Ansible是功能强大但又很轻量级的自动化运维工具，基于SSH协议批量对远程主机进行管理，不仅可用于日常的服务维护，也可与Jenkins等CI/CD工具结合实现自动化部署。如果你需要在多于一台服务器上做重复又稍显复杂的操作，那么建议你使用Ansible，这将极大提高你的操作效率，并且所有操作文档化，更易维护与迁移。 如果你对Java、Spring Boot、Spring Cloud、Docker，技术管理心得等感兴趣欢迎关注作者微信公众号：空山新雨的技术空间，一起学习成长","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.jboost.cn/categories/DevOps/"}],"tags":[{"name":"ansible","slug":"ansible","permalink":"http://blog.jboost.cn/tags/ansible/"}]},{"title":"Docker笔记（十三）：容器日志采集实践","slug":"docker-13","date":"2020-04-01T06:08:12.000Z","updated":"2020-08-26T01:32:01.275Z","comments":true,"path":"docker-13.html","link":"","permalink":"http://blog.jboost.cn/docker-13.html","excerpt":"日志是服务运行过程中的一个关键环节，借助日志，我们可以排查定位问题，也可以借助集中化的日志管理平台（如ELK）来做一些必要的数据统计分析。在Docker环境中，日志的采集比传统环境更为复杂，因此了解Docker日志的管理机制，及基于此熟悉日志采集的最佳实践对于开发运维人员来说也是避不开的一个知识点。那就开始吧。","text":"日志是服务运行过程中的一个关键环节，借助日志，我们可以排查定位问题，也可以借助集中化的日志管理平台（如ELK）来做一些必要的数据统计分析。在Docker环境中，日志的采集比传统环境更为复杂，因此了解Docker日志的管理机制，及基于此熟悉日志采集的最佳实践对于开发运维人员来说也是避不开的一个知识点。那就开始吧。 Docker容器的日志管理机制1. Docker Daemon日志Docker Daemon在Linux中本身作为systemd service启动，因此可以通过 sudo journalctl -u docker 命令来查看Daemon本身的日志。 2. Docker容器日志通过 docker logs container_id|container_name 可以查看Docker容器的输出日志，但这里的日志只包含容器的标准输出（STDOUT）与标准错误输出（STDERR），适用于一些将日志输出到STDOUT的容器,比如Nginx，查看nginx的dockerfile可发现其是将日志文件链接到了STDOUT与STDERR来实现的， 12RUN ln -sf /dev/stdout /var/log/nginx/access.log&amp;&amp; ln -sf /dev/stderr /var/log/nginx/error.log 但如果容器内部应用日志是输出到日志文件（比如Spring Boot项目或Tomcat容器，一般将日志输出到日志文件中），则无法通过 docker logs 命令查看。 docker logs 会显示历史日志，日志太多的话要等半天才能看到最新日志，同时也对Docker Daemon造成一定的压力，可使用 docker logs --tail 200 container_id来查看最新的N条或使用docker logs -f container_id（类似于tail -f） 3. Docker日志处理机制当我们启动一个容器时，其实是作为Docker Daemon的一个子进程运行，Docker Daemon可以拿到容器里进程的标准输出与标准错误输出，然后通过Docker的Log Driver模块来处理。如下图所示 目前支持的Log Drvier包括： none：容器没有日志，docker logs不输出任何内容 local：日志以自定义格式存储 json-file：日志以json格式存储，默认的Log Driver syslog：将日志写入syslog。syslog守护程序必须在主机上运行 journald：将日志写入journald。journald守护程序必须在主机上运行 gelf：将日志写入Graylog Extended Log Format端点，如Graylog或Logstash fluentd：将日志写入fluentd。fluentd守护程序必须在主机上运行 awslogs：将日志写入Amazon CloudWatch Logs splunk：通过HTTP Event Collector将日志写入splunk etwlogs：将日志作为ETW（Event Tracing for Windows）事件写入。只在Windows平台可用 gcplogs：将日志写入Google Cloud Platform Logging logentries：将日志写入Rapid7 Logentries 使用Docker-CE版本时，docker logs命令仅适用于 local， json-file， journald 三种Log Driver。 可通过docker info来查看Docker Daemon（针对所有容器）或docker inspect来查看单个容器所使用的Log Driver 123456# Docker Daemon[devuser@test-server-1 ~]$ docker info |grep \"Logging Driver\"Logging Driver: json-file# 单个Docker 容器[devuser@test-server-1 ~]$ docker inspect -f '&#123;&#123;.HostConfig.LogConfig.Type&#125;&#125;' 76f82aa32468json-file 修改Docker Daemon使用的Log Driver可通过修改配置文件 /etc/docker/daemon.json 进行，重启Docker后该配置对该Docker Daemon管理的所有容器生效， 如 1234567&#123; \"log-driver\": \"local\", \"log-opts\": &#123; \"max-size\": \"10m\", \"max-file\": 3 &#125;&#125; 设置单个容器的Log Driver则可以在容器运行时通过参数指定，如 123456[root@tool-server ~]# docker run -d --name nginx -p 80:80 --log-driver local --log-opt max-size=10m --log-opt max-file=3 --restart=always nginx63155291e724276d6154a26958b0e523a003958b1cdf7df9f1f0903bfc989b99[root@tool-server ~]# tail -f /var/lib/docker/containers/63155291e724276d6154a26958b0e523a003958b1cdf7df9f1f0903bfc989b99/local-logs/container.logstdoutҭʡ󹾖ā192.168.40.160 - - [02/Apr/2020:06:05:56 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\" \"-\"ܻ stdout򪸶¡󹾖㿱92.168.40.160 - - [02/Apr/2020:06:05:56 +0000] \"GET /favicon.ico HTTP/1.1\" 404 555 \"http://192.168.40.110/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\" \"-\" 以下对常用的几种Log Driver进行详细介绍 local local Log Driver会将容器的STDOUT/STDERR输出写到宿主机的磁盘。前面示例了将Docker Daemon或单个容器的Log Driver设置为local，可以看到local的日志保存路径为 /var/lib/docker/containers/{container_id}/local-logs/container.log local Log Driver支持的配置属性如下 配置属性 描述 max-size 单个日志文件的最大大小，默认为20m（单位可为k,m,g） max-file 最多存在多少个日志文件，文件数超过该值则会删除最旧的文件，默认为5 compress 是否对切割文件进行压缩，默认为true json-file json-file Log Driver是Docker默认启用的Driver，将容器的STDOUT/STDERR输出以json的格式写到宿主机的磁盘，日志文件路径为 /var/lib/docker/containers/{container_id}/{container_id}-json.log 格式如下，包含三个字段： log, stream, time。 123[root@tool-server ~]# tail -f /var/lib/docker/containers/2cef9daeac7b009c636ed2b1a7ad8fe3342bc0d5dcd55e69d7a45a586d7abc47/2cef9daeac7b009c636ed2b1a7ad8fe3342bc0d5dcd55e69d7a45a586d7abc47-json.log&#123;\"log\":\"2020-03-31T10:27:30.639+0000 I SHARDING [conn4] Marking collection yapi.project as collection version: \\u003cunsharded\\u003e\\n\",\"stream\":\"stdout\",\"time\":\"2020-03-31T10:27:30.639749587Z\"&#125;&#123;\"log\":\"2020-03-31T10:27:30.756+0000 I SHARDING [conn2] Marking collection yapi.log as collection version: \\u003cunsharded\\u003e\\n\",\"stream\":\"stdout\",\"time\":\"2020-03-31T10:27:30.756744876Z\"&#125; json-file将日志的每一行封装到一个json串中，因此像Java的异常栈日志将会被拆分为多条json，在导入到ELK日志管理系统中时需要做合并处理。 json-file Log Driver支持的配置属性如下 配置属性 描述 max-size 单个日志文件的最大大小，单位可为k,m,g。默认-1，表示无限制 max-file 最多存在多少个日志文件，文件数超过该值则会删除最旧的文件，默认为1 labels 在启动Docker容器时以逗号分隔的与日志相关的标签列表 env 在启动Docker容器时以逗号分隔的与日志相关的环境变量列表 env-regex 类似于env，用于匹配与日志相关的环境变量的正则表达式 compress 是否对切割文件进行压缩，默认为disabled journald journald Log Driver将容器的STDOUT/STDERR发送到systemd journal，与local，json-file一样可以使用 docker logs 来查看。也可以使用 journalctl命令来查看，如 1234567[root@tool-server ~]# docker run -d --name nginx -p 80:80 --log-driver journald --log-opt labels=profile --log-opt env=ONLINE --label profile=dev --env \"ONLINE=false\" --restart=always nginx2011dc967d7e068b14d974bdc083d072fd09498a7de74984d482897d1b5c4200[root@tool-server ~]# journalctl -f CONTAINER_NAME=nginx-- Logs begin at Tue 2020-03-31 18:24:36 CST. --Apr 02 18:20:05 tool-server 2011dc967d7e[3655]: 192.168.40.160 - - [02/Apr/2020:10:20:05 +0000] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\" \"-\" journalctl的命令形式 1234journalctl CONTAINER_NAME=nginx # 输出指定容器的日志journalctl -b CONTAINER_NAME=nginx # 输出从上次启动以来的所有日志journalctl -o json CONTAINER_NAME=nginx # 以json格式显示日志，包含了label，env中指定的属性值journalctl -f CONTAINER_NAME=nginx # 类似于tail -f journald Log Driver支持的配置属性如下 配置属性 描述 tag 指定要在日志中设置CONTAINER_TAG与SYSLOG_IDENTIFIER值的模板 labels 定义一个标签列表，可在后面通过 –label 设置标签的值，该标签值会包含在日志体中 env 定义一个环境变量列表，可在后面通过 –env 指定环境变量的值，并且值会包含在日志体重 env-regex 与env类似，用于匹配与日志相关的环境变量的正则表达式 下图是使用 journalctl -o json CONTAINER_NAME=nginx 命令输出的完整json格式日志，其中包含了前面设置的profile标签与ONLINE环境变量。 除此之外，journald日志体中还会加上下面的数据 CONTAINER_ID： 容器ID，12位 CONTAINER_ID_FULL：完整的容器ID，64位 CONTAINER_NAME：容器名称 CONTAINER_TAG，SYSLOG_IDENTIFIER：容器的tag 具体从上图也可以看出。 syslog syslog Log Driver将日志发送到syslog的服务器，在Linux中，一般使用rsyslog服务。 修改rsyslog配置，打开udp或tcp监听 12345678[root@tool-server ~]# vim /etc/rsyslog.conf# Provides UDP syslog reception$ModLoad imudp$UDPServerRun 514# Provides TCP syslog reception#$ModLoad imtcp#$InputTCPServerRun 514 重启rsyslog 12345[root@tool-server ~]# systemctl restart rsyslog[root@tool-server ~]# netstat -ano|grep 514udp 0 0 0.0.0.0:514 0.0.0.0:* off (0.00/0/0)udp6 0 0 :::514 :::* off (0.00/0/0) 以syslog Log Driver启动nginx容器 123456[root@tool-server ~]# docker run -d --name nginx -p 80:80 --log-driver syslog --log-opt syslog-address=udp://127.0.0.1:514 --restart=always nginx989db94a01c36b7ea767bcb8db8ccc64bd558291ef7bcb364efa1352c78b8878# 查看syslog日志[root@tool-server ~]# tail -f /var/log/messagesApr 2 18:58:06 localhost 989db94a01c3[3655]: 192.168.40.160 - - [02/Apr/2020:10:58:06 +0000] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\" \"-\" 容器日志采集实践 对于使用STDOUT/STDERR输出日志的容器，比如nginx，可通过默认的json-file，从前文提到的目录下通过filebeat或logstash进行监听采集 对于使用日志文件记录的容器，比如tomcat，可通过目录挂载的方式将容器日志目录挂载到宿主机目录，然后监听宿主机目录进行日志采集，比如启动时指定参数 -v /data/tomcat/logs:/usr/local/tomcat/logs。但这种方式如果同一应用的容器在一个服务器节点上启动多个时，会造成日志文件名相同产生冲突，对于这种情况，如果使用的是logback日志框架，之前的文章自定义logback日志文件的名称 提供了一种方案 如果既有标准输出又有日志文件输出，可考虑第三方日志采集框架，比如阿里巴巴开源的log-pilot 如果是Serverless环境，即没有具体的物理机或虚拟机，通过云容器服务部署的情况，则可以通过挂载云盘的方式，将容器日志目录挂载到云盘目录下，通过监听云盘目录进行日志采集 出于篇幅与时间关系，这里只列出几种不同场景的日志采集方案，1,2场景比较好理解，对于4一般云平台都有相关的文档可查阅，场景3后续可再整理一篇实操文来补充说明。 作者：空山新雨近期作者写了几十篇技术博客，内容包括Java、Spring Boot、Spring Cloud、Docker，技术管理心得等欢迎关注作者微信公众号：空山新雨的技术空间，一起学习成长","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.jboost.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.jboost.cn/tags/docker/"},{"name":"log-pilot","slug":"log-pilot","permalink":"http://blog.jboost.cn/tags/log-pilot/"}]},{"title":"Linux开机自启动配置","slug":"linux-autoboot","date":"2020-03-24T06:39:09.000Z","updated":"2020-03-25T07:00:01.980Z","comments":true,"path":"linux-autoboot.html","link":"","permalink":"http://blog.jboost.cn/linux-autoboot.html","excerpt":"很多时候，我们需要将一些服务在Linux系统启动时即自动运行，省得每次都要去手动启动一遍，如Redis， MySQL， Nginx等。本文对CentOS与Ubuntu下开机自启动的配置方法进行整理，供参考查阅。","text":"很多时候，我们需要将一些服务在Linux系统启动时即自动运行，省得每次都要去手动启动一遍，如Redis， MySQL， Nginx等。本文对CentOS与Ubuntu下开机自启动的配置方法进行整理，供参考查阅。 CentOS7的开机自启动配置一. rc.local方式rc.local是CentOS以前版本的方式，在CentOS7中仍然以兼容的形式存在，虽仍可用，但不推荐（推荐使用systemd service）。 编写需要开机自启动的脚本，并添加执行权限 1234567[root@dev-server-1 ~]# vim test_rclocal.sh#!/bin/bashtime=`date +%F_%T`echo $time' from rc.local' &gt;&gt; /tmp/test.log[root@dev-server-1 ~]# chmod +x test_rclocal.sh 作为测试，上述脚本打印一个时间到/tmp/test.log文件中 在/etc/rc.d/rc.local配置文件中添加脚本运行命令（使用绝对路径） 1234567[root@dev-server-1 ~]# vim /etc/rc.d/rc.local #!/bin/bash# ...注释部分touch /var/lock/subsys/local/root/test_rclocal.sh &gt;/dev/null 2&gt;/dev/null 添加/etc/rc.d/rc.local文件的执行权限 在centos7中，/etc/rc.d/rc.local没有执行权限，需要手动授权 1[root@dev-server-1 ~]# chmod +x /etc/rc.d/rc.local 以上三步，即可使/root/test_rclocal.sh &gt;/dev/null 2&gt;/dev/null 命令在服务器系统启动时自动运行。 二. chkconfig方式 编写需要开机自启动的测试脚本，并添加执行权限 12345678[root@dev-server-1 ~]# vim test_chkconfig.sh#!/bin/bashtime=`date +%F_%T`echo $time' from chkconfig' &gt;&gt; /tmp/test.log[root@dev-server-1 ~]# chmod +x test_chkconfig.sh 在/etc/rc.d/init.d/目录下添加一个可执行脚本testchkconfig 123456789[root@dev-server-1 ~]# vim /etc/rc.d/init.d/testchkconfig#!/bin/bash# chkconfig: 2345 90 10# description: test chkconfig/root/test_chkconfig.sh &gt;/dev/null 2&gt;/dev/null[root@dev-server-1 ~]# chmod 755 /etc/rc.d/init.d/testchkconfig 上述testchkconfig脚本的头部必须遵循一定的格式 # chkconfig: 2345 90 10， 其中2345指定服务在哪些执行等级中开启或关闭，90表示启动的优先级（0-100，越大优先级越低），10表示关闭的优先级。执行等级包括 0：表示关机 1：单用户模式 2：无网络连接的多用户命令行模式 3：有网络连接的多用户命令行模式 4：保留未使用 5：带图形界面的多用户模式 6：重新启动 加入开机启动服务列表 1234567891011121314[root@dev-server-1 ~]# chkconfig --add testchkconfig[root@dev-server-1 ~]# chkconfig --listNote: This output shows SysV services only and does not include native systemd services. SysV configuration data might be overridden by native systemd configuration. If you want to list systemd services use 'systemctl list-unit-files'. To see services enabled on particular target use 'systemctl list-dependencies [target]'.netconsole 0:off 1:off 2:off 3:off 4:off 5:off 6:offnetwork 0:off 1:off 2:on 3:on 4:on 5:on 6:offtestchkconfig 0:off 1:off 2:on 3:on 4:on 5:on 6:off 使用 chkconfig --list 可查看当前加入开机自启动的服务列表，但如Note部分所述，该命令只显示SysV服务，不包含原生的systemd服务，查看systemd服务可使用systemctl list-unit-files命令。 以上三步，即可使/root/test_chkconfig.sh &gt;/dev/null 2&gt;/dev/null 命令在服务器系统启动时自动运行。 chkconfig的其它命令参考 12345$chkconfig --list # 表示查看所有服务在各个运行级别下的状态。$chkconfig testchkconfig on # 表示指定服务在所有的运行级别下都是开启状态。$chkconfig testchkconfig off # 表示指定服务在所有的运行级别下都是关闭状态。$chkconfig --level 5 testchkconfig on # 表示指定服务在运行级别5图形模式的状态下开机启动服务。$chkconfig --level 5 testchkconfig off # 表示指定服务在运行级别5图形模式的状态下开机不启动服务。 三. 自定义systemd service方式CentOS7的systemd服务脚本存放在：/usr/lib/systemd/system（系统级）/usr/lib/systemd/user（用户级）下，以.service结尾。这里以nginx为例 在/usr/lib/systemd/system目录下创建nginx.service文件 123456789101112131415161718192021222324252627[devuser@test-server-1 ~]$ sudo vim /usr/lib/systemd/system/nginx.service[Unit]Description=nginx serverDocumentation=http://nginx.org/en/docs/# 依赖服务，仅当依赖的服务启动之后再启动自定义的服务After=network.target remote-fs.target nss-lookup.target [Service]# 启动类型，包括simple、forking、oneshot、notify、dbusType=forking# pid文件路径PIDFile=/var/run/nginx.pid# 启动前执行的操作ExecStartPre=/usr/local/nginx/sbin/nginx -t -c /usr/local/nginx/conf/nginx.conf# 启动命令ExecStart=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf# 重载命令ExecReload=/usr/local/nginx/sbin/nginx -s reload# 停止命令ExecStop=/usr/local/nginx/sbin/nginx -s stop# 是否给服务分配独立的临时空间PrivateTmp=true[Install]# 服务安装的用户模式，一般使用multi-user即可WantedBy=multi-user.target 其中Service部分的Type包括如下几种类型： simple：表示ExecStart启动的进程是该服务的主进程。如果它需要为其他进程提供服务，那么必须在该服务启动之前先建立好通信渠道，比如套接字，以加快后续单元的启动速度。 forking：表示ExecStart进程将会在启动时使用fork()函数，这是传统Unix系统的做法，也就是说这个进程将由systemd进程fork出来，然后当该进程都准备就绪时，systemd进程退出，而fork出来的进程作为服务的主进程继续运行，对于此类型的进程，建议设置PIDFile选项，以帮助systemd准确定位该服务的主进程。 oneshot：该进程会在systemd启动后续单元之前退出，适用于仅需要执行一次的程序。比如清理磁盘，你只需要执行一次，不需要一直在后台运行这个程序。 notify：与simple类似，不同之处在于该进程会在启动完成之后通过sd_notify之类的接口发送一个通知消息。systemd在启动后续单元之前，必须确保该进程已经成功地发送了一个消息。 dbus：该进程需要在D-Bus上获得一个由BusName指定的名称，systemd将会在启动后续单元之前，首先确保该进程已经成功获取了指定D-Bus名称。 开启开机自启动 12[devuser@test-server-1 ~]$ sudo systemctl enable nginx.serviceCreated symlink from /etc/systemd/system/multi-user.target.wants/nginx.service to /usr/lib/systemd/system/nginx.service. 以上两步，就将nginx服务配置成了在操作系统启动时自动启动。 其它命令参考 12345678910111213$sudo systemctl start nginx.service # 启动$sudo systemctl restart nginx.service # 重启$sudo systemctl reload nginx.service # 重载$sudo systemctl stop nginx.service # 停止$sudo systemctl status nginx.service # 查看服务状态$sudo systemctl cat nginx.service # 查看服务配置$systemctl list-unit-files |grep nginx # 查看服务enabled状态$sudo systemctl disable nginx.service # 关闭开机自启动$sudo journalctl -f -u nginx.service # 查看日志$sudo systemctl daemon-reload # 配置修改后，重新加载 根据以上配置，通过start启动nginx服务时，报PID file /var/run/nginx.pid not readable (yet?) after start.的错误，启动失败，日志如下 12345678[devuser@test-server-1 ~]$ sudo journalctl -f -u nginx.service-- Logs begin at Wed 2020-03-25 09:14:55 CST. --Mar 25 11:02:27 test-server-1 nginx[14144]: nginx: configuration file /usr/local/nginx/conf/nginx.conf test is successfulMar 25 11:02:27 test-server-1 systemd[1]: PID file /run/nginx.pid not readable (yet?) after start.Mar 25 11:04:29 test-server-1 systemd[1]: nginx.service start operation timed out. Terminating.Mar 25 11:04:29 test-server-1 systemd[1]: Failed to start nginx.Mar 25 11:04:29 test-server-1 systemd[1]: Unit nginx.service entered failed state.Mar 25 11:04:29 test-server-1 systemd[1]: nginx.service failed. 从字面看是PID文件不可读，查看/var/run/nginx.pid，该文件也确实不存在，查看nginx.conf配置文件，发现是pid /var/run/nginx.pid;这行配置被注释掉了， 如果不指定pid文件位置，nginx默认会把pid文件保存在logs目录中。所以出现systemd启动服务时找不到pid文件而报错，将nginx.conf中的pid配置注释去掉，重启nginx.service即可。 Ubuntu18.04的开机自启动配置在Ubuntu18.04中，主要也是以systemd服务来实现开机自启动，systemd默认读取/etc/systemd/system/下的配置文件，该目录下的一些文件会链接到/lib/systemd/system/下的文件。 因此可以在/etc/systemd/system/目录下面创建一个自启动服务配置，以内网穿透服务frp客户端为例，如 12345678910111213[Unit]Description=frpcAfter=network.targetWants=network.target[Service]TimeoutStartSec=30ExecStart=/home/devuser/apps/frp/frpc -c /home/devuser/apps/frp/frpc.iniExecStop=/bin/kill $MAINPIDRestart=1[Install]WantedBy=multi-user.target 各配置项与CentOS类似。然后将服务器加到自启动列表中并启动服务 12$sudo systemctl enable frpc$sudo systemctl start frpc 其它更多systemctl命令与CentOS类似。 也可以使用/lib/systemd/system/rc-local.service来执行一些开机需要执行的脚本，该文件内容为 1234567891011121314151617181920212223# SPDX-License-Identifier: LGPL-2.1+## This file is part of systemd.## systemd is free software; you can redistribute it and/or modify it# under the terms of the GNU Lesser General Public License as published by# the Free Software Foundation; either version 2.1 of the License, or# (at your option) any later version.# This unit gets pulled automatically into multi-user.target by# systemd-rc-local-generator if /etc/rc.local is executable.[Unit]Description=/etc/rc.local CompatibilityDocumentation=man:systemd-rc-local-generator(8)ConditionFileIsExecutable=/etc/rc.localAfter=network.target[Service]Type=forkingExecStart=/etc/rc.local startTimeoutSec=0RemainAfterExit=yesGuessMainPID=no 从Description看它是为了兼容之前版本的/etc/rc.local的，该服务启动命名就是/etc/rc.local start，将该文件链接到/etc/systemd/system下 1$ sudo ln -s /lib/systemd/system/rc-local.service /etc/systemd/system/rc-local.service 创建/etc/rc.local文件，并赋予可执行权限 12345$ vim /etc/rc.local#!/bin/bash echo \"test rc \" &gt; /var/test.log $ sudo chmod +x /etc/rc.local &lt;完&gt; 欢迎关注作者微信公众号：空山新雨的技术空间，查看更多关于Java、Spring Boot、Spring Cloud、Docker等技术实践文章","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.jboost.cn/categories/DevOps/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://blog.jboost.cn/tags/linux/"}]},{"title":"Spring Boot（十三）：整合Redis集群","slug":"springboot-redis","date":"2020-03-18T10:05:01.000Z","updated":"2020-03-21T02:40:17.242Z","comments":true,"path":"springboot-redis.html","link":"","permalink":"http://blog.jboost.cn/springboot-redis.html","excerpt":"前面的两篇文章（Redis的持久化方案， 一文掌握Redis的三种集群方案）分别介绍了Redis的持久化与集群方案 —— 包括主从复制模式、哨兵模式、Cluster模式，其中主从复制模式由于不能自动做故障转移，当节点出现故障时需要人为干预，不满足生产环境的高可用需求，所以在生产环境一般使用哨兵模式或Cluster模式。那么在Spring Boot项目中，如何访问这两种模式的Redis集群，可能遇到哪些问题，是本文即将介绍的内容。","text":"前面的两篇文章（Redis的持久化方案， 一文掌握Redis的三种集群方案）分别介绍了Redis的持久化与集群方案 —— 包括主从复制模式、哨兵模式、Cluster模式，其中主从复制模式由于不能自动做故障转移，当节点出现故障时需要人为干预，不满足生产环境的高可用需求，所以在生产环境一般使用哨兵模式或Cluster模式。那么在Spring Boot项目中，如何访问这两种模式的Redis集群，可能遇到哪些问题，是本文即将介绍的内容。 Spring Boot 2 整合Redisspring boot中整合Redis非常简单，在pom.xml中添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; spring boot 2的spring-boot-starter-data-redis中，默认使用的是lettuce作为redis客户端，它与jedis的主要区别如下： Jedis是同步的，不支持异步，Jedis客户端实例不是线程安全的，需要每个线程一个Jedis实例，所以一般通过连接池来使用Jedis Lettuce是基于Netty框架的事件驱动的Redis客户端，其方法调用是异步的，Lettuce的API也是线程安全的，所以多个线程可以操作单个Lettuce连接来完成各种操作，同时Lettuce也支持连接池 如果不使用默认的lettuce，使用jedis的话，可以排除lettuce的依赖，手动加入jedis依赖，配置如下 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;io.lettuce&lt;/groupId&gt; &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 在配置文件application.yml中添加配置（针对单实例） 12345678910111213spring: redis: host: 192.168.40.201 port: 6379 password: passw0rd database: 0 # 数据库索引，默认0 timeout: 5000 # 连接超时，单位ms jedis: # 或lettuce, 连接池配置，springboot2.0中使用jedis或者lettuce配置连接池，默认为lettuce连接池 pool: max-active: 8 # 连接池最大连接数（使用负值表示没有限制） max-wait: -1 # 连接池分配连接最大阻塞等待时间（阻塞时间到，抛出异常。使用负值表示无限期阻塞） max-idle: 8 # 连接池中的最大空闲连接数 min-idle: 0 # 连接池中的最小空闲连接数 然后添加配置类。其中@EnableCaching注解是为了使@Cacheable、@CacheEvict、@CachePut、@Caching注解生效 123456789101112131415161718192021222324252627282930@Configuration@EnableCachingpublic class RedisConfig &#123; @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory factory) &#123; RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;&gt;(); template.setConnectionFactory(factory); // 使用Jackson2JsonRedisSerialize 替换默认的jdkSerializeable序列化 Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper om = new ObjectMapper(); om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(om); StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); // key采用String的序列化方式 template.setKeySerializer(stringRedisSerializer); // hash的key也采用String的序列化方式 template.setHashKeySerializer(stringRedisSerializer); // value序列化方式采用jackson template.setValueSerializer(jackson2JsonRedisSerializer); // hash的value序列化方式采用jackson template.setHashValueSerializer(jackson2JsonRedisSerializer); template.afterPropertiesSet(); return template; &#125;&#125; 上述配置类注入了自定义的RedisTemplate&lt;String, Object&gt;， 替换RedisAutoConfiguration中自动配置的RedisTemplate&lt;Object, Object&gt;类（RedisAutoConfiguration另外还自动配置了StringRedisTemplate）。 此时，我们可以通过定义一个基于RedisTemplate的工具类，或通过在Service层添加@Cacheable、@CacheEvict、@CachePut、@Caching注解来使用缓存。比如定义一个RedisService类，封装常用的Redis操作方法， 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Component@Slf4jpublic class RedisService &#123; @Autowired private RedisTemplate&lt;String, Object&gt; redisTemplate; /** * 指定缓存失效时间 * * @param key 键 * @param time 时间(秒) * @return */ public boolean expire(String key, long time) &#123; try &#123; if (time &gt; 0) &#123; redisTemplate.expire(key, time, TimeUnit.SECONDS); &#125; return true; &#125; catch (Exception e) &#123; log.error(\"exception when expire key &#123;&#125;. \", key, e); return false; &#125; &#125; /** * 根据key获取过期时间 * * @param key 键 不能为null * @return 时间(秒) 返回0代表为永久有效 */ public long getExpire(String key) &#123; return redisTemplate.getExpire(key, TimeUnit.SECONDS); &#125; /** * 判断key是否存在 * * @param key 键 * @return true 存在 false不存在 */ public boolean hasKey(String key) &#123; try &#123; return redisTemplate.hasKey(key); &#125; catch (Exception e) &#123; log.error(\"exception when check key &#123;&#125;. \", key, e); return false; &#125; &#125; ...&#125; 出于篇幅，完整代码请查阅本文示例源码： https://github.com/ronwxy/springboot-demos/tree/master/springboot-redis-sentinel 或在Service层使用注解，如 1234567891011121314151617181920212223242526@Service@CacheConfig(cacheNames = \"users\")public class UserService &#123; private static Map&lt;String, User&gt; userMap = new HashMap&lt;&gt;(); @CachePut(key = \"#user.username\") public User addUser(User user)&#123; user.setUid(UUID.randomUUID().toString()); System.out.println(\"add user: \" + user); userMap.put(user.getUsername(), user); return user; &#125; @Caching(put = &#123; @CachePut( key = \"#user.username\"), @CachePut( key = \"#user.uid\") &#125;) public User addUser2(User user) &#123; user.setUid(UUID.randomUUID().toString()); System.out.println(\"add user2: \" + user); userMap.put(user.getUsername(), user); return user; &#125; ...&#125; Spring Boot 2 整合Redis哨兵模式Spring Boot 2 整合Redis哨兵模式除了配置稍有差异，其它与整合单实例模式类似，配置示例为 12345678910111213spring: redis: password: passw0rd timeout: 5000 sentinel: master: mymaster nodes: 192.168.40.201:26379,192.168.40.201:36379,192.168.40.201:46379 # 哨兵的IP:Port列表 jedis: # 或lettuce pool: max-active: 8 max-wait: -1 max-idle: 8 min-idle: 0 完整示例可查阅源码： https://github.com/ronwxy/springboot-demos/tree/master/springboot-redis-sentinel 上述配置只指定了哨兵节点的地址与master的名称，但Redis客户端最终访问操作的是master节点，那么Redis客户端是如何获取master节点的地址，并在发生故障转移时，如何自动切换master地址的呢？我们以Jedis连接池为例，通过源码来揭开其内部实现的神秘面纱。 在 JedisSentinelPool 类的构造函数中，对连接池做了初始化，如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263 public JedisSentinelPool(String masterName, Set&lt;String&gt; sentinels, final GenericObjectPoolConfig poolConfig, final int connectionTimeout, final int soTimeout, final String password, final int database, final String clientName) &#123; this.poolConfig = poolConfig; this.connectionTimeout = connectionTimeout; this.soTimeout = soTimeout; this.password = password; this.database = database; this.clientName = clientName; HostAndPort master = initSentinels(sentinels, masterName); initPool(master); &#125;private HostAndPort initSentinels(Set&lt;String&gt; sentinels, final String masterName) &#123; for (String sentinel : sentinels) &#123; final HostAndPort hap = HostAndPort.parseString(sentinel); log.fine(\"Connecting to Sentinel \" + hap); Jedis jedis = null; try &#123; jedis = new Jedis(hap.getHost(), hap.getPort()); List&lt;String&gt; masterAddr = jedis.sentinelGetMasterAddrByName(masterName); // connected to sentinel... sentinelAvailable = true; if (masterAddr == null || masterAddr.size() != 2) &#123; log.warning(\"Can not get master addr, master name: \" + masterName + \". Sentinel: \" + hap + \".\"); continue; &#125; master = toHostAndPort(masterAddr); log.fine(\"Found Redis master at \" + master); break; &#125; catch (JedisException e) &#123; // resolves #1036, it should handle JedisException there's another chance // of raising JedisDataException log.warning(\"Cannot get master address from sentinel running @ \" + hap + \". Reason: \" + e + \". Trying next one.\"); &#125; finally &#123; if (jedis != null) &#123; jedis.close(); &#125; &#125; &#125; //省略了非关键代码 for (String sentinel : sentinels) &#123; final HostAndPort hap = HostAndPort.parseString(sentinel); MasterListener masterListener = new MasterListener(masterName, hap.getHost(), hap.getPort()); // whether MasterListener threads are alive or not, process can be stopped masterListener.setDaemon(true); masterListeners.add(masterListener); masterListener.start(); &#125; return master; &#125; initSentinels 方法中主要干了两件事： 遍历哨兵节点，通过get-master-addr-by-name命令获取master节点的地址信息，找到了就退出循环。get-master-addr-by-name命令执行结果如下所示 12345[root@dev-server-1 master-slave]# redis-cli -p 26379127.0.0.1:26379&gt; sentinel get-master-addr-by-name mymaster1) \"192.168.40.201\"2) \"7001\"127.0.0.1:26379&gt; 对每一个哨兵节点通过一个 MasterListener 进行监听（Redis的发布订阅功能），订阅哨兵节点+switch-master频道，当发生故障转移时，客户端能收到哨兵的通知，通过重新初始化连接池，完成主节点的切换。MasterListener.run方法中监听哨兵部分代码如下 12345678910111213141516171819202122j.subscribe(new JedisPubSub() &#123; @Override public void onMessage(String channel, String message) &#123; log.fine(\"Sentinel \" + host + \":\" + port + \" published: \" + message + \".\"); String[] switchMasterMsg = message.split(\" \"); if (switchMasterMsg.length &gt; 3) &#123; if (masterName.equals(switchMasterMsg[0])) &#123; initPool(toHostAndPort(Arrays.asList(switchMasterMsg[3], switchMasterMsg[4]))); &#125; else &#123; log.fine(\"Ignoring message on +switch-master for master name \" + switchMasterMsg[0] + \", our master name is \" + masterName); &#125; &#125; else &#123; log.severe(\"Invalid message received on Sentinel \" + host + \":\" + port + \" on channel +switch-master: \" + message); &#125; &#125; &#125;, \"+switch-master\"); initPool 方法如下：如果发现新的master节点与当前的master不同，则重新初始化。 12345678910111213141516171819private void initPool(HostAndPort master) &#123; if (!master.equals(currentHostMaster)) &#123; currentHostMaster = master; if (factory == null) &#123; factory = new JedisFactory(master.getHost(), master.getPort(), connectionTimeout, soTimeout, password, database, clientName, false, null, null, null); initPool(poolConfig, factory); &#125; else &#123; factory.setHostAndPort(currentHostMaster); // although we clear the pool, we still have to check the // returned object // in getResource, this call only clears idle instances, not // borrowed instances internalPool.clear(); &#125; log.info(\"Created JedisPool to master at \" + master); &#125; &#125; 通过以上两步，Jedis客户端在只知道哨兵地址的情况下便能获得master节点的地址信息，并且当发生故障转移时能自动切换到新的master节点地址。 Spring Boot 2 整合Redis Cluster模式Spring Boot 2 整合Redis Cluster模式除了配置稍有差异，其它与整合单实例模式也类似，配置示例为 1234567891011121314spring: redis: password: passw0rd timeout: 5000 database: 0 cluster: nodes: 192.168.40.201:7100,192.168.40.201:7200,192.168.40.201:7300,192.168.40.201:7400,192.168.40.201:7500,192.168.40.201:7600 max-redirects: 3 # 重定向的最大次数 jedis: pool: max-active: 8 max-wait: -1 max-idle: 8 min-idle: 0 完整示例可查阅源码： https://github.com/ronwxy/springboot-demos/tree/master/springboot-redis-cluster 在 一文掌握Redis的三种集群方案 中已经介绍了Cluster模式访问的基本原理，可以通过任意节点跳转到目标节点执行命令，上面配置中 max-redirects 控制在集群中跳转的最大次数。 查看JedisClusterConnection的execute方法， 12345678910public Object execute(String command, byte[]... args) &#123; Assert.notNull(command, \"Command must not be null!\"); Assert.notNull(args, \"Args must not be null!\"); return clusterCommandExecutor .executeCommandOnArbitraryNode((JedisClusterCommandCallback&lt;Object&gt;) client -&gt; JedisClientUtils.execute(command, EMPTY_2D_BYTE_ARRAY, args, () -&gt; client)) .getValue();&#125; 集群命令的执行是通过ClusterCommandExecutor.executeCommandOnArbitraryNode来实现的， 12345678910111213141516171819202122232425262728293031323334353637383940public &lt;T&gt; NodeResult&lt;T&gt; executeCommandOnArbitraryNode(ClusterCommandCallback&lt;?, T&gt; cmd) &#123; Assert.notNull(cmd, \"ClusterCommandCallback must not be null!\"); List&lt;RedisClusterNode&gt; nodes = new ArrayList&lt;&gt;(getClusterTopology().getActiveNodes()); return executeCommandOnSingleNode(cmd, nodes.get(new Random().nextInt(nodes.size())));&#125;private &lt;S, T&gt; NodeResult&lt;T&gt; executeCommandOnSingleNode(ClusterCommandCallback&lt;S, T&gt; cmd, RedisClusterNode node, int redirectCount) &#123; Assert.notNull(cmd, \"ClusterCommandCallback must not be null!\"); Assert.notNull(node, \"RedisClusterNode must not be null!\"); if (redirectCount &gt; maxRedirects) &#123; throw new TooManyClusterRedirectionsException(String.format( \"Cannot follow Cluster Redirects over more than %s legs. Please consider increasing the number of redirects to follow. Current value is: %s.\", redirectCount, maxRedirects)); &#125; RedisClusterNode nodeToUse = lookupNode(node); S client = this.resourceProvider.getResourceForSpecificNode(nodeToUse); Assert.notNull(client, \"Could not acquire resource for node. Is your cluster info up to date?\"); try &#123; return new NodeResult&lt;&gt;(node, cmd.doInCluster(client)); &#125; catch (RuntimeException ex) &#123; RuntimeException translatedException = convertToDataAccessException(ex); if (translatedException instanceof ClusterRedirectException) &#123; ClusterRedirectException cre = (ClusterRedirectException) translatedException; return executeCommandOnSingleNode(cmd, topologyProvider.getTopology().lookup(cre.getTargetHost(), cre.getTargetPort()), redirectCount + 1); &#125; else &#123; throw translatedException != null ? translatedException : ex; &#125; &#125; finally &#123; this.resourceProvider.returnResourceForSpecificNode(nodeToUse, client); &#125;&#125; 上述代码逻辑如下 从集群节点列表中随机选择一个节点 从该节点获取一个客户端连接（如果配置了连接池，从连接池中获取），执行命令 如果抛出ClusterRedirectException异常，则跳转到返回的目标节点上执行 如果跳转次数大于配置的值 max-redirects， 则抛出TooManyClusterRedirectionsException异常 可能遇到的问题 Redis连接超时检查服务是否正常启动（比如 ps -ef|grep redis查看进程，netstat -ano|grep 6379查看端口是否起来，以及日志文件），如果正常启动，则查看Redis服务器是否开启防火墙，关闭防火墙或配置通行端口。 Cluster模式下，报连接到127.0.0.1被拒绝错误，如 Connection refused: no further information: /127.0.0.1:7600这是因为在redis.conf中配置 bind 0.0.0.0 或 bind 127.0.0.1导致，需要改为具体在外部可访问的IP，如 bind 192.168.40.201。如果之前已经起了集群，并产生了数据，则修改redis.conf文件后，还需要修改cluster-config-file文件，将127.0.0.1替换为bind 的具体IP，然后重启。 master挂了，slave升级成为master，重启master，不能正常同步新的master数据如果设置了密码，需要在master, slave的配置文件中都配置masterauth password 相关阅读： Redis的持久化方案 一文掌握Redis的三种集群方案 作者：空山新雨近期作者写了几十篇技术博客，内容包括Java、Spring Boot、Spring Cloud、Docker，技术管理心得等欢迎关注作者微信公众号：空山新雨的技术空间，一起学习成长","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.jboost.cn/categories/SpringBoot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"},{"name":"redis","slug":"redis","permalink":"http://blog.jboost.cn/tags/redis/"}]},{"title":"一文了解Redis的三种集群方案","slug":"redis-cluster","date":"2020-03-12T06:08:10.000Z","updated":"2020-08-26T01:36:03.221Z","comments":true,"path":"redis-cluster.html","link":"","permalink":"http://blog.jboost.cn/redis-cluster.html","excerpt":"在开发测试环境中，我们一般搭建Redis的单实例来应对开发测试需求，但是在生产环境，如果对可用性、可靠性要求较高，则需要引入Redis的集群方案。虽然现在各大云平台有提供缓存服务可以直接使用，但了解一下其背后的实现与原理总还是有些必要（比如面试）， 本文就一起来学习一下Redis的几种集群方案。","text":"在开发测试环境中，我们一般搭建Redis的单实例来应对开发测试需求，但是在生产环境，如果对可用性、可靠性要求较高，则需要引入Redis的集群方案。虽然现在各大云平台有提供缓存服务可以直接使用，但了解一下其背后的实现与原理总还是有些必要（比如面试）， 本文就一起来学习一下Redis的几种集群方案。 Redis支持三种集群方案 主从复制模式 Sentinel（哨兵）模式 Cluster模式 主从复制模式1. 基本原理主从复制模式中包含一个主数据库实例（master）与一个或多个从数据库实例（slave），如下图 客户端可对主数据库进行读写操作，对从数据库进行读操作，主数据库写入的数据会实时自动同步给从数据库。 具体工作机制为： slave启动后，向master发送SYNC命令，master接收到SYNC命令后通过bgsave保存快照（即上文所介绍的RDB持久化），并使用缓冲区记录保存快照这段时间内执行的写命令 master将保存的快照文件发送给slave，并继续记录执行的写命令 slave接收到快照文件后，加载快照文件，载入数据 master快照发送完后开始向slave发送缓冲区的写命令，slave接收命令并执行，完成复制初始化 此后master每次执行一个写命令都会同步发送给slave，保持master与slave之间数据的一致性 2. 部署示例本示例基于Redis 5.0.3版。 redis.conf的主要配置 1234567891011121314151617181920212223242526###网络相关#### bind 127.0.0.1 # 绑定监听的网卡IP，注释掉或配置成0.0.0.0可使任意IP均可访问protected-mode no # 关闭保护模式，使用密码访问port 6379 # 设置监听端口，建议生产环境均使用自定义端口timeout 30 # 客户端连接空闲多久后断开连接，单位秒，0表示禁用###通用配置###daemonize yes # 在后台运行pidfile &#x2F;var&#x2F;run&#x2F;redis_6379.pid # pid进程文件名logfile &#x2F;usr&#x2F;local&#x2F;redis&#x2F;logs&#x2F;redis.log # 日志文件的位置###RDB持久化配置###save 900 1 # 900s内至少一次写操作则执行bgsave进行RDB持久化save 300 10save 60 10000 # 如果禁用RDB持久化，可在这里添加 save &quot;&quot;rdbcompression yes #是否对RDB文件进行压缩，建议设置为no，以（磁盘）空间换（CPU）时间dbfilename dump.rdb # RDB文件名称dir &#x2F;usr&#x2F;local&#x2F;redis&#x2F;datas # RDB文件保存路径，AOF文件也保存在这里###AOF配置###appendonly yes # 默认值是no，表示不使用AOF增量持久化的方式，使用RDB全量持久化的方式appendfsync everysec # 可选值 always， everysec，no，建议设置为everysec###设置密码###requirepass 123456 # 设置复杂一点的密码 部署主从复制模式只需稍微调整slave的配置，在redis.conf中添加 123replicaof 127.0.0.1 6379 # master的ip，portmasterauth 123456 # master的密码replica-serve-stale-data no # 如果slave无法与master同步，设置成slave不可读，方便监控脚本发现问题 本示例在单台服务器上配置master端口6379，两个slave端口分别为7001,7002，启动master，再启动两个slave 123[root@dev-server-1 master-slave]# redis-server master.conf[root@dev-server-1 master-slave]# redis-server slave1.conf[root@dev-server-1 master-slave]# redis-server slave2.conf 进入master数据库，写入一个数据，再进入一个slave数据库，立即便可访问刚才写入master数据库的数据。如下所示 123456789101112131415161718192021[root@dev-server-1 master-slave]# redis-cli 127.0.0.1:6379&gt; auth 123456OK127.0.0.1:6379&gt; set site blog.jboost.cnOK127.0.0.1:6379&gt; get site\"blog.jboost.cn\"127.0.0.1:6379&gt; info replication# Replicationrole:masterconnected_slaves:2slave0:ip=127.0.0.1,port=7001,state=online,offset=13364738,lag=1slave1:ip=127.0.0.1,port=7002,state=online,offset=13364738,lag=0...127.0.0.1:6379&gt; exit[root@dev-server-1 master-slave]# redis-cli -p 7001127.0.0.1:7001&gt; auth 123456OK127.0.0.1:7001&gt; get site\"blog.jboost.cn\" 执行info replication命令可以查看连接该数据库的其它库的信息，如上可看到有两个slave连接到master 3. 主从复制的优缺点优点： master能自动将数据同步到slave，可以进行读写分离，分担master的读压力 master、slave之间的同步是以非阻塞的方式进行的，同步期间，客户端仍然可以提交查询或更新请求 缺点： 不具备自动容错与恢复功能，master或slave的宕机都可能导致客户端请求失败，需要等待机器重启或手动切换客户端IP才能恢复 master宕机，如果宕机前数据没有同步完，则切换IP后会存在数据不一致的问题 难以支持在线扩容，Redis的容量受限于单机配置 Sentinel（哨兵）模式1. 基本原理哨兵模式基于主从复制模式，只是引入了哨兵来监控与自动处理故障。如图 哨兵顾名思义，就是来为Redis集群站哨的，一旦发现问题能做出相应的应对处理。其功能包括 监控master、slave是否正常运行 当master出现故障时，能自动将一个slave转换为master（大哥挂了，选一个小弟上位） 多个哨兵可以监控同一个Redis，哨兵之间也会自动监控 哨兵模式的具体工作机制： 在配置文件中通过 sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt; 来定位master的IP、端口，一个哨兵可以监控多个master数据库，只需要提供多个该配置项即可。哨兵启动后，会与要监控的master建立两条连接： 一条连接用来订阅master的_sentinel_:hello频道与获取其他监控该master的哨兵节点信息 另一条连接定期向master发送INFO等命令获取master本身的信息 与master建立连接后，哨兵会执行三个操作： 定期（一般10s一次，当master被标记为主观下线时，改为1s一次）向master和slave发送INFO命令 定期向master和slave的_sentinel_:hello频道发送自己的信息 定期（1s一次）向master、slave和其他哨兵发送PING命令 发送INFO命令可以获取当前数据库的相关信息从而实现新节点的自动发现。所以说哨兵只需要配置master数据库信息就可以自动发现其slave信息。获取到slave信息后，哨兵也会与slave建立两条连接执行监控。通过INFO命令，哨兵可以获取主从数据库的最新信息，并进行相应的操作，比如角色变更等。 接下来哨兵向主从数据库的sentinel:hello频道发送信息与同样监控这些数据库的哨兵共享自己的信息，发送内容为哨兵的ip端口、运行id、配置版本、master名字、master的ip端口还有master的配置版本。这些信息有以下用处： 其他哨兵可以通过该信息判断发送者是否是新发现的哨兵，如果是的话会创建一个到该哨兵的连接用于发送PING命令。 其他哨兵通过该信息可以判断master的版本，如果该版本高于直接记录的版本，将会更新 当实现了自动发现slave和其他哨兵节点后，哨兵就可以通过定期发送PING命令定时监控这些数据库和节点有没有停止服务。 如果被PING的数据库或者节点超时（通过 sentinel down-after-milliseconds master-name milliseconds 配置）未回复，哨兵认为其主观下线（sdown，s就是Subjectively —— 主观地）。如果下线的是master，哨兵会向其它哨兵发送命令询问它们是否也认为该master主观下线，如果达到一定数目（即配置文件中的quorum）投票，哨兵会认为该master已经客观下线（odown，o就是Objectively —— 客观地），并选举领头的哨兵节点对主从系统发起故障恢复。若没有足够的sentinel进程同意master下线，master的客观下线状态会被移除，若master重新向sentinel进程发送的PING命令返回有效回复，master的主观下线状态就会被移除 哨兵认为master客观下线后，故障恢复的操作需要由选举的领头哨兵来执行，选举采用Raft算法： 发现master下线的哨兵节点（我们称他为A）向每个哨兵发送命令，要求对方选自己为领头哨兵 如果目标哨兵节点没有选过其他人，则会同意选举A为领头哨兵 如果有超过一半的哨兵同意选举A为领头，则A当选 如果有多个哨兵节点同时参选领头，此时有可能存在一轮投票无竞选者胜出，此时每个参选的节点等待一个随机时间后再次发起参选请求，进行下一轮投票竞选，直至选举出领头哨兵 选出领头哨兵后，领头者开始对系统进行故障恢复，从出现故障的master的从数据库中挑选一个来当选新的master,选择规则如下： 所有在线的slave中选择优先级最高的，优先级可以通过slave-priority配置 如果有多个最高优先级的slave，则选取复制偏移量最大（即复制越完整）的当选 如果以上条件都一样，选取id最小的slave 挑选出需要继任的slave后，领头哨兵向该数据库发送命令使其升格为master，然后再向其他slave发送命令接受新的master，最后更新数据。将已经停止的旧的master更新为新的master的从数据库，使其恢复服务后以slave的身份继续运行。 2. 部署演示本示例基于Redis 5.0.3版。 哨兵模式基于前文的主从复制模式。哨兵的配置文件为sentinel.conf，在文件中添加 123456sentinel monitor mymaster 127.0.0.1 6379 1 # mymaster定义一个master数据库的名称，后面是master的ip， port，1表示至少需要一个Sentinel进程同意才能将master判断为失效，如果不满足这个条件，则自动故障转移（failover）不会执行sentinel auth-pass mymaster 123456 # master的密码sentinel down-after-milliseconds mymaster 5000 # 5s未回复PING，则认为master主观下线，默认为30ssentinel parallel-syncs mymaster 2 # 指定在执行故障转移时，最多可以有多少个slave实例在同步新的master实例，在slave实例较多的情况下这个数字越小，同步的时间越长，完成故障转移所需的时间就越长sentinel failover-timeout mymaster 300000 # 如果在该时间（ms）内未能完成故障转移操作，则认为故障转移失败，生产环境需要根据数据量设置该值 一个哨兵可以监控多个master数据库，只需按上述配置添加多套 分别以26379,36379,46379端口启动三个sentinel 123[root@dev-server-1 sentinel]# redis-server sentinel1.conf --sentinel[root@dev-server-1 sentinel]# redis-server sentinel2.conf --sentinel[root@dev-server-1 sentinel]# redis-server sentinel3.conf --sentinel 也可以使用redis-sentinel sentinel1.conf 命令启动。此时集群包含一个master、两个slave、三个sentinel，如图， 我们来模拟master挂掉的场景，执行 kill -9 3017 将master进程干掉，进入slave中执行 info replication查看， 12345678910111213141516171819202122[root@dev-server-1 sentinel]# redis-cli -p 7001127.0.0.1:7001&gt; auth 123456OK127.0.0.1:7001&gt; info replication# Replicationrole:slavemaster_host:127.0.0.1master_port:7002master_link_status:upmaster_last_io_seconds_ago:1master_sync_in_progress:0# 省略127.0.0.1:7001&gt; exit[root@dev-server-1 sentinel]# redis-cli -p 7002127.0.0.1:7002&gt; auth 123456OK127.0.0.1:7002&gt; info replication# Replicationrole:masterconnected_slaves:1slave0:ip=127.0.0.1,port=7001,state=online,offset=13642721,lag=1# 省略 可以看到slave 7002已经成功上位晋升为master（role：master），接收一个slave 7001的连接。此时查看slave2.conf配置文件，发现replicaof的配置已经被移除了，slave1.conf的配置文件里replicaof 127.0.0.1 6379 被改为 replicaof 127.0.0.1 7002。重新启动master，也可以看到master.conf配置文件中添加了replicaof 127.0.0.1 7002的配置项，可见大哥（master）下位后，再出来混就只能当当小弟（slave）了，三十年河东三十年河西。 3. 哨兵模式的优缺点优点： 哨兵模式基于主从复制模式，所以主从复制模式有的优点，哨兵模式也有 哨兵模式下，master挂掉可以自动进行切换，系统可用性更高 缺点： 同样也继承了主从模式难以在线扩容的缺点，Redis的容量受限于单机配置 需要额外的资源来启动sentinel进程，实现相对复杂一点，同时slave节点作为备份节点不提供服务 Cluster模式1. 基本原理哨兵模式解决了主从复制不能自动故障转移，达不到高可用的问题，但还是存在难以在线扩容，Redis容量受限于单机配置的问题。Cluster模式实现了Redis的分布式存储，即每台节点存储不同的内容，来解决在线扩容的问题。如图 Cluster采用无中心结构,它的特点如下： 所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽 节点的fail是通过集群中超过半数的节点检测失效时才生效 客户端与redis节点直连,不需要中间代理层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可 Cluster模式的具体工作机制： 在Redis的每个节点上，都有一个插槽（slot），取值范围为0-16383 当我们存取key的时候，Redis会根据CRC16的算法得出一个结果，然后把结果对16384求余数，这样每个key都会对应一个编号在0-16383之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作 为了保证高可用，Cluster模式也引入主从复制模式，一个主节点对应一个或者多个从节点，当主节点宕机的时候，就会启用从节点 当其它主节点ping一个主节点A时，如果半数以上的主节点与A通信超时，那么认为主节点A宕机了。如果主节点A和它的从节点都宕机了，那么该集群就无法再提供服务了 Cluster模式集群节点最小配置6个节点(3主3从，因为需要半数以上)，其中主节点提供读写操作，从节点作为备用节点，不提供请求，只作为故障转移使用。 2. 部署演示本示例基于Redis 5.0.3版。 Cluster模式的部署比较简单，首先在redis.conf中 1234567port 7100 # 本示例6个节点端口分别为7100,7200,7300,7400,7500,7600 daemonize yes # r后台运行 pidfile &#x2F;var&#x2F;run&#x2F;redis_7100.pid # pidfile文件对应7100,7200,7300,7400,7500,7600 cluster-enabled yes # 开启集群模式 masterauth passw0rd # 如果设置了密码，需要指定master密码cluster-config-file nodes_7100.conf # 集群的配置文件，同样对应7100,7200等六个节点cluster-node-timeout 15000 # 请求超时 默认15秒，可自行设置 分别以端口7100,7200,7300,7400,7500,7600 启动六个实例(如果是每个服务器一个实例则配置可一样) 123[root@dev-server-1 cluster]# redis-server redis_7100.conf[root@dev-server-1 cluster]# redis-server redis_7200.conf... 然后通过命令将这个6个实例组成一个3主节点3从节点的集群， 1redis-cli --cluster create --cluster-replicas 1 127.0.0.1:7100 127.0.0.1:7200 127.0.0.1:7300 127.0.0.1:7400 127.0.0.1:7500 127.0.0.1:7600 -a passw0rd 执行结果如图 可以看到 7100， 7200， 7300 作为3个主节点，分配的slot分别为 0-5460， 5461-10922， 10923-16383， 7600作为7100的slave， 7500作为7300的slave，7400作为7200的slave。 我们连接7100设置一个值 12345678[root@dev-server-1 cluster]# redis-cli -p 7100 -c -a passw0rdWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.127.0.0.1:7100&gt; set site blog.jboost.cn-&gt; Redirected to slot [9421] located at 127.0.0.1:7200OK127.0.0.1:7200&gt; get site\"blog.jboost.cn\"127.0.0.1:7200&gt; 注意添加 -c 参数表示以集群模式，否则报 (error) MOVED 9421 127.0.0.1:7200 错误， 以 -a 参数指定密码，否则报(error) NOAUTH Authentication required错误。 从上面命令看到key为site算出的slot为9421，落在7200节点上，所以有Redirected to slot [9421] located at 127.0.0.1:7200，集群会自动进行跳转。因此客户端可以连接任何一个节点来进行数据的存取。 通过cluster nodes可查看集群的节点信息 1234567127.0.0.1:7200&gt; cluster nodeseb28aaf090ed1b6b05033335e3d90a202b422d6c 127.0.0.1:7500@17500 slave c1047de2a1b5d5fa4666d554376ca8960895a955 0 1584165266071 5 connected4cc0463878ae00e5dcf0b36c4345182e021932bc 127.0.0.1:7400@17400 slave 5544aa5ff20f14c4c3665476de6e537d76316b4a 0 1584165267074 4 connecteddbbb6420d64db22f35a9b6fa460b0878c172a2fb 127.0.0.1:7100@17100 master - 0 1584165266000 1 connected 0-5460d4b434f5829e73e7e779147e905eea6247ffa5a2 127.0.0.1:7600@17600 slave dbbb6420d64db22f35a9b6fa460b0878c172a2fb 0 1584165265000 6 connected5544aa5ff20f14c4c3665476de6e537d76316b4a 127.0.0.1:7200@17200 myself,master - 0 1584165267000 2 connected 5461-10922c1047de2a1b5d5fa4666d554376ca8960895a955 127.0.0.1:7300@17300 master - 0 1584165268076 3 connected 10923-16383 我们将7200通过 kill -9 pid杀死进程来验证集群的高可用，重新进入集群执行cluster nodes可以看到7200 fail了，但是7400成了master，重新启动7200，可以看到此时7200已经变成了slave。 3. Cluster模式的优缺点优点： 无中心架构，数据按照slot分布在多个节点。 集群中的每个节点都是平等的关系，每个节点都保存各自的数据和整个集群的状态。每个节点都和其他所有节点连接，而且这些连接保持活跃，这样就保证了我们只需要连接集群中的任意一个节点，就可以获取到其他节点的数据。 可线性扩展到1000多个节点，节点可动态添加或删除 能够实现自动故障转移，节点之间通过gossip协议交换状态信息，用投票机制完成slave到master的角色转换 缺点： 客户端实现复杂，驱动要求实现Smart Client，缓存slots mapping信息并及时更新，提高了开发难度。目前仅JedisCluster相对成熟，异常处理还不完善，比如常见的“max redirect exception” 节点会因为某些原因发生阻塞（阻塞时间大于 cluster-node-timeout）被判断下线，这种failover是没有必要的 数据通过异步复制，不保证数据的强一致性 slave充当“冷备”，不能缓解读压力 批量操作限制，目前只支持具有相同slot值的key执行批量操作，对mset、mget、sunion等操作支持不友好 key事务操作支持有线，只支持多key在同一节点的事务操作，多key分布不同节点时无法使用事务功能 不支持多数据库空间，单机redis可以支持16个db，集群模式下只能使用一个，即db 0 Redis Cluster模式不建议使用pipeline和multi-keys操作，减少max redirect产生的场景。 总结本文介绍了Redis集群方案的三种模式，其中主从复制模式能实现读写分离，但是不能自动故障转移；哨兵模式基于主从复制模式，能实现自动故障转移，达到高可用，但与主从复制模式一样，不能在线扩容，容量受限于单机的配置；Cluster模式通过无中心化架构，实现分布式存储，可进行线性扩展，也能高可用，但对于像批量操作、事务操作等的支持性不够好。三种模式各有优缺点，可根据实际场景进行选择。 参考： https://blog.csdn.net/q649381130/article/details/79931791 https://www.cnblogs.com/51life/p/10233340.html https://www.cnblogs.com/chensuqian/p/10538365.html https://stor.51cto.com/art/201910/604653.htm 作者：空山新雨，一枚仍在学习路上的大龄码农近期作者写了几十篇技术博客，内容包括Java、Spring Boot、Spring Cloud、Docker，技术管理心得等欢迎关注作者微信公众号：空山新雨的技术空间，一起学习成长","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.jboost.cn/categories/DevOps/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://blog.jboost.cn/tags/redis/"}]},{"title":"Redis的持久化方案","slug":"redis-persistence","date":"2020-03-10T00:45:17.000Z","updated":"2020-03-10T10:20:08.256Z","comments":true,"path":"redis-persistence.html","link":"","permalink":"http://blog.jboost.cn/redis-persistence.html","excerpt":"Redis支持RDB与AOF两种持久化机制，持久化可以避免因进程异常退出或down机导致的数据丢失问题，在下次重启时能利用之前的持久化文件实现数据恢复。","text":"Redis支持RDB与AOF两种持久化机制，持久化可以避免因进程异常退出或down机导致的数据丢失问题，在下次重启时能利用之前的持久化文件实现数据恢复。 RDB持久化RDB持久化即通过创建快照（压缩的二进制文件）的方式进行持久化，保存某个时间点的全量数据。RDB持久化是Redis默认的持久化方式。RDB持久化的触发包括手动触发与自动触发两种方式。 手动触发 save， 在命令行执行save命令，将以同步的方式创建rdb文件保存快照，会阻塞服务器的主进程，生产环境中不要用 bgsave, 在命令行执行bgsave命令，将通过fork一个子进程以异步的方式创建rdb文件保存快照，除了fork时有阻塞，子进程在创建rdb文件时，主进程可继续处理请求 自动触发 在redis.conf中配置 save m n 定时触发，如 save 900 1表示在900s内至少存在一次更新就触发 主从复制时，如果从节点执行全量复制操作，主节点自动执行bgsave生成RDB文件并发送给从节点 执行debug reload命令重新加载Redis时 执行shutdown且没有开启AOF持久化 redis.conf中RDB持久化配置 12345678910# 只要满足下列条件之一，则会执行bgsave命令save 900 1 # 在900s内存在至少一次写操作save 300 10save 60 10000# 禁用RBD持久化，可在最后加 save &quot;&quot;# 当备份进程出错时主进程是否停止写入操作stop-writes-on-bgsave-error yes # 是否压缩rdb文件 推荐no 相对于硬盘成本cpu资源更贵rdbcompression no AOF持久化AOF（Append-Only-File）持久化即记录所有变更数据库状态的指令，以append的形式追加保存到AOF文件中。在服务器下次启动时，就可以通过载入和执行AOF文件中保存的命令，来还原服务器关闭前的数据库状态。 redis.conf中AOF持久化配置如下 1234567891011121314# 默认关闭AOF，若要开启将no改为yesappendonly no# append文件的名字appendfilename &quot;appendonly.aof&quot;# 每隔一秒将缓存区内容写入文件 默认开启的写入方式appendfsync everysec # 当AOF文件大小的增长率大于该配置项时自动开启重写（这里指超过原大小的100%）。auto-aof-rewrite-percentage 100# 当AOF文件大小大于该配置项时自动开启重写auto-aof-rewrite-min-size 64mb AOF持久化的实现包括3个步骤: 命令追加：将命令追加到AOF缓冲区 文件写入：缓冲区内容写到AOF文件 文件保存：AOF文件保存到磁盘 其中后两步的频率通过appendfsync来配置，appendfsync的选项包括 always， 每执行一个命令就保存一次，安全性最高，最多只丢失一个命令的数据，但是性能也最低（频繁的磁盘IO） everysec，每一秒保存一次，推荐使用，在安全性与性能之间折中，最多丢失一秒的数据 no， 依赖操作系统来执行（一般大概30s一次的样子），安全性最低，性能最高，丢失操作系统最后一次对AOF文件触发SAVE操作之后的数据 AOF通过保存命令来持久化，随着时间的推移，AOF文件会越来越大，Redis通过AOF文件重写来解决AOF文件不断增大的问题（可以减少文件的磁盘占有量，加快数据恢复的速度），原理如下： 调用fork，创建一个子进程 子进程读取当前数据库的状态来“重写”一个新的AOF文件（这里虽然叫“重写”，但实际并没有对旧文件进行任何读取，而是根据数据库的当前状态来形成指令） 主进程持续将新的变动同时写到AOF重写缓冲区与原来的AOF缓冲区中 主进程获取到子进程重写AOF完成的信号，调用信号处理函数将AOF重写缓冲区内容写入新的AOF文件中，并对新文件进行重命名，原子地覆盖原有AOF文件，完成新旧文件的替换 AOF的重写也分为手动触发与自动触发 手动触发： 直接调用bgrewriteaof命令 自动触发： 根据auto-aof-rewrite-min-size和auto-aof-rewrite-percentage参数确定自动触发时机。其中auto-aof-rewrite-min-size表示运行AOF重写时文件最小体积，默认为64MB。auto-aof-rewrite-percentage表示当前AOF文件大小（aof_current_size）和上一次重写后AOF文件大小（aof_base_size）的比值。自动触发时机为 aof_current_size &gt; auto-aof-rewrite-min-size &amp;&amp;（aof_current_size - aof_base_size）/aof_base_size&gt; = auto-aof-rewrite-percentage RDB vs AOFRDB与AOF两种方式各有优缺点。 RDB的优点：与AOF相比，RDB文件相对较小，恢复数据比较快（原因见数据恢复部分）RDB的缺点：服务器宕机，RBD方式会丢失掉上一次RDB持久化后的数据；使用bgsave fork子进程时会耗费内存。 AOF的优点： AOF只是追加文件，对服务器性能影响较小，速度比RDB快，消耗内存也少，同时可读性高。AOF的缺点：生成的文件相对较大，即使通过AOF重写，仍然会比较大；恢复数据的速度比RDB慢。 数据库的恢复服务器启动时，如果没有开启AOF持久化功能，则会自动载入RDB文件，期间会阻塞主进程。如果开启了AOF持久化功能，服务器则会优先使用AOF文件来还原数据库状态，因为AOF文件的更新频率通常比RDB文件的更新频率高，保存的数据更完整。 redis数据库恢复的处理流程如下， 在数据恢复方面，RDB的启动时间会更短，原因有两个： RDB 文件中每一条数据只有一条记录，不会像AOF日志那样可能有一条数据的多次操作记录。所以每条数据只需要写一次就行了，文件相对较小。 RDB 文件的存储格式和Redis数据在内存中的编码格式是一致的，不需要再进行数据编码工作，所以在CPU消耗上要远小于AOF日志的加载。 但是在进行RDB持久化时，fork出来进行dump操作的子进程会占用与父进程一样的内存，采用的copy-on-write机制，对性能的影响和内存的消耗都是比较大的。比如16G内存，Redis已经使用了10G，这时save的话会再生成10G，变成20G，大于系统的16G。这时候会发生交换，要是虚拟内存不够则会崩溃，导致数据丢失。所以在用redis的时候一定对系统内存做好容量规划。 RDB、AOF混合持久化Redis从4.0版开始支持RDB与AOF的混合持久化方案。首先由RDB定期完成内存快照的备份，然后再由AOF完成两次RDB之间的数据备份，由这两部分共同构成持久化文件。该方案的优点是充分利用了RDB加载快、备份文件小及AOF尽可能不丢数据的特性。缺点是兼容性差，一旦开启了混合持久化，在4.0之前的版本都不识别该持久化文件，同时由于前部分是RDB格式，阅读性较低。 开启混合持久化 1aof-use-rdb-preamble yes 数据恢复加载过程就是先按照RDB进行加载，然后把AOF命令追加写入。 持久化方案的建议 如果Redis只是用来做缓存服务器，比如数据库查询数据后缓存，那可以不用考虑持久化，因为缓存服务失效还能再从数据库获取恢复。 如果你要想提供很高的数据保障性，那么建议你同时使用两种持久化方式。如果你可以接受灾难带来的几分钟的数据丢失，那么可以仅使用RDB。 通常的设计思路是利用主从复制机制来弥补持久化时性能上的影响。即Master上RDB、AOF都不做，保证Master的读写性能，而Slave上则同时开启RDB和AOF（或4.0以上版本的混合持久化方式）来进行持久化，保证数据的安全性。 认真生活，快乐分享 欢迎关注微信公众号：空山新雨的技术空间 ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.jboost.cn/categories/DevOps/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://blog.jboost.cn/tags/redis/"}]},{"title":"Spring Cloud（八）：使用Spring Cloud Bus来实现配置动态更新","slug":"springcloud-8","date":"2020-02-26T02:04:16.000Z","updated":"2020-02-27T05:11:37.349Z","comments":true,"path":"springcloud-8.html","link":"","permalink":"http://blog.jboost.cn/springcloud-8.html","excerpt":"使用Spring Cloud Config我们能实现服务配置的集中化管理，在服务启动时从Config Server获取需要的配置属性。但如果在服务运行过程中，我们需要将某个配置属性进行修改，比如将验证码的失效时间从五分钟调整为十分钟，如何将这个更新在服务端不重启服务就能动态生效，是本文讨论的内容。","text":"使用Spring Cloud Config我们能实现服务配置的集中化管理，在服务启动时从Config Server获取需要的配置属性。但如果在服务运行过程中，我们需要将某个配置属性进行修改，比如将验证码的失效时间从五分钟调整为十分钟，如何将这个更新在服务端不重启服务就能动态生效，是本文讨论的内容。 Spring Cloud BusSpring Cloud Bus可以理解为Spring Cloud体系架构中的消息总线，通过一个轻量级的Message Broker来将分布式系统中的节点连接起来。可用来实现广播状态更新（如配置更新），或其它管理指令。Spring Cloud Bus 就像是一个分布式的Spring Boot Actuator， 目前提供了两种类型的消息队列中间件支持：RabbitMQ与Kafka（对应的pom依赖分别为spring-cloud-starter-bus-amqp， spring-cloud-starter-bus-kafka）。 Spring Cloud 在spring-cloud-context中添加了两个actuator管理接口（POST请求）： /actuator/env 与 /actuator/refresh， 前者可用于更新当前服务实例Environment对象中的配置属性，后者可用于刷新当前服务实例的配置信息。 Spring Cloud Bus也提供了两个对应的接口 /actuator/bus-env，相对于/actuator/env ， 使用键值对更新每个实例的Environment，默认不暴露，需配置management.endpoints.web.exposure.include=bus-env 来开放接口访问 /actuator/bus-refresh，相对于/actuator/refresh，对每个实例，清空RefreshScope缓存，重新绑定@ConfigurationProperties， 默认不暴露，可通过配置management.endpoints.web.exposure.include=bus-refresh 来开放接口访问 综上，/actuator/env 与 /actuator/refresh 是针对单个服务实例修改或刷新其配置信息，而 /actuator/bus-env 与 /actuator/bus-refresh 则是借助于Spring Cloud Bus的消息机制作用于分布式系统中的所有服务实例，因此前面有Spring Cloud Bus 就像是一个分布式的Spring Boot Actuator的说法。 使用Spring Cloud Bus来实现服务配置动态更新的结构图如下 更新配置仓库中的配置文件，push到远程Git仓库 远程Git仓库通过Webhook调用配置服务器的通知更新接口 配置服务器发送配置更新消息到消息总线 其它服务节点监听到配置服务器发送的配置更新消息 其它服务节点向配置服务器发送拉取最新配置的请求 配置服务器向配置仓库拉取最新的配置返回给其它服务节点 案例演示我们还是以前面的springcloud-config， springcloud-eureka， springcloud-eureka-client三个项目来完成本文的案例演示。源码地址 使用Actuator在不引入Spring Cloud Bus的情况下，我们可以通过Spring Cloud提供的actuator接口来实现单个实例的配置动态更新。 依次启动springcloud-eureka， springcloud-config， springcloud-eureka-client项目，然后修改springcloud-eureka-client的启动端口，将8080改为8081，再启动一个springcloud-eureka-client的服务实例。 springcloud-eureka-client 的测试接口代码如下 123456789101112131415@RestController@RefreshScopepublic class HelloController &#123; @Autowired private Environment env; @Value(\"$&#123;app&#125;\") private String app; @RequestMapping(\"/hello\") public String hello()&#123; return \"Hello, welcome to spring cloud 2. env: \" + env.getProperty(\"app\") + \", value: \" + app; &#125;&#125; 此时依次请求两个实例的hello接口，得到结果如下 我们通过/actuator/env接口来修改端口8080实例的属性app的值，使用postman操作如图 此时再请求接口返回结果如下 可以看到Environment对象中app属性的值已更新，但是 @Value注解的属性值未变，可见 /actuator/env 接口只是更新了Environment对象，并不负责刷新其它方式引用的属性值。此时请求另一个端口为8081的实例接口，其属性值都未更新，也可见 /actuator/env 只作用于当前实例本身。 如果要让8080实例的@Value属性也动态更新，则可再调用/actuator/refresh接口，如图 此时再请求测试接口，得到结果如下（@Value注解的属性也已经更新了） 使用Spring Cloud Bus前面我们使用 /actuator/env 与 /actuator/refresh 两个接口可以实现单个服务实例配置的动态更新，但在微服务架构中，服务实例可能达几十甚至几百个，一个个调用来做动态更新就有点太不方便了。这时就该Spring Cloud Bus登场了。 1.添加依赖与配置 在springcloud-config， 与springcloud-eureka-client两个项目中，添加spring cloud bus的依赖与配置。在pom.xml文件中添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-bus-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 在application.yml配置文件中添加RabbitMQ的相关配置 123456spring: rabbitmq: host: 127.0.0.1 port: 5672 username: rabbitmq password: passw0rd 2.依次启动springcloud-eureka， springcloud-config， springcloud-eureka-client项目，并以8081端口再启动一个springcloud-eureka-client的服务实例。 3.我们使用postman对配置服务器调用/actuator/bus-env接口， 请求两个服务实例的测试接口，得到结果 两个实例的Environment对象都已经更新，如果要将@Value注解的属性也更新，则可再调用配置服务器的/actuator/bus-refresh接口。 /actuator/bus-env接口是直接更新的内存Environment实例属性，如果服务重启，则又还原到之前的配置了， 所以还是需要借助配置仓库来永久更新。配置更新后还需要手动调用接口使其生效？DevOps时代了，能自动化的就自动化吧，我们可以借助Git的webhook机制来实现自动化。 自动化本文开头的“使用Spring Cloud Bus来实现服务配置动态更新的结构图”已经示例了使用Git仓库的webhook来触发自动更新配置的流程。但是在Git（如Github）中，我们不能直接使用/actuator/bus-refresh接口来作为webhook（因为接口协议不一致，会出现解析异常），也有人通过提供自己的接口来作为webhook，在自己接口中再转发请求到/actuator/bus-refresh来实现。但实际上，spring-cloud-config-monitor已经提供了对Git webhook的支持。 如下图，spring-cloud-config-monitor提供了对Github，Gitlab，Gitee，BitBucket等的支持 1.在配置服务器springcloud-config的pom.xml文件中添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-monitor&lt;/artifactId&gt;&lt;/dependency&gt; 2.在配置仓库的设置页面配置webhook，比如Github的配置如图 Payload URL 配置为配置服务器的monitor接口地址，path参数必须。如果你的配置服务器在内网，比如做本地测试时，还需要实现一下内网穿透（如frp）。在配置仓库项目中修改配置属性，提交代码，Github webhook就会触发自动更新，上图下方红色框为触发自动更新的记录。 自动更新配置未生效排查如果出现Github触发了自动更新，但服务的配置更新未生效的情况，则需要查看webhook的匹配规则与服务实例的ServiceID是否匹配，webhook的匹配规则为 spring.application.name:spring.cloud.config.profile:**，服务实例的ServiceID可通过spring.cloud.bus.id配置，如果没有配置，则默认为 1$&#123;vcap.application.name:$&#123;spring.application.name:application&#125;&#125;:$&#123;vcap.application.instance_index:$&#123;spring.application.index:$&#123;local.server.port:$&#123;server.port:0&#125;&#125;&#125;&#125;:$&#123;vcap.application.instance_id:$&#123;random.value&#125;&#125; 遵循app:index:id的格式， app：如果vcap.application.name存在，使用vcap.application.name，否则使用spring.application.name，默认值为application index：优先使用vcap.application.instance_index，如果不存在则依次使用spring.application.index、local.server.port、server.port， 默认值为0 id：如果vcap.application.instance_id存在，使用vcap.application.instance_id，否则给一个随机值 我们可以在服务项目中打开spring cloud bus的debug日志 123logging: level: org.springframework.cloud.bus: debug 通过DefaultBusPathMatcher的debug日志来查看是否匹配，如 1DEBUG 286196 --- [7O8XC9KNWbyDA-1] o.s.cloud.bus.DefaultBusPathMatcher : In match: hello-service:8081:c96f04c81dfce6dffaa9d116811d127c, hello-service:8081:c96f04c81dfce6dffaa9d116811d127c 如果没有匹配则可以按照webhook的匹配规则设置spring.cloud.bus.id值或vcap.application.instance_index值，如 123456789101112131415spring: application: name: hello-service cloud: config: discovery: service-id: config-server enabled: true profile: $&#123;spring.profiles.active:default&#125; bus: id: $&#123;spring.application.name&#125;:$&#123;spring.cloud.config.profile&#125;:$&#123;random.value&#125;#或vcap: application: instance_index: $&#123;spring.cloud.config.profile&#125; 配置更新未生效的另一个情况是查看是否用了@RefreshScope注解。 @RefreshScope细心的人会发现本文开头的测试接口类上加了@RefreshScope注解。 @RefreshScope是Spring Cloud提供的用来实现配置、实例热加载的注解。被@RefreshScope修饰的@Bean都是延迟加载的，即在第一次访问（调用方法）时才会被初始化，并且这些bean存于缓存中。当收到配置更新的消息时，缓存中的@RefreshScope bean会被清除，这样下次访问时将会重新创建bean，此时使用的就是最新的配置信息，从而实现配置的热加载。 总结本文分别示例了使用spring boot actuator与spring cloud bus来实现服务配置的更新及两者之间的区别， spring cloud bus一定程度上像是一个分布式的spring boot actuator。同时演示了使用webhook与spring cloud bus，monitor结合来实现配置自动更新的具体流程及可能遇到的问题。 认真生活，快乐分享 欢迎关注微信公众号：空山新雨的技术空间 ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/tags/SpringCloud/"},{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"}]},{"title":"Spring Cloud（七）：服务网关zuul过滤器","slug":"springcloud-7","date":"2020-02-20T05:47:57.000Z","updated":"2020-02-21T04:05:09.935Z","comments":true,"path":"springcloud-7.html","link":"","permalink":"http://blog.jboost.cn/springcloud-7.html","excerpt":"上文介绍了Zuul的基本使用与路由功能，本文接着介绍Zuul的核心概念 —— Zuul过滤器（filter）。 Zuul的功能基本通过Zuul过滤器来实现（类比于Struts的拦截器，只是Struts拦截器用到责任链模式，Zuul则是通过FilterProcessor来控制执行），在不同的阶段，通过不同类型的过滤器来实现相应的功能。","text":"上文介绍了Zuul的基本使用与路由功能，本文接着介绍Zuul的核心概念 —— Zuul过滤器（filter）。 Zuul的功能基本通过Zuul过滤器来实现（类比于Struts的拦截器，只是Struts拦截器用到责任链模式，Zuul则是通过FilterProcessor来控制执行），在不同的阶段，通过不同类型的过滤器来实现相应的功能。 Zuul过滤器过滤器类型zuul的过滤器根据对HTTP请求的不同处理阶段包括如下四种类型 pre ：在请求转发到后端目标服务之前执行，一般用于请求认证、确定路由地址、日志记录等 route ：转发请求，使用Apache HttpClient 或 Ribbon来构造对目标服务的请求 post ：在目标服务返回结果后对结果进行处理，比如添加响应头、收集统计性能数据等 error ：在请求处理的整个流程中如果出现错误，则会触发error过滤器执行，对错误进行处理 客户端请求经过zuul过滤器处理的流程如下图 zuul使用RequestContext来在过滤器之间传递数据，数据存于每个request的ThreadLocal，包含请求路由到哪里，错误，HttpServletRequest，HttpServletResponse 等这些数据都存储于RequestContext中。RequestContext 扩展了ConcurrentHashMap，所以我们可以根据需要将信息存于context中进行传递。 @EnableZuulProxy vs @EnableZuulServerzuul提供了两个注解 @EnableZuulProxy， @EnableZuulServer，来启用不同的过滤器集合。@EnableZuulProxy 启用的过滤器 是@EnableZuulServer 的超集， 它包含了@EnableZuulServer 的所有过滤器，proxy主要多了一些提供路由功能的过滤器（可见@EnableZuulServer 不提供路由功能，作为server模式而不是代理模式运行） @EnableZuulServer 注解启用的过滤器包括 filter类型 实现类 filter顺序值 功能说明 pre ServletDetectionFilter -3 检测请求是否通过Spring Dispatcher，并在RequestContext 中添加一个key为isDispatcherServletRequest， 值为true（不通过则为false）的属性 pre FormBodyWrapperFilter -1 解析Form data，为请求的下游进行重新编码 pre DebugFilter 1 如果请求参数设置了debug，则会将RequestContext.setDebugRouting() ，RequestContext.setDebugRequest() 设置为ture route SendForwardFilter 500 使用RequestDispatch servlet来转发请求，转发地址存于RequestContext中key为FilterConstants.FORWARD_TO_KEY的属性中，对于转发到当前应用的接口比较有用 post SendResponseFilter 1000 将代理请求的响应内容写到当前的响应中 error SendErrorFilter 0 如果RequestContext.getThrowable() 不为空，则会转发到/error，可以通过error.path来改变默认的转发路径/error @EnableZuulProxy 除了上面的过滤器，还包含如下过滤器 filter类型 实现类 filter顺序值 功能说明 pre PreDecorationFilter 5 确定路由到哪里，如何路由，依赖提供的RouteLocator，同时也为下游请求设置多个与proxy相关的header route RibbonRoutingFilter 10 使用ribbon，hystrix，以及内嵌的http client来发送请求，可在RequestContext中通过FilterConstants.SERVICE_ID_KEY 来找到路由Service的ID route SimpleHostRoutingFilter 100 使用Apache httpClient来发送请求到一个预先确定的url，可通过RequestContext.getRouteHost()来获取urls 由上可见@EnableZuulServer 注解并不包含往后端服务负载均衡地路由请求的代理功能，@EnableZuulProxy的PreDecorationFilter，RibbonRoutingFilter过滤器才能担当此任。PreDecorationFilter通过提供的DiscoveryClientRouteLocator 从 DiscoveryClient（如Eureka）与属性文件中加载路由定义， 为每个serviceId创建一个route，新服务添加进来，路由也会动态刷新。路由确定了，在RibbonRoutingFilter 中通过ribbon与hystrix结合来向后端目标服务发起请求，并进行负载均衡。过滤器的顺序值表示在同类型过滤器中的执行顺序，值越小越先执行。 自定义Zuul过滤器自定义的zuul过滤器与框架自带过滤器类似，包括四部分 过滤器类型，包括pre， route， post 过滤器顺序，定义在同类型过滤器中的执行顺序，数值越小越先执行 是否执行过滤，通过一些条件判断来确定是否执行该过滤器 过滤器执行体，定义具体执行的操作 比如我们需要在Http请求头中设置一个值，供请求链路的下游环节访问，则可以自定义一个过滤器如下， 12345678910111213141516171819202122232425@Componentpublic class ReqIdPreFilter extends ZuulFilter &#123; @Override public String filterType() &#123; return FilterConstants.PRE_TYPE; &#125; @Override public int filterOrder() &#123; return FilterConstants.PRE_DECORATION_FILTER_ORDER - 1; //在PreDecorationFilter过滤器之前执行 &#125; @Override public boolean shouldFilter() &#123; return true; &#125; @Override public Object run() throws ZuulException &#123; RequestContext ctx = RequestContext.getCurrentContext(); ctx.addZuulRequestHeader(\"reqId\", UUID.randomUUID().toString()); return null; &#125;&#125; 在请求的后续环节，比如后端服务的filter或接口中，则可直接从HttpServletRequest 获取该header值，如 1234@GetMapping(\"hello/reqId\")public String getReqId(HttpServletRequest request) &#123; return \"hello-service返回：\" + request.getHeader(\"reqId\");&#125; Zuul的错误处理在zuul过滤器的生命周期中，如果任何一个环节抛出异常，则error过滤器会被执行，SendErrorFilter只有当RequestContext.getThrowable()不为null时才会运行，会设置javax.servlet.error.* 属性到request中，然后将请求转发到spring boot的error page， 默认为BasicErrorController实现的/error接口。 有时候我们需要将返回响应格式进行统一，而默认的/error接口实现可能不满足要求，则可以自定义/error接口。需要实现ErrorController 接口以使默认的BasicErrorController 失效。 1234567891011121314151617@RestControllerpublic class ZuulErrorController implements ErrorController &#123; @RequestMapping(\"/error\") public Map&lt;String, String&gt; error(HttpServletRequest request)&#123; Map&lt;String, String&gt; result = Maps.newHashMap(); result.put(\"code\", request.getAttribute(\"javax.servlet.error.status_code\").toString()); result.put(\"message\", request.getAttribute(\"javax.servlet.error.message\").toString()); result.put(\"exception\", request.getAttribute(\"javax.servlet.error.exception\").toString()); return result; &#125; @Override public String getErrorPath() &#123; return \"/error\"; &#125;&#125; Zuul的服务降级当调用服务出现超时或异常时，在zuul侧可提供回调进行服务降级，返回默认响应结果，如 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@Componentpublic class MyFallbackProvider implements FallbackProvider &#123; @Override public String getRoute() &#123; return null; //指定这个回调针对的route Id，如果对所有route，则返回* 或null &#125; @Override public ClientHttpResponse fallbackResponse(String route, Throwable cause) &#123; if (cause instanceof HystrixTimeoutException) &#123; return response(HttpStatus.GATEWAY_TIMEOUT); &#125; else &#123; return response(HttpStatus.INTERNAL_SERVER_ERROR); &#125; &#125; private ClientHttpResponse response(final HttpStatus status) &#123; return new ClientHttpResponse() &#123; @Override public HttpStatus getStatusCode() throws IOException &#123; return status; &#125; @Override public int getRawStatusCode() throws IOException &#123; return status.value(); &#125; @Override public String getStatusText() throws IOException &#123; return status.getReasonPhrase(); &#125; @Override public void close() &#123; &#125; @Override public InputStream getBody() throws IOException &#123; Map&lt;String, String&gt; result = Maps.newLinkedHashMap(); result.put(\"code\", \"\" + status.value()); String msg = HttpStatus.GATEWAY_TIMEOUT == getStatusCode() ? \"请求服务超时\" : \"服务器内部错误\"; result.put(\"message\", msg); return new ByteArrayInputStream(new ObjectMapper().writeValueAsString(result).getBytes()); &#125; @Override public HttpHeaders getHeaders() &#123; HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_JSON); return headers; &#125; &#125;; &#125;&#125; 则当服务请求失败时，统一返回如下格式的响应 1234&#123; \"code\": \"500\", \"message\": \"服务器内部错误\"&#125; 总结本文主要对Zuul过滤器相关内容及自定义使用进行了介绍，同时对过滤器运行过程中异常的处理及服务调用失败的降级回调进行了简单说明。出于篇幅，开发过程中更具体的细节我们后续再继续探讨。 认真生活，快乐分享 欢迎关注微信公众号：空山新雨的技术空间 ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/tags/SpringCloud/"},{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"}]},{"title":"Spring Cloud（六）：服务网关zuul","slug":"springcloud-6","date":"2020-02-14T02:45:40.000Z","updated":"2020-02-14T09:17:54.862Z","comments":true,"path":"springcloud-6.html","link":"","permalink":"http://blog.jboost.cn/springcloud-6.html","excerpt":"通过前面几篇文章的介绍，Spring Cloud微服务架构可通过Eureka实现服务注册与发现，通过Ribbon或Feign来实现服务间的负载均衡调用，通过Hystrix来为服务调用提供服务降级、熔断机制避免雪崩效应，通过Spring Cloud Config实现服务配置的集中化管理。微服务架构内部管理的基本组件差不多都已涵盖了，但是我们的服务最终是需要提供给客户端访问的，客户端如何来访问这些微服务，就需要引入一个叫服务网关的组件了。","text":"通过前面几篇文章的介绍，Spring Cloud微服务架构可通过Eureka实现服务注册与发现，通过Ribbon或Feign来实现服务间的负载均衡调用，通过Hystrix来为服务调用提供服务降级、熔断机制避免雪崩效应，通过Spring Cloud Config实现服务配置的集中化管理。微服务架构内部管理的基本组件差不多都已涵盖了，但是我们的服务最终是需要提供给客户端访问的，客户端如何来访问这些微服务，就需要引入一个叫服务网关的组件了。 zuulzuul是netflix提供的一个基于JVM的路由与服务端负载均衡器。它在客户端与后端服务之间建立了一道关卡，客户端所有请求必须经过zuul转发到后端对应的微服务，返回结果再经由zuul返回给客户端。zuul与Eureka，Config组合的基本结构如图 zuul作为Eureka Client从Eureka Server获取其它微服务的配置信息，从而可以将客户端请求通过Service ID来负载均衡地转发到后端的服务实例，同时也作为Config Client从Config Server获取自身所需的配置信息。 在netflix内部，zuul被用来实现安全认证、动态路由、反向代理、服务迁移、服务削峰、压力测试、金丝雀测试（灰度发布测试）等功能。本文介绍zuul的基本使用与路由规则。 基本使用创建maven项目 springcloud-zuul pom.xml中引入依赖 spring-cloud-starter-netflix-zuul 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-zuul&lt;/artifactId&gt;&lt;/dependency&gt; application.yml配置文件中添加必要的配置，主要是eureka客户端配置 1234567891011spring: application: name: zuul-serverserver: port: 8765eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ 启动类添加注解 @EnableZuulProxy 12345678@SpringBootApplication@EnableZuulProxypublic class ZuulApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ZuulApplication.class, args); &#125;&#125; 一如既往的简单，Spring Cloud之所以流行就是因为它基于Spring Boot将一些通用的功能进行了开箱即用的封装，使得开发者简单几步就能快速集成一个微服务框架。 依次启动前文所创建的springcloud-eureka, springcloud-config, springcloud-eureka-client, springcloud-zuul，http://localhost:8765/hello-service/hello 返回 Hello, welcome to spring cloud. env: hello-service-dev, value: hello-service-dev 可见通过zuul的请求转发到了hello-service。 为了验证zuul转发请求具备负载均衡的能力，可以将springcloud-eureka-client 中的hello接口返回值做一些调整，并改变端口重启一个实例，再次请求http://localhost:8765/hello-service/hello 将能看到返回结果在两者之间切换。 以上配置文件中并没有加任何路由配置，zuul是怎么将请求正确转发到对应的微服务的呢？ 请看下面的路由规则。 路由规则 默认路由规则 zuul提供了默认的路由规则，不需要任何配置就会默认将注册的服务进行路径映射。我们可以通过actuator提供的接口来查看，在application.yml中添加配置 12345management: endpoints: web: exposure: include: \"*\" 放开actuator的其它接口访问（默认只放开了/info 与/health接口）， 浏览器中访问 http://localhost:8765/actuator/routes， 可以看到返回的zuul默认的路由映射关系 zuul默认将 /service-id/** 的请求路由到Service ID（即spring.application.name的值）为 service-id的服务，如 /hello-service/hello，将转发到hello-service服务的/hello接口。 自定义路由规则 我们看到zuul的默认路由规则将config-server也映射出来了，对于这类内部服务我们不希望暴露，则可以通过 zuul.ignoredServices 来进行屏蔽，在application.yml配置文件中添加 12zuul: ignored-services: \"config-server\" 重启，再次查看http://localhost:8765/actuator/routes ， config-server已经被屏蔽了。 通过zuul.routes可添加自定义路由，可以有 zuul.routes.{route-name}.path + zuul.routes.{route-name}.serviceId或url 或 zuul.routes.{service-id}: path 两个格式， 如下 12345678910zuul: ignored-services: \"config-server\" routes: hello: path: /hi/** serviceId: hello-service hello-service: /hi2/** jboost: path: /jboost/** url: http://blog.jboost.cn 访问 http://localhost:8765/hi/hello 或 http://localhost:8765/hi2/hello 都将路由到 hello-service的hello接口，访问 http://localhost:8765/jboost/ 将访问到jboost博客首页。添加自定义路由后，默认路由仍然存在， 你仍然可以通过 http://localhost:8765/hello-service/hello 来访问 hello-service的hello接口。 默认的路由规则将Service ID作为匹配路径，看起来有点长，我们想将匹配路径缩短一点，比如hello-service的匹配路径想改为 /hello/**， 而不是/hello-service/**， 如果像上面配置，一个微服务系统可能涉及几十甚至上百个服务，那配置起来将是一场噩梦。别急， zuul提供了 ServiceRouteMapper 接口来解决这一问题，其中 PatternServiceRouteMapper 可以基于正则表达式来进行路由抽取。 创建一个配置类，注入一个 PatternServiceRouteMapper 的bean，如下 12345678910@Configurationpublic class ZuulConfiguration &#123; @Bean public PatternServiceRouteMapper serviceRouteMapper() &#123; return new PatternServiceRouteMapper( \"(?&lt;name&gt;^.+)-(?&lt;postfix&gt;.+$)\", \"$&#123;name&#125;\"); &#125;&#125; 该实现将会对所有服务的路由进行调整，service id 形如 name-postfix的匹配路径为 /name/**， 如hello-service 匹配 /hello/**。 如果正则表达式匹配失败，则还是以默认规则进行路由，如果匹配成功，则默认规则失效，但在配置文件中定义的路由仍然有效。上述验证中，你都可以通过 http://localhost:8765/actuator/routes 来查看当前生效的路由。 其它配置zuul使用Ribbon来定位服务实例，所有请求都在hystrix command里执行，所以在zuul中可以添加Ribbon， Hystrix相关配置（具体参考前面Ribbon、Hystrix相关文章） zuul.ignoredPatterns 对某些路径进行屏蔽，如 /**/admin/** 将会屏蔽所有路径中包含admin的接口访问 zuul.sensitiveHeaders 对一些header进行过滤，不传递给后端服务，默认包括Cookie,Set-Cookie,Authorization， 如果要让zuul发送所有header，则需要显式地将sensitiveHeaders置空值 zuul.prefix 为所有映射添加前缀，如/api， 这样route里配的 /myusers/** 就能匹配客户端请求的/api/myusers/**。默认zuul代理在转发时，前缀会被移除，通过设置zuul.stripPrefix=false可不移除 总结本文简单介绍了zuul的基本使用与路由规则，更高阶的应用我们后面继续。 认真生活，快乐分享 欢迎关注微信公众号：空山新雨的技术空间 ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/tags/SpringCloud/"},{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"}]},{"title":"Spring Cloud（五）：服务配置管理中心","slug":"springcloud-5","date":"2020-02-11T02:59:41.000Z","updated":"2020-02-11T07:39:11.424Z","comments":true,"path":"springcloud-5.html","link":"","permalink":"http://blog.jboost.cn/springcloud-5.html","excerpt":"","text":"Spring Cloud Config为微服务提供了集中化的配置管理，可以通过git仓库的形式来对各个服务的配置属性进行管理维护，在配置更新时，可通过消息总线的方式实现动态更新，而不需要重启服务。 架构Spring Cloud Config属于CS架构，包括Config Server与Config Client， Config Server从配置存储库（可以是git，svn，jdbc数据库，或本地文件系统）获取配置属性，Config Client通过http请求对应的配置属性。如图 Config Server在客户端请求配置信息时，从git获取配置信息（可以配置为启动时即从git获取）返回给客户端，当配置发生更新时，可通过webhook的方式通知到Config Server，Config Server发出RefreshRemoteApplicationEvent 事件，通知客户端更新配置信息。 Config Server搭建一个Config Server很简单。 首先pom.xml中引入依赖， 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 加入Eureka Client是使配置服务作为一个微服务注册到Eureka被其它微服务（作为Config Client）发现。 然后application.yml中添加配置，123456789101112131415161718192021222324spring: application: name: config-server cloud: config: server: git: uri: https://github.com/ronwxy/jboost-config searchPaths: '&#123;application&#125;' # 按应用名称分文件夹目录存储配置文件，只在应用名所在目录及顶层目录下寻找配置文件 cloneOnStart: true # 启动时就获取配置，否则只有当客户端请求时才去获取配置 basedir: D:\\config #本地缓存路径 forcePull: true # 在本地配置被污染（篡改）时， 强制拉取远程配置覆盖 profile: $&#123;spring.profiles.active:default&#125; discovery: enabled: falseserver: port: 8888eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ 上述配置使Config Server从github仓库获取配置信息，searchPaths: ‘{application}’ 可以将每个服务的配置属性文件放入仓库的同名文件夹下。 注意这里的引号是必须的，否则因为不符合yaml文件语法导致不生效。 定期刷新： spring.cloud.config.server.git.refreshRate 单位秒， 默认为0， 表示每次请求时，config server都会从git 仓库获取更新的配置。本地缓存： 默认本地副本存在临时目录中，有些操作系统可能会定时清理临时目录，导致问题，设置配置的本地目录：spring.cloud.config.server.git.basedir 最后启动类上添加注解 12345678@SpringBootApplication@EnableConfigServerpublic class ConfigServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConfigServerApplication.class, args); &#125;&#125; 完成以上三步一个简单的Config Server就搭建完成了，启动项目，访问 http://localhost:8888/hello-service/dev 可获取到仓库 hello-service 目录下对应profile为dev的配置属性文件hello-service-dev.yml与默认配置文件（包括同目录下application.yml与仓库根目录下的application.yml, application-dev.yml配置文件） 通过url获取配置的访问方式： /{application}/{profile}[/{label}] /{application}-{profile}.yml /{label}/{application}-{profile}.yml /{application}-{profile}.properties /{label}/{application}-{profile}.properties 其中 application是spring.config.name的值，profile可以是以逗号分隔的列表，label是git的分支，默认是master。 Config Server的高可用： 起多个实例，客户端配置spring.cloud.config.uri 以逗号隔开配多个uri 或将实例注册到服务注册中心 对于500,401等异常，客户端不会重试其它实例， 只在实例挂掉或连接超时时，才会重试其它实例 Config Client客户端以springcloud-eureka-client项目为基础进行改造 在pom.xml中添加config相关依赖 12345678 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 添加actuator可以暴露相关接口，如/env 来查看配置属性加载情况，springboot2中actuator默认只开放/info /health 两个接口，可通过如下配置放开（生产环境根据需要调整） 12345management: endpoints: web: exposure: include: \"*\" 添加bootstrap.yml配置文件 123456789101112spring: application: name: hello-service cloud: config: discovery: enabled: true serviceId: config-server failfast: true # 在启动时如果连不上config server，则启动失败 name: $&#123;spring.application.name&#125; profile: $&#123;spring.profiles.active:default&#125; 依次启动服务注册中心 springcloud-eureka， 配置管理服务 springcloud-config， 配置客户端 springcloud-eureka-client， 访问 http://localhost:8080/actuator/env 可看到配置客户端加载的配置信息。在配置客户端启动时，控制台也会打印从配置服务获取配置的相关信息，如 1Located property source: OriginTrackedCompositePropertySource &#123;name='configService', propertySources=[MapPropertySource &#123;name='configClient'&#125;, OriginTrackedMapPropertySource &#123;name='https://github.com/ronwxy/jboost-config/hello-service/hello-service-dev.yml'&#125;, OriginTrackedMapPropertySource &#123;name='https://github.com/ronwxy/jboost-config/application-dev.yml'&#125;, OriginTrackedMapPropertySource &#123;name='https://github.com/ronwxy/jboost-config/hello-service/application.yml'&#125;, OriginTrackedMapPropertySource &#123;name='https://github.com/ronwxy/jboost-config/application.yml'&#125;]&#125; 这样，在客户端就可以通过 @ConfigurationProperties注解的属性类， @Value 注解，或Environment对象来访问相关属性，如 1234567@Autowiredprivate Environment env;env.getProperty(\"app\")@Value(\"$&#123;app&#125;\")private String app; 客户端重试机制： 首先设置 spring.cloud.config.failfast=true 然后添加 spring-retry， spring-boot-starter-aop 依赖 默认进行6次重试，每次间隔一开始1s，然后每次1.1倍递增。如果要自定义，则通过spring.cloud.config.retry.* 配置参数， 或通过定义一个ID为configServerRetryInterceptor 的RetryOperationsInterceptor 类型的@Bean，可通过RetryInterceptorBuilder 来创建。 本文示例源码地址： https://github.com/ronwxy/springcloud-demos 认真生活，快乐分享 欢迎关注微信公众号：空山新雨的技术空间 ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/tags/SpringCloud/"},{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"}]},{"title":"mybatis缓存，从一个“灵异”事件说起","slug":"mybatis-cache","date":"2020-01-20T03:19:30.000Z","updated":"2020-08-26T01:37:36.186Z","comments":true,"path":"mybatis-cache.html","link":"","permalink":"http://blog.jboost.cn/mybatis-cache.html","excerpt":"刚准备下班走人，被一开发同事叫住，让帮看一个比较奇怪的问题：Mybatis同一个Mapper接口的查询方法，第一次返回与第二次返回结果不一样，百思不得其解！","text":"刚准备下班走人，被一开发同事叫住，让帮看一个比较奇怪的问题：Mybatis同一个Mapper接口的查询方法，第一次返回与第二次返回结果不一样，百思不得其解！ 问题Talk is cheap. Show me the code. 该问题涉及的主要代码实现包括 mapper接口定义 123public interface GoodsTrackMapper extends BaseMapper&lt;GoodsTrack&gt; &#123; List&lt;GoodsTrackDTO&gt; listGoodsTrack(@Param(\"criteria\") GoodsTrackQueryCriteria criteria);&#125; xml定义 123&lt;select id=\"listGoodsTrack\" resultType=\"xxx.GoodsTrackDTO\"&gt; SELECT ...&lt;/select&gt; service定义 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Service@Transactional(propagation = Propagation.SUPPORTS, readOnly = true, rollbackFor = Exception.class)public class GoodsTrackService extends BaseService&lt;GoodsTrack, GoodsTrackDTO&gt; &#123; @Autowired private GoodsTrackMapper goodsTrackMapper; public List&lt;GoodsTrackDTO&gt; listGoodsTrack(GoodsTrackQueryCriteria criteria)&#123; return goodsTrackMapper.listGoodsTrack(criteria); &#125; public List&lt;GoodsTrackDTO&gt; goodsTrackList(GoodsTrackQueryCriteria criteria)&#123; List&lt;GoodsTrackDTO&gt; listGoodsTrack = goodsTrackMapper.listGoodsTrack(criteria); Map&lt;String, GoodsTrackDTO&gt; goodsTrackDTOMap = new HashMap&lt;String, GoodsTrackDTO&gt;(); for (GoodsTrackDTO goodsTrackDTO : listGoodsTrack)&#123; String goodsId = String.valueOf(goodsTrackDTO.getGoodsId()); if (!goodsTrackDTOMap.containsKey(goodsId))&#123; goodsTrackDTOMap.put(goodsId, goodsTrackDTO); &#125;else &#123; GoodsTrackDTO goodsTrack = goodsTrackDTOMap.get(goodsId); int num = goodsTrack.getGoodsNum() + goodsTrackDTO.getGoodsNum(); goodsTrack.setGoodsNum(num); &#125; &#125; List&lt;GoodsTrackDTO&gt; list = new ArrayList(goodsTrackDTOMap.values()); return list; &#125;&#125;@Service@Transactional(propagation = Propagation.SUPPORTS, readOnly = true, rollbackFor = Exception.class)public class GoodsOrderService extends BaseService&lt;GoodsOrder, GoodsOrderDTO&gt; &#123; @Autowired private GoodsTrackService goodsTrackService; @Override public GoodsOrderDTO create(GoodsOrderDTO goodsOrderDTO) &#123; //... List&lt;GoodsTrackDTO&gt; rs1 = goodsTrackList(criteria); //... List&lt;GoodsTrackDTO&gt; rs2 = listGoodsTrack(criteria); //... &#125;&#125; 大致逻辑就是在 GoodsTrackService 定义了两个查询方法，一个是直接从数据库中获取数据，第二个是从数据库中获取数据后进行了一些加工（通过某个字段进行合并累加，类似sum group by），然后在GoodsOrderService 的同一个方法（该方法是一个事务方法 ）中调用这两个查询，发现rs2中的数据存在问题， 期望是都应该与数据库表的数据一致，但其中部分数据却与查出后进行了修改的rs1中的一致。 即本来期望查出的结果如图 rs2与数据库表数据一致。 但是经上面代码运行的结果却如图 rs2中的数据包含了rs1中的数据。 定位初步看，listGoodsTrack 方法直接调用的mapper方法 goodsTrackMapper.listGoodsTrack(criteria) 没做任何应用层的处理，第一反应是缓存的原因。 我问前面的查询有没有改变查询返回的结果（一开始没细看具体实现），答曰没有。折腾一阵后，返过去细看 goodsTrackList 的实现，果然还是眼见为实、耳听为虚。在该方法中，通过goodsId对返回的列表进行分组，对goodsNum进行累加，最后返回累加后的几个对象。但是在累加的时候，是直接作用于返回结果对象的，明明就是改变了查询结果（居然说没有？！！）。 这就是问题所在了，mybatis在同一个事务中，对同一个查询（同样的sql，同样的参数）的返回结果进行了缓存（称为一级缓存），下一次做同样的查询时，如果中间没有任何更新操作，则直接返回缓存的数据，而在本例中因为对缓存数据做了人为的修改，所以最后导致查出的数据与数据库不一致。 mybatis缓存机制简单介绍下mybatis的两级缓存机制 一级缓存：一级缓存包括SqlSession与STATEMENT两种级别，默认在 SqlSession 中实现。在一次会话中，如果两次查询sql相同，参数相同，且中间没有任何更新操作，则第二次查询会直接返回第一次查询缓存的结果，不再请求数据库。如果中间存在更新操作，则更新操作会清除掉缓存，后面的查询就会访问数据库了。STATEMENT级别则每次查询都会清掉一级缓存，每次查询都会进行数据库访问。 二级缓存：二级缓存则是在同一个namesapce的多个 SqlSession 间共享的缓存，默认未开启。当开启二级缓存后，数据查询的流程就是 二级缓存 ——&gt; 一级缓存 ——&gt; 数据库， 同一个namespace下的更新操作，会影响同一个Cache。 如何开启二级缓存 需要在mybatis-config.xml中设置：123&lt;settings&gt; &lt;setting name=\"cacheEnabled\" value=\"true\"/&gt;&lt;/settings&gt; 然后在mapper的xml文件的&lt;mapper&gt;下设置cache相关配置：12345&lt;cache eviction=\"LRU\" flushInterval=\"60000\" size=\"512\" readOnly=\"true\"/&gt; 支持的属性： type：cache使用的类型，默认是PerpetualCache eviction： 回收的策略，常见的有LRU，FIFO flushInterval： 配置一定时间自动刷新缓存，单位毫秒 size： 最多缓存的对象个数 readOnly： 是否只读，若配置为可读写，则需要对应的实体类实现Serializable接口 blocking： 如果缓存中找不到对应的key，是否会一直blocking，直到有对应的数据进入缓存 也可以使用 &lt;cache-ref namespace=&quot;mapper.UserMapper&quot;/&gt; 来与另一个mapper共享二级缓存 解决已经定位到是由于mybatis的一级缓存导致，那如何解决本文提到的问题呢？ 基本上有三个解决方向。 使用缓存的方案 既然要使用缓存，那就不能更改缓存的数据，此时我们可以在需要更改数据的地方把数据做一次副本拷贝，使其不改变缓存数据本身， 如 12345678910for (GoodsTrackDTO goodsTrackDTO : listGoodsTrack)&#123; String goodsId = String.valueOf(goodsTrackDTO.getGoodsId()); if (!goodsTrackDTOMap.containsKey(goodsId))&#123; goodsTrackDTOMap.put(goodsId, ObjectUtil.clone(goodsTrackDTO)); &#125;else &#123; GoodsTrackDTO goodsTrack = goodsTrackDTOMap.get(goodsId); int num = goodsTrack.getGoodsNum() + goodsTrackDTO.getGoodsNum(); goodsTrack.setGoodsNum(num); &#125;&#125; 使用ObjectUtil.clone()方法（hutool工具包中提供）对需要更改的数据做副本拷贝。 禁用缓存的方案 在xml的sql定义中添加 flushCache=”true” 的配置，使该查询不使用缓存，如下 123&lt;select id=\"listGoodsTrack\" resultType=\"xxx.GoodsTrackDTO\" flushCache=\"true\"&gt; SELECT ...&lt;/select&gt; 禁用缓存的另一种方案是将一级缓存直接设置为STATEMENT来进行全局禁用，在mybatis-config.xml中配置： 123&lt;settings&gt; &lt;setting name=\"localCacheScope\" value=\"STATEMENT\"/&gt;&lt;/settings&gt; 避开缓存的方案 再定义一个实现相同查询的mapper方法，id不一样来避开使用相同的缓存，这种做法就不怎么优雅了。 123&lt;select id=\"listGoodsTrack2\" resultType=\"xxx.GoodsTrackDTO\" flushCache=\"true\"&gt; SELECT ...&lt;/select&gt; 避开缓存的另一种做法是不使用事务，使两个查询不在一个SqlSession中，但有时候事务是必须的，所以得分场景来。 另外由于mybatis的缓存都是基于本地的，在分布式环境下可能导致读取的数据与数据库不一致，比如一个服务实例两次读取中间，另一个服务实例对数据进行了更新，则后一次读取由于缓存还是读取的旧数据，而不是更新后的数据，可能导致问题。这时可以通过将缓存设置为STATEMENT级别来禁用mybatis缓存，通过Redis，MemCached等来提供分布式的全局缓存。 认真生活，快乐分享 欢迎关注微信公众号：空山新雨的技术空间 ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.jboost.cn/categories/Java/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://blog.jboost.cn/tags/mybatis/"}]},{"title":"Spring Cloud（四）：啥是服务降级与熔断之Hystrix","slug":"springcloud-4","date":"2020-01-19T01:31:02.000Z","updated":"2020-01-20T02:50:05.938Z","comments":true,"path":"springcloud-4.html","link":"","permalink":"http://blog.jboost.cn/springcloud-4.html","excerpt":"在微服务的架构中，一个业务的实现通常包括多层服务的调用，底层基础服务的故障可能会导致上层服务级联发生故障，进而故障不断蔓延导致系统整体不可用，这种现象称为服务的雪崩效应。","text":"在微服务的架构中，一个业务的实现通常包括多层服务的调用，底层基础服务的故障可能会导致上层服务级联发生故障，进而故障不断蔓延导致系统整体不可用，这种现象称为服务的雪崩效应。 断路器断路器（CircuitBreaker， 或叫熔断器）是用来避免服务雪崩效应，提升系统整体可用性的一种技术手段。通过在一个时间窗口内监测服务的调用失败情况，在失败时通过回调返回默认实现（这叫服务降级），当失败达到一定程度时，后续的调用直接导致快速失败，不再访问远程服务（这叫熔断机制），防止不断尝试调用可能会失败的服务，以使服务有机会恢复，一定时间后，当断路器监测到服务恢复后，会继续尝试调用。 断路器原理有点类似于电路中的保险丝或电闸，当发现电路中出现短路异常情况时，通过保险丝熔断或电闸跳闸来断开电路，避免事故发生。 Hystrix服务降级当使用Hystrix时，每一个远程服务的调用都被封装到一个HystrixCommand中，HystrixCommand有两个主要方法：run(), getFallback()。其中run方法封装了远程服务的调用逻辑，如果run方法超时或者抛出异常，并且启用了服务降级，则会调用getFallback方法来进行降级处理。 降级处理由 HystrixCommandProperties.Setter 中定义的配置属性来控制，主要包括： fallbackEnabled， 是否启用降级处理，如果启用则在超时或异常时调用getFallback来进行降级处理，默认启用 fallbackIsolationSemaphoreMaxConcurrentRequests，控制getFallback方法并发请求的信号量，默认为10，如果请求超过了信号量限制，则不再尝试调用getFallback方法，而是快速失败 executionIsolationThreadInterruptOnFutureCancel，隔离策略为THREAD，当Future#cancel(true)时，是否进行中断处理，默认为false executionIsolationThreadInterruptOnTimeout，隔离策略为THREAD，当执行线程超时时，是否进行中断，默认为true executionTimeoutInMilliseconds，执行超时时间，默认为1000ms，如果隔离策略为THREAD（线程池隔离），且配置了executionIsolationThreadInterruptOnTimeout=true，则执行线程将被中断，如果隔离策略为SEMAPHORE（信号量隔离），则终止操作，信号量隔离下执行线程与主线程是同一个线程，所以不会中断线程处理 在进行降级处理调用getFallback方法时，需注意： 该方法最大并发数受fallbackIsolationSemaphoreMaxConcurrentRequests控制，默认为10，如果失败率很高，则需配置该参数，如果并发数超过了配置，则不会执行getFallback，而是快速失败，抛出异常“HystrixRuntimeException: xxx fallback executionrejected” 尽量避免在getFallback中进行网络请求，而是能快速返回的缓存数据或静态数据（如默认值）；如果需要做网络请求，则应该是调用另一个被Hystrix保护的请求，即对fallback进行串联，第一个fallback中请求网络做业务调用，第二个fallback中回调缓存或静态数据 上文提到Hystrix的线程池隔离策略与信号量隔离策略，两者如何理解？ 线程池隔离：执行在一个单独的线程中，通过线程池中线程数量来控制并发请求量。每一个服务使用一个单独的线程池进行隔离，避免互相影响。这种策略下的服务调用是异步的，可通过hystrix来配置超时。 信号量隔离：执行在调用线程中，通过信号量来控制控制并发请求量（executionIsolationSemaphoreMaxConcurrentRequests， 默认为10），如果并发量超过该值，则调用getFallback方法对服务进行降级。这种策略下的服务调用是同步的，无法对调用进行超时配置，只能通过调用协议（如http）的超时。信号量隔离策略一般只有在高并发量的情况下使用（如一秒几百次），这种情况下使用单独的线程池开销比较大；或者如果需要在调用服务的线程中，如RequestInterceptor中使用ThreadLocal中的变量，也可以通过将隔离策略设置为信号量来实现(hystrix.command.default.execution.isolation.strategy=SEMAPHORE) 注： 如果只是需要在服务调用中使用安全上下文 SecurityContext， 则也可以通过配置 hystrix.shareSecurityContext=true 来实现，这样Hystrix的并发策略插件会将SecurityContext从主线程传递到Hystrix command使用的线程。 线程池隔离策略与信号量隔离策略两者之间的区别 隔离策略 实现原理 调用模式 是否支持超时配置 降级实现 资源消耗 线程池隔离 每个服务使用单独的线程池 异步调用 支持 线程池满则请求拒绝，降级处理 较大，容易造成服务器负载高 信号量隔离 使用信号量的计数器 同步调用 不支持 信号量达到最大值则请求拒绝，降级处理 较小 Hystrix熔断机制Hystrix客户端会对调用失败情况进行采样统计。当在一个时间窗口（由metrics.rollingStats.timeInMilliseconds配置， 默认10s）内，调用某个服务超过一定次数（由circuitBreaker.requestVolumeThreshold配置，默认20），失败率超过一定比例（由circuitBreaker.errorThresholdPercentage配置，默认50%），则熔断开关打开，调用会被快速失败（不再进行远程调用），如果开发者提供了fallback，则会调用fallback进行降级处理，如果没有，则抛出 异常。 调用失败包括如下几种情况： 调用中抛出异常 调用超时 线程池拒绝 信号量拒绝 熔断开关的状态： 闭合（closed）：如果配置了熔断开关强制闭合，或者当前的请求失败率没有超过设置的阈值，则熔断开关处于闭合状态，不启动熔断机制。但这时如果调用超时或失败，仍会进行降级处理（除非fallbackEnabled为false） 打开（open）：如果配置了熔断开关强制打开，或者当前的请求失败率超过了设置的阈值，则熔断开关打开，启动熔断机制，直接进行降级处理，不再进行远程调用 半打开（half-open）：当熔断开关处理打开状态，需要在一定的时间窗口后进行重试，检测服务是否恢复，这种状态就是半打开状态。如果测试成功则关闭熔断开关，否则还是处于打开状态 Hystrix熔断开关的状态关系如图所示 熔断相关的参数配置（HystrixCommandProperties.Setter）： circuitBreakerEnabled， 是否开启熔断机制，默认true circuitBreakerForceOpen，是否强制打开熔断开关，如果为true，则对请求进行强制降级，默认为false circuitBreakerForceClosed， 是否强制关闭熔断开关，默认为false circuitBreakerRequestVolumeThreshold， 在熔断开关闭合的情况下，一个采样时间窗口内需要进行至少多少个请求才进行采用统计计算失败率，默认为20 circuitBreakerErrorThresholdPercentage， 在一个采样时间窗口内，失败率超过该值，则打开熔断开关，进行快速失败，默认采样时间窗口为10s，失败率为50% circuitBreakerSleepWindowInMilliseconds， 熔断后的重试时间窗口，在该时间窗口允许一次重试，如果重试成功，则关闭熔断开关，否则还是打开状态，默认5s 案例演示熔断是服务调用端的一种保护机制，因此通常与Feign结合使用，Feign的Hystrix支持在Dalston版本之前，hystrix只要在类路径中，feign默认就会自动将所有方法封装到断路器中，Dalston版及以后的版本改变了这一做法，需要进行显示配置 feign.hystrix.enabled=true。 本文案例还是基于前面创建的springcloud-eureka（注册中心）， springcloud-eureka-client（一个简单的hello service）两个项目。 新建springcloud-hystrix项目，pom.xml中引入相关依赖 12345678910111213141516 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; applicaiton.yml配置文件中添加必要配置 12345678eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/feign: hystrix: enabled: true 启动类添加必要注解 1234@SpringBootApplication@EnableCircuitBreaker@EnableFeignClientspublic class HystrixApplication 其它Feign Client类与Controller类详见源码：https://github.com/ronwxy/springcloud-demos springcloud-hystrix项目 测试 这里分四种情形分别进行演示。依次启动springcloud-eureka， springcloud-eureka-client（debug模式启动，并在hello接口里设置断点，模拟超时），springcloud-hystrix 启用断路器，未指定fallback 12@FeignClient(name = \"hello-service\")public interface HelloClient 访问 http://localhost:8084/hystrix0 得到结果如下 抛出异常： HystrixRuntimeException: HelloClient#hello() timed-out and no fallback available.] with root cause java.util.concurrent.TimeoutException: null 启用断路器，指定fallback 12@FeignClient(name = \"hello-service\", contextId = \"hello-with-fallback\", fallback = HelloClientFallback.class)public interface HelloClient1 访问 http://localhost:8084/hystrix1 得到结果返回 “调用hello-service返回：this is returned by fallback”， 不会抛出异常。 启用断路器，指定fallbackFactory 12@FeignClient(name = \"hello-service\", contextId = \"hello-with-fallbackFactory\", fallbackFactory = HelloClientFallbackFactory.class)public interface HelloClient2 访问 http://localhost:8084/hystrix2 得到结果返回 “调用hello-service返回：this is returned from fallbackFactory, cause: com.netflix.hystrix.exception.HystrixTimeoutException”，不会抛出异常。 不启用断路器 12345678910@FeignClient(name = \"hello-service\", contextId = \"hello-without-circuitBreaker\", configuration = DisableCircuitBreakerConfiguration.class)public interface HelloClient3public class DisableCircuitBreakerConfiguration &#123; @Bean @Scope(\"prototype\") public Feign.Builder feignBuilder() &#123; return Feign.builder(); &#125;&#125; 对单个feign client禁用断路器可以配置一个注入了 prototype scope的 Feign.Builder实例的配置类来实现。 访问 http://localhost:8084/hystrix3 得到结果如下 抛出异常： feign.RetryableException: Read timed out executing GET http://hello-service/hello] with root cause java.net.SocketTimeoutException: Read timed out 可见，在启用断路器，不指定fallback时，抛出HystrixRuntimeException异常，指定fallback时，调用fallback方法降级处理，但获取不到失败原因，如果需要获取失败原因，可使用fallbackFactory，不启用断路器时，抛出feign.RetryableException异常。 注意上面的contextId， 当使用同一个名称或url来创建多个指向同一服务的feign client时， 需要使用contextId来避免配置bean的名称冲突。该属性可以改变feign 客户端的ApplicationContext的名称，覆盖feign客户端别名，作为客户端配置bean名称的一部分。 本文示例代码 认真生活，快乐分享 欢迎关注微信公众号：空山新雨的技术空间 ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/tags/SpringCloud/"},{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"}]},{"title":"Spring Cloud（三）：Web服务客户端之Feign","slug":"springcloud-3","date":"2020-01-15T08:59:09.000Z","updated":"2020-01-19T11:29:53.145Z","comments":true,"path":"springcloud-3.html","link":"","permalink":"http://blog.jboost.cn/springcloud-3.html","excerpt":"前文介绍了实现客户端负载均衡的Ribbon，但直接使用Ribbon的API来实现服务间的调用相对较为繁琐，服务间的调用能否像本地接口调用一样便捷、透明，更符合编程习惯呢？ Feign就是用来干这事的。","text":"前文介绍了实现客户端负载均衡的Ribbon，但直接使用Ribbon的API来实现服务间的调用相对较为繁琐，服务间的调用能否像本地接口调用一样便捷、透明，更符合编程习惯呢？ Feign就是用来干这事的。 FeignFeign是一个声明式的Web服务客户端，让服务之间的调用变得非常简单——定义带@FeignClient注解的接口，本地直接@Autowired 接口，通过调用接口的方法来实现远程服务的调用。 支持的注解包括Feign注解与JAX-RS（Java API for RESTful Web Services）注解。 每一个Feign的客户端都包含一系列对应的组件，Spring Cloud通过FeignClientsConfiguration 为每一个命名的Feign客户端创建一个组件集合，包括feign.Decoder，feign.Encoder，feign.Contract等。 Feign提供的默认bean实现及说明 Bean类型 默认实现类 说明 Decoder ResponseEntityDecoder ResponseEntityDecoder封装了SpringDecoder，解码器，将服务的响应消息进行解码 Encoder SpringEncoder 编码器 Logger Slf4jLogger 日志框架 Contract SpringMvcContract 支持注解契约，使用SpringMvcContract可以对Spring MVC注解提供支持 Feign.Builder HystrixFeign.Builder 使用断路器来装饰Feign接口 Client LoadBalancerFeignClient 如果是ribbon则 LoadBalancerFeignClient， 如果是spring cloud LoadBalancer 则 FeignBlockingLoadBalancerClient，默认ribbon 跟Ribbon类似，可以通过配置类来自定义Feign客户端，如 123456789101112131415@FeignClient(name = \"hello-service\", configuration = CustomConfiguration.class)public interface StoreClient &#123; //..&#125;public class CustomConfiguration &#123; @Bean public Contract feignContract() &#123; return new feign.Contract.Default(); &#125; @Bean public BasicAuthRequestInterceptor basicAuthRequestInterceptor() &#123; return new BasicAuthRequestInterceptor(\"user\", \"password\"); &#125;&#125; 这样Feign客户端就包含了FeignClientsConfiguration 与CustomConfiguration 中定义的组件，并且后者会覆盖前者（即自定义配置的优先级高于默认配置）。 自定义配置类不需要加注解@Configuration，如果加了且被@ComponentScan扫描到，则将成为所有Feign客户端的默认配置 同样Feign客户端也支持通过配置文件来配置 12345678910111213141516feign: client: config: feignName: connectTimeout: 5000 readTimeout: 5000 loggerLevel: full errorDecoder: com.example.SimpleErrorDecoder retryer: com.example.SimpleRetryer requestInterceptors: - com.example.FooRequestInterceptor - com.example.BarRequestInterceptor decode404: false encoder: com.example.SimpleEncoder decoder: com.example.SimpleDecoder contract: com.example.SimpleContract 对于应用于所有Feign客户端的全局默认配置，也可以通过两种方式 通过@EnableFeignClients 的defaultConfiguration 属性指定默认配置类 在配置文件中通过名称为default的配置实现1234567feign: client: config: default: connectTimeout: 5000 readTimeout: 5000 loggerLevel: basic 优先级同Ribbon， 配置文件&gt;自定义配置类&gt;默认的FeignClientsConfiguration 案例演示本文案例演示基于前面搭建的springcloud-eureka 与 springcloud-eureka-client 两个示例项目。 新建springcloud-feign项目，pom.xml中加入依赖 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; spring-cloud-starter-openfeign 包含了spring-cloud-starter-netflix-ribbon 与 spring-cloud-starter-loadbalancer。 启动类加上@EnableFeignClients 注解 12345678@SpringBootApplication@EnableFeignClientspublic class FeignApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(FeignApplication.class, args); &#125;&#125; 定义Feign client（feign client支持继承） 123456@FeignClient(\"hello-service\")public interface HelloClient extends BaseHelloClient&#123; @RequestMapping(\"hello/param\") String hello(@SpringQueryMap QueryParam param);&#125; 调用Feign client 12345678910111213141516@RestControllerpublic class FeignController &#123; @Autowired private HelloClient helloClient; @RequestMapping(\"feign\") public String feignTest()&#123; return \"调用Hello-service返回： \" + helloClient.hello(); &#125; @RequestMapping(\"feign/param\") public String feignTestParam(QueryParam param) &#123; return \"调用Hello-service返回： \" + helloClient.hello(param); &#125;&#125; 依次启动三个项目，调用http://localhost:8083/feign 能正常返回调用hello-service的结果。 本示例项目还通过@SrpingQueryMap 注解实现了Feign对 pojo用于GET请求参数的支持。如果不加@SrpingQueryMap， 则pojo参数是无法通过Feign client传递的，可去掉注解自行验证下。 一些知识点 如果需要定制化产生的查询参数map，可以实现并注入一个自定义的 QueryMapEncoder bean Feign client的日志可通过feign client接口的全路径名进行配置，如logging.level.project.user.UserClient: DEBUG，默认为NONE（即不打印日志）。全局设置 1234567@Configurationpublic class FooConfiguration &#123; @Bean Logger.Level feignLoggerLevel() &#123; return Logger.Level.FULL; &#125;&#125; 可设置的level值 NONE： 不记录日志 ，默认 BASIC：只记录请求方法，url，以及响应状态码与执行时间 HEADERS：包括BASIC与请求、响应头 FULL：包括请求与响应的headers，body，metadata Feign默认使用Ribbon来做负载均衡，可通过配置spring.cloud.loadbalancer.ribbon.enabled=false 来使用spring cloud loadbalancer（目前Ribbon处于维护状态，近期内不做更新） 可通过配置feign.okhttp.enabled=true 或 feign.httpclient.enabled=true 来使用OkHttpClient 或ApacheHttpClient， 默认使用的是JDK 原生的URLConnection 发送HTTP请求，没有连接池 如果需要在RequestInterceptor 中使用ThreadLocal中的变量， 那么要么禁用Hystrix，要么设置hystrix的线程隔离策略为SEMAPHORE 12345678910feign: hystrix: enabled: false# 或者hystrix: command: default: execution: isolation: strategy: SEMAPHORE 使用有Hystrix fallback的Feign时，会在ApplicationContext中存在多个同类型bean， 导致@Autowired 失效。为了解决这个问题，Spring cloud netflix 将所有feign实例标为@Primary，如果要关闭该特性， 可将@FeignClient的 primary属性置为false。 1234@FeignClient(name = \"hello\", primary = false)public interface HelloClient &#123; // ...&#125; 本文示例代码 认真生活，快乐分享 欢迎关注微信公众号：空山新雨的技术空间 ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/tags/SpringCloud/"},{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"}]},{"title":"Spring Cloud（二）：Web服务客户端之Ribbon","slug":"springcloud-2","date":"2020-01-14T05:04:40.000Z","updated":"2020-01-15T08:59:16.159Z","comments":true,"path":"springcloud-2.html","link":"","permalink":"http://blog.jboost.cn/springcloud-2.html","excerpt":"上文介绍了服务如何通过Eureka实现注册，以及如何从Eureka获取已经注册的服务列表。那么拿到注册服务列表后， 如何进行服务调用？一个简单的实现是可以从被调用服务的实例列表中选择一个服务实例，通过其hostname（或IP），端口，及API的路径拼接成完整的url，通过http client来完成调用。但生产环境中，为了高性能、高可用等要素，服务的调用一般涉及负载均衡、故障转移、失败重试等实现，因此引入实现这些功能的客户端组件也成为了微服务架构中的必备要素。Spring Cloud中可通过Ribbon与Feign来实现服务间的调用。 本系列文章与示例均基于最新的Spring Cloud Hoxton版编写。","text":"上文介绍了服务如何通过Eureka实现注册，以及如何从Eureka获取已经注册的服务列表。那么拿到注册服务列表后， 如何进行服务调用？一个简单的实现是可以从被调用服务的实例列表中选择一个服务实例，通过其hostname（或IP），端口，及API的路径拼接成完整的url，通过http client来完成调用。但生产环境中，为了高性能、高可用等要素，服务的调用一般涉及负载均衡、故障转移、失败重试等实现，因此引入实现这些功能的客户端组件也成为了微服务架构中的必备要素。Spring Cloud中可通过Ribbon与Feign来实现服务间的调用。 本系列文章与示例均基于最新的Spring Cloud Hoxton版编写。 RibbonRibbon是一个可实现负载均衡的Web客户端。我们一般理解的负载均衡是在服务端实现的，如Nginx（但这都是相对的，如果相对后端服务来说，也可以把Nginx当做一个实现了负载均衡的客户端）， 而Ribbon是客户端的负载均衡实现。 Ribbon的核心概念是命名的客户端（named client），Spring Cloud会为每个命名客户端创建一个子应用上下文（ApplicationContext），在该上下文中，通过RibbonClientConfiguration创建ILoadBalancer，RestClient，ServerListFilter等Bean。 Spring Cloud Netflix提供的默认的Ribbon bean及说明 Bean类型 默认实现类 说明 IClientConfig DefaultClientConfigImpl Ribbon客户端配置加载实现，加载各实现bean及客户端连接超时、通讯超时等配置 IRule ZoneAvoidanceRule 基于zone与可用性来过滤服务器的规则实现 IPing DummyPing 判断服务器是否存活的实现，默认总是返回true ServerList ConfigurationBasedServerList 获取服务器列表的实现，默认基于配置 ServerListFilter ZonePreferenceServerListFilter 服务器过滤实现，默认过滤出与客户端在同一个zone中的服务器列表 ILoadBalancer ZoneAwareLoadBalancer 负载均衡实现，默认根据zone的请求负载量排除掉负载最高的zone，从剩下的zone中选择一个根据给定的Rule选择其中一个服务器 ServerListUpdater PollingServerListUpdater 动态的服务器列表更新器 Spring Cloud允许我们通过声明一个configuration来对客户端进行自定义，来调整或覆盖上述默认实现，如 12345@Configuration@RibbonClient(name = \"custom\", configuration = CustomConfiguration.class)public class TestConfiguration &#123;&#125; 这样，客户端将由RibbonClientConfiguration 与 CustomConfiguration中定义的组件一起组成，且CustomConfiguration 中的组件会覆盖前者。 注意CustomConfiguration 必须是@Configuration 修饰的类，且不能被main application context的 @ComponentScan 扫描，否则会被所有@RibbonClients 共享 如果要为所有Ribbon Clients定制默认配置，则可使用@RibbonClients 注解 1234@RibbonClients(defaultConfiguration = DefaultRibbonConfig.class)public class RibbonClientDefaultConfigurationTestsConfig &#123;&#125; 也可以通过配置属性来定制Ribbon Client，支持的配置属性 12345&lt;clientName&gt;.ribbon.NFLoadBalancerClassName: ILoadBalancer接口实现类&lt;clientName&gt;.ribbon.NFLoadBalancerRuleClassName: IRule接口实现类&lt;clientName&gt;.ribbon.NFLoadBalancerPingClassName: IPing接口实现类&lt;clientName&gt;.ribbon.NIWSServerListClassName: ServerList接口实现类&lt;clientName&gt;.ribbon.NIWSServerListFilterClassName: ServerListFilter接口实现类 比如对于一个服务名称为users的配置 1234users: ribbon: NIWSServerListClassName: com.netflix.loadbalancer.ConfigurationBasedServerList NFLoadBalancerRuleClassName: com.netflix.loadbalancer.WeightedResponseTimeRule 配置属性的优先级 &gt; configuration指定配置类的优先级 &gt; 默认RibbonClientConfiguration的优先级， 即同样的实现，前者覆盖后者。 当Eureka与Ribbon同时存在时，ribbonServerList会被 DiscoveryEnabledNIWSServerList覆盖，从Eureka来获取server list，同时 NIWSDiscoveryPing也会替换IPing接口，代理Eureka来确定服务器是否处于运行状态。 Ribbon的超时与重试配置 &lt;clientName&gt;.ribbon.ConnectTimeout： 请求连接超时时间，默认2000 &lt;clientName&gt;.ribbon.ReadTimeout： 请求处理超时时间，默认5000 &lt;clientName&gt;.ribbon.MaxAutoRetries： 在同一台服务器上的重试次数，排除第一次调用，默认0 &lt;clientName&gt;.ribbon.MaxAutoRetriesNextServer： 切换服务器的重试次数，默认1 &lt;clientName&gt;.ribbon.OkToRetryOnAllOperations： 对所有请求都进行重试，默认false 当项目中添加了Spring Retry的依赖，则会启用重试机制。当请求失败时，会再尝试访问当前服务器（次数由MaxAutoRetries配置），如果不行，就换一个服务器进行访问，如果还是不行，再换服务器访问（更换次数由MaxAutoRetriesNextServer配置），如果还是不行，则返回请求失败。 Ribbon的负载均衡策略前文提到Ribbon的负载均衡默认实现为ZoneAwareLoadBalancer，那么Ribbon提供的负载均衡策略还有哪些？ 罗列如下 BestAvailableRule： 排除掉断路器打开的服务器，选取并发请求最小的服务器 AvailabilityFilteringRule： 过滤掉断路器打开或活跃连接数超过限制（通过&lt;clientName&gt;.&lt;nameSpace&gt;.ActiveConnectionsLimit配置，默认为Integer.MAX_VALUE）的服务器 WeightedResponseTimeRule: 根据平均响应时间来动态为服务器赋予权值，实现基于权重的轮询 RetryRule： 对选择负载均衡策略添加重试机制 RoundRobinRule： 简单轮询 RandomRule： 随机轮询 ZoneAvoidanceRule： 结合区域与可用性来选择服务器，也是默认实现 可通过如下配置修改Ribbon的负载均衡策略 123client-name: ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.WeightedResponseTimeRule 案例演示本文案例演示基于上文搭建的springcloud-eureka 与 springcloud-eureka-client 两个示例项目 （源码），依次启动两个项目，然后将springcloud-eureka-client项目的端口 server.port改为8081，新开一个springboot运行配置，如图 以8081端口再起一个springcloud-eureka-client的服务实例。这是查看Eureka页面 http://localhost:8761/, 可以看到hello-service服务注册了两个实例 新建springcloud-ribbon项目 （源码） pom.xml中引入依赖 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 编写测试接口， LoadBalanceClient 是Ribbon的API 12345678910111213@RestControllerpublic class RibbonTestController &#123; @Autowired private LoadBalancerClient loadBalancer; @GetMapping(\"ribbon\") public String testRibbon()&#123; ServiceInstance instance = loadBalancer.choose(\"hello-service\"); return String.format(\"http://%s:%s\", instance.getHost(), instance.getPort()); &#125;&#125; 启动springcloud-ribbon， 调用测试接口 http://localhost:8082/ribbon， 可以看到返回结果交替显示 http://CN-201911061714:8080， http://CN-201911061714:8081 （CN-201911061714是我电脑的hostname，你的可能不一样），可见Ribbon实现了客户端的负载均衡。 一些知识点 Ribbon如果对所有请求进行重试，则需要保证接口的幂等性（多次调用产生的结果是一致的） 每一个命名的Ribbon客户端都有一个相应的由Spring cloud维护的子应用上下文，默认是lazy load的（第一次请求客户端时才load），可以通过如下配置更改为启动立即加载 1234ribbon: eager-load: enabled: true clients: client1, client2, client3 client.ribbon.* 针对单个客户端进行配置，针对所有客户端默认配置，则使用ribbon.* 当结合断路器使用时， 断路器的超时时间要大于Ribbon的超时时间，不然不会触发重试（还没重试就触发断路器打开了） 除了Ribbon，能做负载均衡访问的Web客户端还有@LoadBalance 注解的RestTemplate， 与Feign 本文示例代码 认真生活，快乐分享 欢迎关注微信公众号：空山新雨的技术空间 ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/tags/SpringCloud/"},{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"}]},{"title":"微服务漫谈","slug":"micro-service","date":"2019-12-24T06:52:04.000Z","updated":"2019-12-26T07:38:58.727Z","comments":true,"path":"micro-service.html","link":"","permalink":"http://blog.jboost.cn/micro-service.html","excerpt":"微服务可以说是近几年技术圈异常火爆的概念，人人都在说微服务，人人都在致力于打造自己的“微服务”。甚至于某些压根不懂技术的项目招标方都在问你们公司用了微服务吗？“微服务”俨然成了衡量团队技术实力或技术逼格的代名词。 但是，微服务真是万能的吗？是不是来个项目就得微服务一下，不然就显得落伍，显得low了呢？ 本文一起聊聊“微服务”的那些事。","text":"微服务可以说是近几年技术圈异常火爆的概念，人人都在说微服务，人人都在致力于打造自己的“微服务”。甚至于某些压根不懂技术的项目招标方都在问你们公司用了微服务吗？“微服务”俨然成了衡量团队技术实力或技术逼格的代名词。 但是，微服务真是万能的吗？是不是来个项目就得微服务一下，不然就显得落伍，显得low了呢？ 本文一起聊聊“微服务”的那些事。 一. 什么是微服务？微服务是一种架构风格，由Martin Fowler（牛人，ThoughtWorks公司的首席科学家，同时也是敏捷开发方法的创始人之一）提出，是指将复杂应用通过拆分为一系列高内聚、低耦合、自治的小（微）服务，每个服务独立开发（可以使用不同的编程语言），独立部署（运行在不同的进程中），并通过轻量级的通信机制（Restful API）进行交互。微服务本质上还是SOA（Service-Oriented-Architecture, 面向服务的架构），但微服务不限定于特定的技术，通过Restful架构风格来完成系统的统一，因此比传统的SOA（一般基于较重的SOAP、WSDL、UDDI等协议技术，通过企业服务总线ESB进行连接集成）更为灵活，更具扩展性。 微服务的特征： 是一种应用于组件设计(服务如何拆分)和部署架构(服务如何部署和通信)的模式 适用于创建具有“一定功能复杂性”的分布式应用系统 各个服务必须小，只负责某个具体的业务功能，比如商品服务，订单服务，根据功能实现关注点分离 各个服务保持自治和相互解耦，进行独立开发、独立部署，及独立升级与伸缩 各个服务之间通过轻量级 API （Restful API）和异步通信（如消息队列）相结合的方式进行通信 二. 微服务的优缺点微服务的优缺点一般相对单体应用（就是所有功能、代码都整在一个工程项目中）而言 1. 微服务的优点：1.1 简化复杂的业务模型微服务将复杂的业务通过一系列的高内聚、低耦合的小型服务来实现，体现了分治的思想。每个服务的开发与维护都非常高效，可管理性更高，能快速响应需求。 1.2 不局限于某项特定的技术因为服务间是通过轻量级的Restful API交互，每一个服务可以独立开发，可选用不同的编程语言与技术框架（虽然实际中对于一般规模一般都是统一的）。 1.3 独立部署与升级，按需伸缩每个服务都是独立部署，运行在不同的进程中，因为加载内容相对较少，所以一般启动也比较快。单个服务的升级对系统整体的影响也较小。同时可以针对各个服务的负载情况，进行独立的按需伸缩。 2. 微服务的缺点：2.1 对“微”的粒度与服务的边界难以把握。微服务开发过程中，开发人员最常见的疑问就是这个接口应该放到哪个服务里。服务应该微到什么程度，服务边界与服务交互如何定义与规范，需要有对业务、技术充分了解的专业人员做上层设计（一般就是架构师），并且持续跟进实施落地，否则很有可能就会导致只是将一个单体应用拆成了多个单体应用，或编织了一张交互错综复杂的服务网络的尴尬局面。 2.2 引入了分布式的复杂性。微服务中某一个请求可能就涉及好几个服务间的调用，如果出现问题，则定位相对困难复杂；同时基于CAP（Consistency，Availability， Partition Tolerance）理论，分布式系统中一致性、可用性、分区容忍性只能同时满足两个，一般在满足可用性与分区容忍性的基础上，对系统提供最终一致性保障。 2.3 对技术栈的要求更高。团队需要对微服务基本理论与相关技术有一定了解，且需要搭建许多业务服务之外的基础设施服务，比如服务注册发现、配置管理、链路监控等。目前微服务技术最热的就是Spring Cloud，也有部分团队选择Dubbo。 2.4 对运维的要求更高。微服务将一个复杂应用拆分为几十个甚至上百个小型服务，迭代升级部署的频率比单体应用更高，采用传统的运维手段很难满足需求，一般需引进DevOps的相关技术手段，如CI（持续集成）、CD（持续部署）、自动化测试、容器化与服务编排，及丰富的监控告警机制等。 三. 如何抉择？如之前文章说到的，技术人员的能力在于解决问题的能力（当然解决问题也分临时性的解决问题与前瞻性的解决问题——解决方案能在较长一段时间内适用）。技术管理者或技术决策者最基本的修为就是在过火过热的各种技术概念与技术框架面前保持冷静，选择最适合业务场景与自身团队的技术方案。 要不要用微服务，什么时候不该用微服务？结合自身理解，总结整理如下： 业务简单，应用规模很小不该用微服务杀鸡不能用牛刀，微服务旨在将复杂业务拆分，简化业务规模， 如果业务本身很简单，一个单体应用就能处理的场景不该用微服务。 业务领域不够清晰、明确不该用微服务业务领域不清晰、不明确，意味着整个业务定义、业务框架都可能朝令夕改，如果采用微服务，则可能导致牵一发而动全身的痛苦局面。 团队技术储备不够不该硬上微服务微服务的分布式特性对团队的技术要求比单体应用高， 如果团队大部分成员之前都没接触过微服务，对微服务缺乏基本的了解，不该硬上微服务。 小型创业公司不适合用微服务该条其实是前面几条的汇总，因为小型创业公司一般就意味着业务相对简单，并且业务领域、设计不够清晰、明确，以及团队技术实力相对较弱，并且人员流动性大等特点，任何一点都不利于微服务的构建。 对于业务较为明确且复杂的系统，如果你团队的技术储备达到一定水平（如对Spring Boot，Spring Cloud，CI/CD，Docker/K8s等有一定掌握），并且有一个对业务与技术都有充分了解且具备决策权的master，微服务无疑是一个很好的选择。否则，慎重！ 四. 总结任何技术与框架都有其适用场景，微服务不是万能钥匙。应结合具体的业务场景，团队组成，技术储备等因素综合考虑，选择最适合自身的技术方案。 —————————————————————————————作者：空山新雨欢迎关注我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号）","categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://blog.jboost.cn/categories/Architecture/"}],"tags":[{"name":"arch","slug":"arch","permalink":"http://blog.jboost.cn/tags/arch/"}]},{"title":"Spring Cloud（一）：服务注册中心Eureka","slug":"springcloud-1","date":"2019-12-24T05:56:55.000Z","updated":"2020-01-13T11:34:24.509Z","comments":true,"path":"springcloud-1.html","link":"","permalink":"http://blog.jboost.cn/springcloud-1.html","excerpt":"Spring Cloud 基于 Netflix 的几个开源项目进行了封装，提供包括服务注册与发现（Eureka），智能路由（Zuul），熔断器（Hystrix），客户端负载均衡（Ribbon）等在内的核心组件。 在微服务系统中，服务少则十几、几十个，多则上百、几百个（据悉 Netflix 的云平台上运行了500多个微服务），这些微服务通过相互调用来为用户提供功能。那么一个服务调用另一个服务是如何进行的，如何定位到另一个服务的地址？代码中写死，还是配置文件中配置？显然对于服务数量较多的系统，这两种方式先不说后续维护，光写起来就很痛苦。于是，对于微服务架构来说，服务的自动注册与发现就成为非常核心的功能，Eureka就是来负责实现这个功能的。 本系列文章与示例均基于最新的Spring Cloud Hoxton版编写。","text":"Spring Cloud 基于 Netflix 的几个开源项目进行了封装，提供包括服务注册与发现（Eureka），智能路由（Zuul），熔断器（Hystrix），客户端负载均衡（Ribbon）等在内的核心组件。 在微服务系统中，服务少则十几、几十个，多则上百、几百个（据悉 Netflix 的云平台上运行了500多个微服务），这些微服务通过相互调用来为用户提供功能。那么一个服务调用另一个服务是如何进行的，如何定位到另一个服务的地址？代码中写死，还是配置文件中配置？显然对于服务数量较多的系统，这两种方式先不说后续维护，光写起来就很痛苦。于是，对于微服务架构来说，服务的自动注册与发现就成为非常核心的功能，Eureka就是来负责实现这个功能的。 本系列文章与示例均基于最新的Spring Cloud Hoxton版编写。 Eureka Eureka是一个基于REST的服务，包括Eureka Server与Eureka Client两个端。Eureka Server作为服务注册中心接受Eureka Client的注册及获取其它服务的地址信息。基本架构如下图所示： 其中 Eureka Server： 作为服务注册中心，提供服务注册与发现功能接口 Service Provider： 服务提供者，将自身服务注册到服务注册中心，供其它服务消费者发现与调用 Service Consumer： 服务消费者，从服务注册中心发现服务，并通过一些负载均衡客户端来调用（比如Ribbon或Feign） 很多时候同一个应用可能既是服务提供者，也是服务消费者——自己作为服务方，为其它服务提供接口，同时也调用其它服务的接口来完成自身的业务逻辑。 Eureka Server Eureka Server的搭建非常简单，其部署可分为单实例部署与多实例集群部署，一般开发测试环境可以使用单实例部署，但生产环境出于高可用要求，可进行多实例集群部署。 在pom.xml中添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; 为了方便版本引入，可以在pom中添加依赖管理，这样spring cloud相关的starter依赖就不需要指定版本了（如上省略了version） 1234567891011&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 在启动类上添加注解 @EnableEurekaServer 12345678@SpringBootApplication@EnableEurekaServerpublic class EurekaApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaApplication.class, args); &#125;&#125; 在application.yml 或 application.properties配置文件中添加配置（个人比较倾向于yml，两者区别可自行百度） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354spring: application: name: spring-cloud-eureka profiles: active: singleserver: port: 8761---spring: profiles: singleeureka: instance: hostname: localhost client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ ---spring: profiles: peer1server: port: 8761eureka: instance: hostname: peer1 client: serviceUrl: defaultZone: http://peer2:8762/eureka/,http://peer3:8763/eureka/---spring: profiles: peer2server: port: 8762eureka: instance: hostname: peer2 client: serviceUrl: defaultZone: http://peer1:8761/eureka/,http://peer3:8763/eureka/---spring: profiles: peer3server: port: 8763eureka: instance: hostname: peer3 client: serviceUrl: defaultZone: http://peer1:8761/eureka/,http://peer2:8762/eureka/ 在该配置文件中，实际上是定义了两种模式，其中默认的profile single是单实例模式， peer1， peer2， peer3组成多实例模式。 eureka.client.registerWithEureka：表示是否将自身注册到Eureka Server，默认为true，单实例模式下一般设置为false，否则会在启动时报连接不到服务器的错误 eureka.client.fetchRegistry：表示是否从Eureka Server获取注册服务列表，默认为true，同样在单实例模式下设置为false eureka.client.serviceUrl.defaultZone：Eureka Server的地址，多实例模式下多个地址以“,”隔开，多个实例之间只要有一条路线连通，则总会将注册信息进行同步 启动 对于单实例模式，如果按如上配置，则直接启动程序即可。启动完成后，访问 http://localhost:8761，即可查看Eureka Server的相关信息，如 上图所示，当前没有Eureka Server的副本也没有任何服务注册。 对于多实例集群模式，则需要根据不同的profile启动多个实例， 12345mvn clean packagecd targetjava -jar springcloud-eureka-1.0-SNAPSHOT.jar --spring.profiles.active=peer1java -jar springcloud-eureka-1.0-SNAPSHOT.jar --spring.profiles.active=peer2java -jar springcloud-eureka-1.0-SNAPSHOT.jar --spring.profiles.active=peer3 启动完成后，打开 http://localhost:8761， 可以看到Eureka Server已经存在副本与注册的服务了（Eureka将自身作为一个服务完成了注册） 上述操作如果是在单机进行，则需要在hosts文件中添加映射，linux下是/etc/hosts，windows10 下是C:\\Windows\\System32\\drivers\\etc\\hosts， 123127.0.0.1 peer1127.0.0.1 peer2127.0.0.1 peer3 Eureka Client Eureka Client一般集成在各个微服务中，集成也非常简单。 pom.xml中添加依赖 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; application.yml配置文件中添加配置 1234567eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ instance: prefer-ip-address: true instance-id: $&#123;spring.application.name&#125;:$&#123;random.uuid&#125; 如果是多实例集群模式，则 eureka.client.serviceUrl.defaultZone 可以配置多个地址，“，”号分隔。 eureka.client.*： 发现服务的配置参数 eureka.instance.*： 注册服务的配置参数， 如上 eureka.instance.prefer-ip-address 设置为true表示服务注册时使用IP，而不是hostname； eureka.instance.instance-id 配置服务实例的ID，默认为 ${spring.cloud.client.hostname}:${spring.application.name}:${spring.application.instance_id:${server.port}}} 添加了依赖就能集成Eureka Client，主类上添加 @EnableDiscoveryClient 注解不是必须。 启动程序后，进入Eureka Server页面即可看到注册的服务 一些知识点（建议掌握） Eureka Client在注册服务时，提供包括hostname，IP地址， port， health indicator url，status page， home page 等在内的meta-data，其它客户端可通过这些信息来直接与服务进行交互，我们也可以通过 eureka.instance.metadataMap 来添加自定义的meta-data，供客户端访问 Eureka Server通过接收Eureka Client的心跳消息来判断服务实例是否存活，如果某一个实例的心跳在特定时间（可配置）内没收到，则将其从注册表中移除。心跳默认间隔为30s，一个服务被其它客户端发现，可能需要经过3次心跳，这也是有时候服务注册比较慢的原因。可通过eureka.instance.leaseRenewalIntervalInSeconds配置，但生产环境建议最好保持默认 Eureka Client默认不会传播当前应用的健康检查状态，一旦注册成功，只要心跳存在，Eureka总是认为应用处于UP状态。可以启用Eureka的健康检查，将状态传播给Eureka，其它应用只会将请求发给UP状态的服务实例 eureka.client.healthcheck.enabled=true。注意这个配置只能配置在application.yml中，配置在bootstrap.yml中可能导致注册服务时，服务以状态为UNKOWN进行注册 Eureka Server是没有后端存储的，服务实例需要通过心跳来更新注册信息，注册信息存于内存中，Eureka Client也有一个基于内存的缓存，不需要每次请求服务都要访问注册中心获取服务地址信息 Eureka的自我保护机制：Eureka Server在短时间内丢失比较多的客户端时，会进入自我保护模式，在该模式下，Eureka Server即使发现服务实例已经不再发送心跳了，也不会从服务注册表中删除。这样，当发生网络故障时，服务注册信息仍然存于Eureka中，当网络故障恢复后，会自动退出自我保护模式。自我保护模式是一种应对网络异常的安全保护机制。相关配置： eureka.server.renewal-percent-threshold， 触发自我保护机制的阈值，默认为0.85； eureka.server.enable-self-preservation， 自我保护开启，默认为true，如果设置为false，则关闭客户端程序后，可直观地从Eureka Server的页面发现服务实例被注销删除了。 本文示例代码","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/tags/SpringCloud/"},{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"}]},{"title":"Docker笔记（十二）：Docker Compose入门","slug":"docker-12","date":"2019-11-20T11:09:23.000Z","updated":"2020-08-26T01:31:56.874Z","comments":true,"path":"docker-12.html","link":"","permalink":"http://blog.jboost.cn/docker-12.html","excerpt":"1. Compose简介Docker Compose是Docker官方的用于对Docker容器集群实现编排，快速部署分布式应用的开源项目。Docker Compose通过docker-compose.yml文件来定义一组相关联的应用容器的编排，这组相关联的应用容器一般通过互相交互作为一个整体项目提供服务，比如一个Web项目，既包含业务服务容器，也包含数据库服务容器与缓存服务容器等。","text":"1. Compose简介Docker Compose是Docker官方的用于对Docker容器集群实现编排，快速部署分布式应用的开源项目。Docker Compose通过docker-compose.yml文件来定义一组相关联的应用容器的编排，这组相关联的应用容器一般通过互相交互作为一个整体项目提供服务，比如一个Web项目，既包含业务服务容器，也包含数据库服务容器与缓存服务容器等。 Compose中两个重要的概念： 服务（service）： 包含多个运行相同镜像的容器实例 项目（project）： 由一组关联的应用容器（服务）组成一个完整的业务服务单元，在docker-compose.yml（即Compose的模板文件）中定义 Copmpose项目由Python编写，通过调用Docker服务提供的API来对容器进行管理。Compose默认的管理对象是项目，可以通过子命令对项目中的一组容器进行生命周期管理。 2. Compose安装在macOS与Win10下，Docker安装自带了docker-compose的二进制文件，可以直接使用。Linux下， 123[root@iZwz ~]# curl -L \"https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose[root@iZwz ~]# chmod +x /usr/local/bin/docker-compose 验证 12345[root@iZwz ~]# docker-compose versiondocker-compose version 1.24.1, build 4667896bdocker-py version: 3.7.3CPython version: 3.6.8OpenSSL version: OpenSSL 1.1.0j 20 Nov 2018 3. Compose模板文件模板文件是使用Compose的核心，定义了一组相关联的应用容器，使之构成一个项目，里面大部分指令跟docker run相关参数的含义是类似的。默认的模板文件名称为docker-compose.yml，为YAML格式，如 12345678910111213version: '3'services: web: build: . depends_on: - db - redis redis: image: redis db: image: mysql Compose模板文件可以动态读取主机的系统环境变量与当前目录下.env文件中的变量，通过${xx}引用。 模板文件中的常用指令说明 build指定Dockerfile所在文件夹的路径，可以是绝对路径或相对模板文件的路径。Compose将会自动构建镜像，然后使用该镜像。也可以通过如下方式详细指定。cache_from指定构建镜像的缓存 12345678build: context: ./dir dockerfile: Dockerfile-alternate args: buildno: 1 cache_from: - alpine:latest - corp/web_app:3.14 command覆盖容器启动后默认执行的命令。 container_nameCompose默认会使用 项目名称_服务名称_序号的格式作为容器名称。一般不需要特别指定，因为指定具体名称后，服务将无法进行扩展（scale），因为不允许多个容器具有相同的名称。 depends_on解决容器的依赖、启动先后顺序的问题，但是服务不会等待依赖的服务“完全启动”之后才启动。 env_file指定环境变量定义文件，可以为单独文件路径或列表，当与environment中有同名冲突时，以environment为准。 environment设置环境变量，支持数组或字典两种格式。只有名称的变量会自动获取运行Compose主机上的对应变量的值，以防止信息泄露 1234567environment: RACK_ENV: development SESSION_SECRET:environment: - RACK_ENV=development - SESSION_SECRET expose暴露端口，不映射到宿主机，只被连接的服务访问 healthcheck通过命令检查容器是否健康运行，如 12345healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost\"] interval: 1m30s timeout: 10s retries: 3 image指定镜像名称或镜像ID，所有服务都必要要么通过build，要么通过image来指定镜像。 labels为容器添加Docker元数据信息 network_mode设置网络模式，与docker run的–network一样，如bridge，host，none等，也可以是如下形式 12network_mode: \"service:[service name]\"network_mode: \"container:[container name/id]\" networks配置容器连接的网络，如 12345678services: service1: networks: - some-network - other-networknetworks: some-network: other-network: ports暴露端口信息，遵循端口映射规则。 secrets存储敏感数据，如密码等信息 12345678910111213mysql: image: mysql environment: MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password secrets: - db_root_password - my_other_secretsecrets: my_secret: file: ./my_secret.txt my_other_secret: external: true volumes容器的数据卷挂在路径设置，可以设置多个，与docker -v类似，如 1234volumes: - /var/lib/mysql - cache/:/tmp/cache - ~/configs:/etc/configs/:ro 4. Compose命令Compose命令默认是针对项目本身，也可以指定为项目中的服务或容器。docker-compose 命令的基本使用格式为 1docker-compose [-f=&lt;arg&gt;...] [options] [COMMAND] [ARGS...] 命令选项 -f, –file 指定模板文件，默认为docker-compose.yml，可多次指定 -p, –project-name 指定项目名称，默认为所在目录名称 –x-networking 使用Docker的可插拔网络特性 –x-networking-driver 指定网络驱动，默认为bridge –verbose 输出更多调试信息 -v, –version 打印版本信息 命令使用说明 build 格式为docker-compose build [options] [SERVICE...]，构建项目中的服务容器，选项包括 –force-rm（删除构建过程中的临时容器），–no-cache（构建镜像过程不使用cache），–pull（始终尝试通过pull来获取最新版本镜像） config 验证模板文件格式是否正确 down 停止up命令启动的容器，并移除网络 exec 进入指定的容器 images 列出compose文件中包含的镜像 kill 格式为docker-compose kill [options] [SERVICE...]，强制停止服务容器 logs 格式为docker-compose logs [options] [SERVICE...]，查看服务容器的输出 pause 格式为docker-compose pause [SERVICE...]， 暂停一个服务容器 port 格式为docker-compose port [options] SERVICE PRIVATE_PORT，打印容器端口所映射的公共端口，–index=index（指定容器序号，默认为1） ps 格式为docker-compose ps [options] [SERVICE...]，列出项目中目前的所有容器 pull 格式为docker-compose pull [options] [SERVICE...]，拉去服务依赖的镜像 push 推送服务依赖的镜像到Docker镜像仓库 restart 重启项目中服务，格式为docker-compose restart [options] [SERVICE...] rm 删除所有停止的服务容器，格式docker-compose rm [options] [SERVICE...]， -f（强制直接删除） run 在指定服务上执行一个命令，不会自动创建端口，以避免冲突 scale 格式docker-compose scale [options] [SERVICE=NUM...]，设置指定服务运行的容器个数，少则新建，多则删除 start 格式docker-compose start [SERVICE...]，启动已经存在的服务容器 stop 停止运行中的容器 top 查看各个服务容器内运行的进程 unpause 格式docker-compose unpause [SERVICE...]，恢复处于暂停状态的服务 up 格式docker-compose up [options] [SERVICE...]，尝试自动完成包括构建镜像，创建服务，启动服务，关联服务相关容器的一系列操作，大部分时候都可以通过该命令来启动一个项目，-d（在后台启动所有容器）。docker-compose up --no-recreate只启动处于停止状态的容器，忽略已经运行的服务，docker-compose up --no-deps -d &lt;SERVICE_NAME&gt;重新创建服务，但不影响到它所依赖的服务 5. 总结Compose是Docker官方的服务容器编排工具，对一些简单的但包含多个组件的服务可以借助Compose来快速搭建环境，如开源的错误监控系统sentry，包括sentry服务本身，redis，postgres。对于业务生产环境，则一般使用功能更为丰富的第三方编排系统如Kubernetes来部署。 欢迎关注我的微信公众号：jboost-ksxy ——————————————————————————————————————————————————————————————— ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.jboost.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.jboost.cn/tags/docker/"}]},{"title":"软件系统的非功能要素与设计思路","slug":"arch-1","date":"2019-11-15T10:56:54.000Z","updated":"2020-07-22T06:33:45.406Z","comments":true,"path":"arch-1.html","link":"","permalink":"http://blog.jboost.cn/arch-1.html","excerpt":"对于具备一定复杂度的软件系统，我们一般都会进行架构设计。架构设计中涉及功能要素与非功能要素，功能要素对应业务需求，关注需要实现的业务模块与功能，非功能要素对应系统本身的运行需求，一般包括性能、可用性、可伸缩性、可扩展性、安全等几个方面，软件系统的非功能架构设计，就是通过一些技术手段来满足这几个方面的运行需求。","text":"对于具备一定复杂度的软件系统，我们一般都会进行架构设计。架构设计中涉及功能要素与非功能要素，功能要素对应业务需求，关注需要实现的业务模块与功能，非功能要素对应系统本身的运行需求，一般包括性能、可用性、可伸缩性、可扩展性、安全等几个方面，软件系统的非功能架构设计，就是通过一些技术手段来满足这几个方面的运行需求。 一. 性能性能直观表现就是用户使用系统时响应的快慢程度。一般有响应时间（如用户点击一个按钮经服务端处理后，收到反馈的时长）、吞吐量（系统单位时间内能处理事务的个数，TPS —— Transaction-Per-Second）、支持并发数（能支持同时处理多少个并发在线用户）等衡量指标。 系统性能可通过相应的测试进行评估，一般包括： 性能测试：验证系统在资源可接受范围内，是否能达到性能预期。比如2核8G的服务器配置，在CPU负载不超过指定值的情况下，系统的吞吐量能否达到1k。 负载测试：不断给系统增加并发请求以增加对系统的压力，直到系统的某项或多项性能指标达到安全临界值。这时候，继续增加压力，系统的处理能力如吞吐量不增反降。 压力测试：在超过安全负载的情况下，继续对系统施加压力，直到系统崩溃或不能再处理任何请求，即系统在达到崩溃临界点时最大能承受多大的压力。 稳定测试：在模拟生产环境的场景下，包括软硬件配置、网络环境等条件，加载一定的业务压力（业务压力也尽量模拟生产环境下的情况），运行一段比较长的时间，看系统是否能稳定地运行。 测试报告形如下表 |并发数|响应时间（ms）|TPS|错误率（%）|CPU负载|内存使用（GB）:—:|:—:|:—:|:—:|:—:|:—:|:—:|性能测试|10|500|20|0|5|8性能测试|30|1000|40|2|15|14负载测试|40|1200|45|20|30|16压力测试|60|2000|30|40|50|16压力测试|80|超时|0|100|-|- 系统高性能的设计思路： 客户端优化，包括浏览器缓存（App本地缓存）、静态资源压缩、减少Cookie传输，减少HTTP请求（合并接口）等。 缓存，包括CDN缓存与服务端缓存。CDN将静态内容分发至离用户最近的网络服务商机房，通过反向代理服务器，缓存热点资源，从而加快用户请求的响应速度，减轻后端服务的负载压力；服务端缓存通过本地缓存与分布式缓存，缓存热点数据，从而加快数据请求过程，减轻数据库的负载压力。 异步，对不需要立即获取结果的操作异步化，减少用户响应时间，改善系统的可扩展性与性能。异步一般通过消息队列实现。 集群，将同一个服务使用多个实例通过负载均衡来提高服务的整体处理能力。集群需要服务实现无状态化，即对每一个请求的处理在服务本地不保留任何数据与状态。 代码优化，多线程，资源服用（数据库连接池、线程池、HTTP连接池等），减少HTTP及数据库访问次数（如避免在循环中调用数据库访问，可优化成一次获取数据到本地再处理）。 数据库访问，索引的使用，读写分离，分库分表，NoSQL的引入，存储结构优化等。 二. 可用性系统的高可用就是当系统的某个服务器宕机时，系统服务或系统的核心服务依然可用。 对于互联网服务，一般要求7*24小时提供不间断的服务能力。系统的可用性一般就通过服务可用时间比来衡量，如三个9的可用性，就是在一段考核时间内，99.9%的时间服务可用。 系统高可用的设计主要通过冗余与失效转移的手段来实现： 冗余，在系统的每一层，都通过部署多台服务器以负载均衡的形式提供访问（集群的形式），避免单点问题。关系型数据库无法通过集群部署，可提供多台互相备份，在主服务挂掉时，从服务能快速切换。 失效转移，在集群中其中一台服务器出现故障时，负载均衡能实时监测到并且不再往这台服务器分发请求，已失败的请求能重新调度到其它可用服务器。 提高系统高可用也需要在开发测试阶段尽可能地进行质量保证，通过代码review，多维度测试，预发布验证，灰度发布等手段，来减少生产环境的bug引入率，提高系统的可用性。同时，在各环节添加必要的监控与告警，包括服务器资源、网络、应用等多个维度，当问题发生时能及时获得告警通知。即一方面通过多种途径规避问题的发生，另一方面当问题真正发生时，能快速响应尽可能减少影响。 三. 可伸缩性可伸缩性是从提升系统服务能力的角度衡量的一个因素，如果能通过不断向集群中加入服务器就能提高系统的处理能力，来应对不断增长的用户并发访问，则系统是具有可伸缩性的。 系统可伸缩性的设计思路： 应用服务无状态化，对任何一个请求集群中任何一台服务器处理都能做到无差异化。 缓存服务器的伸缩可能导致缓存路由失效，可通过一致性Hash算法来降低缓存路由失效的比率。 关系型数据库很难通过集群实现可伸缩性，需要在数据库之外实现，如分库分表（不到万不得已不要使用分表）。 NoSQL，本身就具备良好的伸缩性，如HDFS。 四. 可扩展性系统的可扩展性关注功能性需求，衡量系统能否快速响应需求变化，即增加一个功能基本不需要修改现有系统或调整很少。 系统可扩展性的设计思路： 解耦，事件驱动架构，生产者、消费者模式，如基于消息队列 拆分，将复杂业务拆分成简单的职责单一的，高内聚、低耦合的服务单元，新增服务对现有服务影响不大 复用，将比较固定的不常变动的服务下沉作为基础服务，新的业务功能基于基础服务的复用实现 开放服务，将平台服务能力通过开放接口的形式提供给第三方，拓展平台业务服务能力 五. 安全攻击无处不在，衡量系统安全性的标准是系统针对现有的与潜在的各种攻击与窃密手段，是否有相应的可靠的应对策略。 系统的安全保障设计思路： 信息加密，包括单项散列加密（如密码加密，MD5，SHA）、对称加密、非对称加密（公钥私钥，https传输） 信息过滤，如敏感词过滤，黑名单机制等 风险控制，通过规则引擎控制访问，或基于统计模型进行监控告警 限流，限制单位时间内的访问量，如手机验证码 安全是相对的，没有绝对安全的系统，只能通过一些保障手段使攻击成本大于其获利成本来保障系统免受攻击。 六. 总结本文主要参考《大型网站技术架构》，对软件系统的几个核心的非功能性要素及其设计思路进行了介绍与总结，为软件系统的设计提供参考。","categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://blog.jboost.cn/categories/Architecture/"}],"tags":[{"name":"arch","slug":"arch","permalink":"http://blog.jboost.cn/tags/arch/"}]},{"title":"Spring Boot（十二）：LocalDateTime格式化处理","slug":"springboot-localdatetime","date":"2019-11-01T09:37:24.000Z","updated":"2019-11-01T10:08:21.753Z","comments":true,"path":"springboot-localdatetime.html","link":"","permalink":"http://blog.jboost.cn/springboot-localdatetime.html","excerpt":"Java 8之后，日期类的处理建议使用java.time包中对应的LocalDateTime, LocalDate, LocalTime类。（参考Java8新特性）","text":"Java 8之后，日期类的处理建议使用java.time包中对应的LocalDateTime, LocalDate, LocalTime类。（参考Java8新特性） 在Spring Boot中（验证版本：2.1.5.RELEASE），日期类的序列化格式可能不是自己所希望的，需要定义为自己的格式。有两种方式实现。 1. 注解方式分别使用 @JsonFormat， @DateTimeFormat 来定义序列化（bean转json）与反序列（json转bean）时的格式，如 123@JsonFormat(pattern = \"yyyy-MM-dd HH:mm:ss\")@DateTimeFormat(pattern = \"yyyy-MM-dd HH:mm:ss\")private LocalDateTime dateTime; 2. 统一配置方式定义一个配置类，对ObjectMapper对象进行定制，指定日期类对应的序列化与反序列化处理对象，如 123456789101112131415161718192021@Configurationpublic class LocalDateTimeFormatConfig &#123; private static final String DEFAULT_DATE_TIME_PATTERN = \"yyyy-MM-dd HH:mm:ss\"; private static final String DEFAULT_DATE_PATTERN = \"yyyy-MM-dd\"; private static final String DEFAULT_TIME_PATTERN = \"HH:mm:ss\"; @Bean @Primary public ObjectMapper objectMapper()&#123; ObjectMapper objectMapper = new ObjectMapper(); JavaTimeModule javaTimeModule = new JavaTimeModule(); javaTimeModule.addSerializer(LocalDateTime.class, new LocalDateTimeSerializer(DateTimeFormatter.ofPattern(DEFAULT_DATE_TIME_PATTERN))); javaTimeModule.addSerializer(LocalDate.class, new LocalDateSerializer(DateTimeFormatter.ofPattern(DEFAULT_DATE_PATTERN))); javaTimeModule.addSerializer(LocalTime.class, new LocalTimeSerializer(DateTimeFormatter.ofPattern(DEFAULT_TIME_PATTERN))); javaTimeModule.addDeserializer(LocalDateTime.class, new LocalDateTimeDeserializer(DateTimeFormatter.ofPattern(DEFAULT_DATE_TIME_PATTERN))); javaTimeModule.addDeserializer(LocalDate.class, new LocalDateDeserializer(DateTimeFormatter.ofPattern(DEFAULT_DATE_PATTERN))); javaTimeModule.addDeserializer(LocalTime.class, new LocalTimeDeserializer(DateTimeFormatter.ofPattern(DEFAULT_TIME_PATTERN))); objectMapper.registerModule(javaTimeModule); return objectMapper; &#125;&#125; 3. 总结注解的方式需要在每个属性上进行标注，如果日期类属性较多则较为繁琐，自定义配置类方式可以对日期进行统一的格式化处理。两者都存在的情况下，以注解为准，即注解方式会覆盖统一配置方式。 ——————————————————————————————————————————————— 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.jboost.cn/categories/SpringBoot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"}]},{"title":"nginx（一）：基本用途与用法","slug":"nginx-1","date":"2019-10-07T05:34:02.000Z","updated":"2019-11-01T07:45:53.758Z","comments":true,"path":"nginx-1.html","link":"","permalink":"http://blog.jboost.cn/nginx-1.html","excerpt":"最近由于事情较多，加上个人的懈怠，有一段时间没更新了。习惯的养成很难，但一旦养成，从中的受益也常会超乎意料，还是得坚持。接下来准备对一些开发维护过程中常用的工具软件与服务进行整理，如本系列的nginx，后续的redis，消息队列，jenkins等，欢迎关注。 nginx是一个轻量级的高性能的HTTP服务器，在Web应用部署中很常见。也正因为很常见，所以掌握其基本原理与用法显得很有必要，本系列文章对nginx的相关内容进行梳理，以供初学者参考、熟悉者回顾。","text":"最近由于事情较多，加上个人的懈怠，有一段时间没更新了。习惯的养成很难，但一旦养成，从中的受益也常会超乎意料，还是得坚持。接下来准备对一些开发维护过程中常用的工具软件与服务进行整理，如本系列的nginx，后续的redis，消息队列，jenkins等，欢迎关注。 nginx是一个轻量级的高性能的HTTP服务器，在Web应用部署中很常见。也正因为很常见，所以掌握其基本原理与用法显得很有必要，本系列文章对nginx的相关内容进行梳理，以供初学者参考、熟悉者回顾。 1. 简介在nginx以前，比较流行的HTTP服务器应属Apache（LAMP中A就是指Apache）。但根据netcraft的调查显示，近两年nginx已经超越Apache，成为市场占有率第一的HTTP服务器。如下图 nginx能战胜Apache有几个主要原因，一是其足够轻量，不管是安装与维护，还是资源的占用都非常简单与轻量；二是其高性能，nginx基于事件驱动机制，具备非常好的性能，据称能支持高达50000个并发连接数；三是其具有很高的稳定性，相对其它HTTP服务器在访问负载很高时会导致内存耗尽进而可能失去响应，nginx采用分阶段资源分配技术，CPU与内存占有率都很低，在高并发场景下，稳定性更高。 在日常使用中，nginx主要在三个方面为我们提供服务： 作为静态服务器提供静态资源的访问，如html网站，文件等 为后端服务提供反向代理 为反向代理的后端服务集群提供负载均衡 2. 静态服务器静态服务器一般就是提供Web前端的一些静态资源，如html页面，js、css文件的访问，用法配置示例如下 123456789server &#123; listen 80; server_name localhost; location &#x2F;static&#x2F; &#123; index index.html index.htm; alias &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;html&#x2F;garten-web&#x2F;dist&#x2F;; &#125;&#125; 其中 index指定网站的初始页，可以跟多个文件，空格隔开，nginx根据顺序检查文件是否存在，如上例如果用户直接输入/static则会访问/usr/local/nginx/html/garten-web/dist/index.html（如果不存在则再看index.htm是否存在） alias是与root对应的用法，都用于访问本地文件系统的资源，在匹配到location配置的url路径后，在alias或root配置的目录寻找对应的资源，区别在于：alias就在配置的目录下寻找对应的资源，而root则会将location配置路径附加到root路径后，在拼接后的目录下寻找对应的资源。如上例中访问 /static/hello.html，使用alias则会访问到/usr/local/nginx/html/garten-web/dist/hello.html，使用root则会访问到/usr/local/nginx/html/garten-web/dist/static/hello.html alias配置的目录后有没有“/”要与location后面的路径是否有“/”保持一致，否则找不到资源 3. 反向代理了解反向代理之前先看看什么是正向代理。 举个不那么和谐的例子，当你需要访问某些国外网站的时候，直接输入域名是打不开的，这时可以找一台能访问这些网站的服务器来做代理（这台服务器能访问你访问不了的网站，你能访问这台服务器），你访问网站时，实际是通过代理来中转访问。这种情况，你是知道目标网站的地址的，但是服务器只知道请求来自于代理服务器，而不知道是你（真正的客户端）在访问，所以正向代理代理的是客户端，是对服务端隐藏了真实的客户端信息。 而对于反向代理，客户端是明确的，但具体在后端请求了哪个服务却不明确了，比如你请求的是 www.abc.com， 在反向代理端，它可能是 www.cba.com 的代理，也可能是 www.ccc.com 的代理， 不看配置你是不知道它到底代理的谁。因此，反向代理代理的是服务器端，隐藏了服务端的信息。 nginx中配置反向代理很简单，如下 12345678server &#123; listen 80; server_name localhost; location &#x2F;api&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;192.168.0.120:8080&#x2F;; &#125;&#125; 使用nginx的反向代理，可以解决两个问题： 跨域问题：前后端分离情况下，前端网页访问后端接口存在跨域问题，对后端接口的访问统一通过前端网站域名访问，在nginx中通过对接口的路径进行匹配后反向代理到后端接口服务。如上例中访问接口login可通过 http://localhost/api/login 访问，nginx将会反向代理到 http://192.168.0.120:8080/login 后端接口地址 负载均衡：如果后端服务部署的是服务器集群，则对服务的访问需要做负载均衡，nginx通过反向代理结合upstream来实现负载均衡 反向代理的路径路由规则：如果proxy_pass配置的路径最后带“/”，则类似于alias，不会在proxy_pass的uri后面拼接location的路径，如果没带“/”，则会进行拼接，类似于root。比如我们按上例配置访问 http://localhost/api/login 则代理到 http://192.168.0.120:8080/login 但如果是按以下配置（proxy_pass配置路径不带子路径，且后面没带“/”）， 123location &#x2F;api&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;192.168.0.120:8080;&#125; 则会被代理到 http://192.168.0.120:8080/api/login， 将location的路径拼接了。 如果proxy_pass配置的路径带子路径，如 123location &#x2F;api&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;192.168.0.120:8080&#x2F;api&#x2F;;&#125; 则不管后面带不带“/”，都不会拼接location的路径，只是与/api/后面的部分进行拼接。 注意：如果是不带“/” proxy_pass http://192.168.0.120:8080/api/login 则会被代理到 proxy_pass http://192.168.0.120:8080/apilogin 了， 这时，可通过将location与proxy_pass配置路径保持一致即可——要么都带“/”，要么都不带。 4. 负载均衡nginx通过反向代理proxy_pass结合upstream来对后端服务器集群实现负载均衡，在nginx配置的http节点下定义upstream，如 1234upstream backend &#123; server 192.168.0.120:8080; server 192.168.0.121:8080;&#125; 然后在server节点下的location里配置反向代理，如 123location &#x2F;api&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;backend&#x2F;api&#x2F;;&#125; 这样就会将接收到的请求顺序循环分配到后端的服务器上，如果某个服务器宕机，也能自动将其剔除，不再分配请求，直到其恢复。这是默认的负载均衡策略，即轮询策略。 nginx负载均衡的策略包括: 权重轮询，权重轮询在上述轮询策略的基础上加了服务器的请求分配权重，以根据服务器配置的不同，将更多的请求分配到配置更高的服务器上。如 1234upstream backend &#123; server 192.168.0.120:8080 weight&#x3D;10; server 192.168.0.121:8080 weight&#x3D;20;&#125; 分配给121的请求将比分配给120的请求多一倍。 ip_hash，通过对请求来源ip求hash值，将相同ip的请求分配到相同的服务器上，此种策略可以解决分布式session的问题。配置如 12345upstream backend &#123; ip_hash; server 192.168.0.120:8080; server 192.168.0.121:8080;&#125; url_hash，对访问url求hash值，将同一个url的请求分配到相同的服务器上，对有本地缓存的场景比较适用。配置如 123456upstream backend &#123; server 192.168.0.120:8080; server 192.168.0.121:8080; hash $request_uri; hash_method crc32;&#125; hash_method指定hash算法 fair，根据后端服务器的响应时间来合理分配请求，响应时间短的优先分配。配置如 12345upstream backend &#123; server 192.168.0.120:8080; server 192.168.0.121:8080; fair;&#125; 5. 总结nginx以其轻量级、高性能、高稳定性的特性成为HTTP服务器的主流，是不论开发者还是运维人员都必须了解掌握的服务软件。本文从静态服务器，反向代理，负载均衡三个日常使用场景的角度对nginx进行了简单介绍。 欢迎关注我的微信公众号：jboost-ksxy ——————————————————————————————————————————————————————————————— ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.jboost.cn/categories/DevOps/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://blog.jboost.cn/tags/nginx/"}]},{"title":"Docker笔记（十一）：Dockerfile详解与最佳实践","slug":"docker-11","date":"2019-09-21T10:55:26.000Z","updated":"2020-08-26T01:31:22.967Z","comments":true,"path":"docker-11.html","link":"","permalink":"http://blog.jboost.cn/docker-11.html","excerpt":"Dockerfile是一个文本文件，包含了一条条指令，每条指令对应构建一层镜像，Docker基于它来构建一个完整镜像。本文介绍Dockerfile的常用指令及相应的最佳实践建议。","text":"Dockerfile是一个文本文件，包含了一条条指令，每条指令对应构建一层镜像，Docker基于它来构建一个完整镜像。本文介绍Dockerfile的常用指令及相应的最佳实践建议。 1. 理解构建上下文（build context）Docker镜像通过docker build指令构建，该指令执行时当前的工作目录就是docker构建的上下文，即build context，上下文中的文件及目录都会作为构建上下文内容发送给Docker Daemon。 1docker build --no-cache -t helloapp:v2 -f dockerfiles/Dockerfile context 如上 –no-cache 表示镜像构建时不使用缓存，-f 指定Dockerfile文件位置， context 指定build context目录。 将一些非必要的文件包含到build context中，会导致build context过大，从而导致镜像过大，会增加镜像构建、推送及拉取的时间，以及容器运行时的大小。 执行docker build时会显示build context的大小， 1Sending build context to Docker daemon 187.8MB 最佳实践建议 使用.dockerignore来排除不需要加入到build context中的文件，类似于.gitignore 不要安装不必要的包，所有包含的东西都是镜像必须的，非必须的不要包含。 解耦应用，如果应用有分层，解耦应用到多个容器，便于横向扩展，如web应用程序栈包含web服务应用，数据库，缓存等。 最少化镜像层数：只有RUN、COPY、ADD指令会创建镜像层，其它指令创建临时的中间镜像，不会增大镜像构建的大小 如果可能，尽可能使用多阶段构建，只复制你需要的组件到最终镜像，这使得你可以在中间构建阶段包含工具与debug信息，同时又不会增大最终镜像的大小。 排序多行参数：将参数按字母排序，有利于避免包重复，及后续的维护与提高易读性 2. FROM作用FROM指定基础镜像，每一个定制镜像，必须以一个现有镜像为基础。因此一个Dockerfile中FROM是必须的指令，并且必须是第一条。使用格式， 123FROM &lt;image&gt;:&lt;tag&gt;# 注释以#开头。基础镜像的tag可不指定，默认使用latest# 示例：FROM mysql:5.7 最佳实践建议 如果不想以任何镜像为基础，则可以使用FROM scratch 尽量使用官方镜像作为基础镜像 推荐使用Alpine镜像，因为它足够轻量级（小于5MB），但麻雀虽小五脏俱全，基本具有Linux的基础功能 3. RUN作用用来执行命令行命令，是最常用的指令之一。使用格式， 123456# shell格式，跟直接在命令行输入命令一行RUN &lt;命令&gt;# 示例：RUN mkdir -p /usr/src/redis # exec格式，类似于函数调用RUN [\"可执行文件\", \"参数1\", \"参数2\"] RUN指令创建的中间镜像会被缓存，并会在下次构建中使用。如果不想使用这些缓存镜像，可以在构建指令中指定–no-cache参数，如：docker build --no-cache 最佳实践建议 将比较长的复杂的指令通过 \\ 分为多行，让Dockerfile文件可读性、可理解性、可维护性更高，将多个指令通过 &amp;&amp; 连接，减少镜像的层数 确保每一层只添加必需的东西，任何无关的东西都应该清理掉，如所有下载、展开的文件，apt 缓存文件等，以尽可能减少镜像各层的大小 将RUN apt-get update 与 RUN apt-get install 组合成一条RUN指令（将apt-get update单独作为一条指令会因为缓存问题导致后续的apt-get install 指令失败） 比如先按如下Dockerfile创建了一个镜像 123FROM ubuntu:18.04RUN apt-get updateRUN apt-get install -y curl 一段时间后，再按以下Dockerfile创建另一个镜像 123FROM ubuntu:18.04RUN apt-get updateRUN apt-get install -y curl nginx 因为RUN指令创建的镜像层会被缓存，所以下面镜像的RUN apt-get update并不会执行，直接使用了前面构建的镜像层，这样，curl、nginx就可能安装已经过时的版本。 因此 在 apt-get update 之后立即接 &amp;&amp; apt-get install -y ，这叫做“ cache busting”（缓存破坏），也可以通过指定包的版本，来达到同样的目的，这叫“ version pinning” （版本指定）示例： 123456RUN apt-get update &amp;&amp; apt-get install -y \\ reprepro \\ ruby1.9.1 \\ ruby1.9.1-dev \\ #删除apt 缓存减少镜像层的大小 &amp;&amp; rm -rf /var/lib/apt/lists/* 使用管道（pipes）。一些RUN指令依赖于从一个指令管道输出到另一个，如1RUN wget -O - https://some.site | wc -l &gt; /number Docker使用/bin/sh -c 解释器来执行这些指令，只会评估管道最后一条命令的退出码来确定是否成功，如上例中只要wc -l成功了就算wget失败，也会认为是成功的。如果要使管道命令的任何一步报错都导致指令失败，则可通过加 set -o pipefile &amp;&amp; 来实现，如1RUN set -o pipefail &amp;&amp; wget -O - https://some.site | wc -l &gt; /number 不是所有的shell都支持-o pipefail选项，如果不支持的话可以使用如下形式，显式地指定一个支持的shell1RUN [\"/bin/bash\", \"-c\", \"set -o pipefail &amp;&amp; wget -O - https://some.site | wc -l &gt; /number\"] 4. COPY | ADD作用COPY从构建上下文的目录中复制文件/目录到镜像层的目标路径。使用格式， 12COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;源路径&gt;... &lt;目标路径&gt; COPY [--chown=&lt;user&gt;:&lt;group&gt;] [\"&lt;源路径1&gt;\",... \"&lt;目标路径&gt;\"] 同RUN一样，也有两种格式。源文件可以多个，甚至可以是通配符，目标路径是容器的绝对路径，可以是相对工作目录（WORKDIR指定）的相对路径，目标路径不存在时会自动创建。使用--chown=&lt;user&gt;:&lt;group&gt;来改变文件的所属用户与组。ADD与COPY的使用格式与性质差不多，但功能更丰富，如源路径可以是URL（下载后放到目标路径下，文件权限为600），也可以为tar压缩包，压缩格式为gzip，bzip2及xz的情况下，ADD 指令将会自动解压缩这个压缩文件到目标路径去 最佳实践建议 如果在Dockerfile中有多处需要使用不同的文件，分别使用COPY，而不是一次性COPY所有的，这可以保证每一步的构建缓存只会在对应文件改变时，才会失效。比如123COPY requirements.txt /tmp/RUN pip install --requirement /tmp/requirements.txtCOPY . /tmp/ 如果把COPY . /tmp/ 放在RUN上面，将使RUN层镜像缓存失效的场景更多——因为 . 目录（当前目录）中任何一个文件的改变都会导致缓存失效。 因为镜像大小的原因， 使用ADD来获取远程包是非常不推荐的，应该使用curl或wget，这种方式可以在不再需要使用时删除对应文件，而不需要增加额外的层，如，应避免如下用法123ADD http://example.com/big.tar.xz /usr/src/things/RUN tar -xJf /usr/src/things/big.tar.xz -C /usr/src/thingsRUN make -C /usr/src/things all 而应使用1234RUN mkdir -p /usr/src/things \\ &amp;&amp; curl -SL http://example.com/big.tar.xz \\ | tar -xJC /usr/src/things \\ &amp;&amp; make -C /usr/src/things all 如果不需要使用ADD的自动解压特性，尽量使用COPY（语义更清晰） 5. CMD作用CMD指定容器的启动命令。容器实质就是进程，进程就需要启动命令及参数，CMD指令就是用于指定默认的容器主进程的启动命令的。使用格式 123456# shell格式 CMD &lt;命令&gt;# exec格式CMD [\"可执行文件\", \"参数1\", \"参数2\"...]# 参数列表格式，在指定了ENTRYPOINT指令后，用CMD来指定具体的参数CMD [\"参数1\", \"参数2\"...] 在容器运行时可以指定新的命令来覆盖Dockerfile中设置的这个默认命令 最佳实践建议 服务类镜像建议：CMD [&quot;apache2&quot;,&quot;-DFOREGROUND&quot;]，CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;] 容器进程都应以前台运行，不能以后台服务的形式运行，否则启动就退出了。 其它镜像，建议给一个交互式的shell，如bash，python，perl等：CMD [&quot;python&quot;], CMD [&quot;php&quot;, &quot;-a&quot;] 6. ENTRYPOINT作用ENTRYPOINT的目的和CMD一样，都是在指定容器启动是要运行的程序及参数。 ENTRYPOINT在运行时也可以替代，不过比CMD要略显繁琐，需要通过docker run的参数 –entrypoint 来指定。如果指定了ENTRYPOINT，则CMD将只是提供参数，传递给ENTRYPOINT。使用ENTRYPOINT可以在容器运行时直接为默认启动程序添加参数。 与RUN指令格式一样，ENTRYPOINT也分为exec格式和shell格式。 最佳实践建议 ENTRYPOINT可用来指定镜像的主命令，允许镜像能像命令一样运行，可以使用CMD来作为默认的标志（参数），如12ENTRYPOINT [\"s3cmd\"]CMD [\"--help\"] 直接run时，相当于执行了s3cmd --help。也可以使用shell脚本，在脚本中做一些预处理的工作，如123COPY ./docker-entrypoint.sh /ENTRYPOINT [\"/docker-entrypoint.sh\"]CMD [\"postgres\"] 7. LABEL作用为镜像添加label以方便组织镜像，记录licensce信息，帮助自动化实现等等。字符串中包含空格需要转义或包含在引号中， 如 123456789101112131415# Set one or more individual labelsLABEL com.example.version=\"0.0.1-beta\"LABEL vendor1=\"ACME Incorporated\"LABEL com.example.release-date=\"2019-09-12\"LABEL com.example.version.is-production=\"\"# Set multiple labels on one lineLABEL com.example.version=\"0.0.1-beta\" com.example.release-date=\"2019-09-12\"# Set multiple labels at once, using line-continuation characters to break long linesLABEL vendor=ACME\\ Incorporated \\ com.example.is-beta= \\ com.example.is-production=\"\" \\ com.example.version=\"0.0.1-beta\" \\ com.example.release-date=\"2019-09-12\" 8. ENV作用ENV设置环境变量，无论是后面的其它指令，如 RUN（使用 $环境变量key 的形式） ，还是运行时的应用，都可以直接使用这里定义的环境变量。 使用格式有两种， 1234#只能设置一个key valueENV &lt;key&gt; &lt;value&gt;#可以设置多个，value中如果包含空格可以使用\\来进行转义，也可以通过\"\"括起来；也可以用反斜线来续行ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;... 除了RUN，还有这些指令可以引用环境变量：ADD 、 COPY 、 ENV 、 EXPOSE 、 LABEL 、 USER 、 WORKDIR 、 VOLUME 、STOPSIGNAL 、 ONBUILD 最佳实践建议 定义环境变量，更新PATH环境变量，如要使 CMD [“nginx”] 运行，可设置环境变量 ENV PATH /usr/local/nginx/bin:$PATH ENV也可以用于定义常量，便于维护 9. ARG作用ARG设置构建参数，即docker build命令时传入的参数。和ENV的效果差不多，都是设置环境变量，不同的是，ARG设置的是构建环境的环境变量，在容器运行时是不会存在这些环境变量的。Dockerfile中的ARG指令是定义参数名称，以及默认值（可选）。该默认值可以在执行构建命令docker build时用 –build-arg &lt;参数名&gt;=&lt;值&gt; 来覆盖。使用格式， 1ARG &lt;参数名&gt;[=&lt;默认值&gt;] 最佳实践建议 不要使用ARG来保存密码之类的信息，因为通过docker history还是可以看到docker build执行时的所有值 使用ARG，对于使用CI系统（持续集成），用同样的构建流程构建不同的 Dockerfile 的时候比较有帮助，避免构建命令必须根据每个 Dockerfile 的内容修改 10. WORKDIR作用WORKDIR用于指定工作目录（或当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，会自动创建。使用格式， 123456789101112131415WORKDIR &lt;工作目录路径&gt;``` **最佳实践建议**1. WORKDIR应该使用绝对路径，显得更为清楚、可靠2. 使用WORKDIR，避免使用`RUN cd … &amp;&amp; do-something`，可读性差，难以维护## 11. VOLUME**作用**VOLUME用于定义匿名卷。容器运行时应该尽量保持容器存储层不发生写操作，应该将数据写入存储卷。VOLUME就是为了防止运行时用户忘记将动态文件所保存的目录挂载为卷，我们事先在Dockerfile中指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。使用格式，```shellVOLUME [\"&lt;路径1&gt;\", \"&lt;路径2&gt;\"...]VOLUME &lt;路径&gt; 如 VOLUME /data， 任何向/data目录写入的数据都会写入匿名卷。可以运行容器时覆盖这个挂载设置 docker run -d -v host-path:/data xxxx 最佳实践建议 VOLUME应该被用来暴露所有的数据存储，配置存储，或者被容器创建的文件、目录。 如果数据动态变化，强烈建议使用VOLUME。 12. EXPOSE作用EXPOSE指令是声明运行时容器提供的服务端口，也只是一个声明，在容器运行时并不会因为这个声明应用就一定会开启这个端口的服务,容器启动时，还是需要通过 -p host-port:container-port来实现映射。EXPOSE主要是帮助镜像使用者了解这个镜像服务的监听端口，以方便进行映射配置，另一个用处是在运行时如果是使用随机端口映射，也就是通过 docker run -P的形式时，会自动随机映射EXPOSE声明的端口。使用格式， 1234567891011121314EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...] ``` **最佳实践建议**1. 应该使用常用的惯用的端口，如nginx 80，mongoDB 27017## 13. USER**作用**USER指令和WORKDIR相似，都是改变环境状态并影响以后的层。 WORKDIR是改变工作目录， USER则是改变之后的层在执行RUN , CMD以及ENTRYPOINT这类命令时的身份。USER帮助你切换到指定的用户，这个用户必须是事先建立好的，否则无法切换。使用格式```shellUSER &lt;用户名&gt;[:&lt;用户组&gt;] 最佳实践建议 如果一个服务不需要权限也能运行，则使用USER来切换到非root用户，如RUN groupadd -r postgres &amp;&amp; useradd --no-log-init -r -g postgres postgres 避免使用sudo，因为可能存在一些不可预见的TTY与信号转发行为导致问题，如果实在需要，考虑使用“gosu”。为了减少镜像层数，应避免不断切换USER使用gosu示例 123456789# 建立 redis 用户，并使用 gosu 换另一个用户执行命令RUN groupadd -r redis &amp;&amp; useradd -r -g redis redis# 下载 gosuRUN wget -O /usr/local/bin/gosu \"https://github.com/tianon/gosu/releases/download/1.7/gosu-amd64\" \\&amp;&amp; chmod +x /usr/local/bin/gosu \\&amp;&amp; gosu nobody true# 设置 CMD，并以另外的用户执行CMD [ \"exec\", \"gosu\", \"redis\", \"redis-server\" ] 14. HEALTHCHECK作用HEALTHCHECK用于检查容器的健康状态，Docker可通过健康状态来决定是否对容器进行重新调度。使用格式 1HEALTHCHECK [选项] CMD &lt;命令&gt; 支持的选项为 –interval=&lt;间隔&gt; ：两次健康检查的间隔，默认为30秒 –timeout=&lt;时长&gt; ：执行健康检查命令的超时时间，如果超时，则本次健康检查就被视为失败，默认30秒 –retries=&lt;次数&gt; ：当连续失败指定的次数后，将容器状态置为unhealthy ，默认3次 命令的返回值决定了该次健康检查的成功与否—— 0 ：成功； 1 ：失败； 2 ：保留（不要使用这个值），如： 123456789101112131415161718192021FROM nginxRUN apt-get update &amp;&amp; apt-get install -y curl &amp;&amp; rm -rf /var/lib/apt/lists/*HEALTHCHECK --interval=5s --timeout=3s \\ CMD curl -fs http://localhost/ || exit 1``` 可以使用docker ps 或docker inspect来查看容器的健康状态。**最佳实践建议**1. 如果基础镜像有健康检查指令，想要屏蔽掉其健康检查，可以使用`HEALTHCHECK NONE`2. 对一些可能造成假死（进程还在， 但提供不了服务了）的服务建议提供健康检查，以便及时重新调度恢复服务## 15. ONBUILD**作用**ONBUILD后跟的指令，只有当以当前镜像为基础镜像，去构建下一级镜像的时候才会被执行。使用格式```shellONBUILD &lt;其它指令&gt; 它后面跟的是其它指令，比如 RUN , COPY 等，这些指令在当前镜像构建时并不会被执行。ONBUILD命令在本镜像的子镜像中执行，把ONBUILD想象为父镜像为子镜像声明的一条指令，Docker会在子镜像所有命令之前执行ONBUILD指令。 最佳实践建议 当在ONBUILD指令中使用ADD或COPY时要注意，如果build context中没有指定的资源，可能导致灾难性的错误。 16. 用缓存镜像提高效率Docker在构建镜像时会复用缓存中已经存在的镜像，如果明确不使用缓存，则可加参数docker build --no-cache=true使用缓存镜像的规则 从一个已存在于缓存的父镜像开始构建，则会将当前镜像的下一行指令与所有继承于那个父镜像的子镜像比较，如果其中没有一个是使用相同的指令构建的，则缓存失效 大部分情况下，将Dockerfile中的指令与其中一个子镜像简单比较就够了，但是某些指令需要更多的检查与说明：对于ADD，COPY指令，文件内容会被检查，会计算每一个文件的checksum，checksum中不会考虑最后修改及最后访问时间，在缓存中查找时，checksum会与已经存在的镜像进行比较，如果文件中有修改，则缓存失效。除了ADD，COPY命令，缓存检查不会查看容器中的文件来决定缓存匹配，如处理RUN apt-get -y update命令时，容器中文件的更新不会进行检查来确定缓存是否命中， 这种情况下， 只会检查指令字符串本身是否匹配。 一旦缓存失效，所有后续的指令都会产生新的镜像，不会再使用缓存。 17. 其它镜像构建方式 通过标准输入来生成Dockerfile构建，不会发送build context（从stdin读取build context，只包含Dockerfile），适用于一次性构建，不需要写Dockerfile 12345678910111213# 将会构建一个名称与tag均为none的镜像echo -e 'FROM busybox\\nRUN echo \"hello world\"' | docker build -#或 docker build - &lt;&lt;EOFFROM busyboxRUN echo \"hello world\"EOF# 构建一个命名的镜像docker build -t myimage:latest - &lt;&lt;EOFFROM busyboxRUN echo \"hello world\"EOF 连字符 - 作为文件名告诉Docker从stdin读取Dockerfile 使用stdin来生成Dockerfile， 但是使用当前目录作为build context 123456# build an image using the current directory as context, and a Dockerfile passed through stdindocker build -t myimage:latest -f- . &lt;&lt;EOFFROM busyboxCOPY somefile.txt .RUN cat /somefile.txtEOF 使用远程git仓库构建镜像，从stdin生成Dockerfile 1234docker build -t myimage:latest -f - https://github.com/docker-library/hello-world.git &lt;&lt;EOFFROM busyboxCOPY hello.c .EOF 欢迎关注我的微信公众号：jboost-ksxy ——————————————————————————————————————————————————————————————— ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.jboost.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.jboost.cn/tags/docker/"}]},{"title":"JDK13，不如温习下Java8","slug":"jdk8","date":"2019-09-18T11:24:09.000Z","updated":"2019-09-18T11:36:40.834Z","comments":true,"path":"jdk8.html","link":"","permalink":"http://blog.jboost.cn/jdk8.html","excerpt":"JDK13于昨天正式GA，版本新特性可参考： https://www.oschina.net/news/109934/jdk-13-released","text":"JDK13于昨天正式GA，版本新特性可参考： https://www.oschina.net/news/109934/jdk-13-released 虽然JDK更新迅速，但开发者貌似并不买账，据统计，目前仍以JDK8使用最多，预计可能还会延续好长一段时间。虽然JDK版本已至13，但对Java8的新特性，掌握程度如何呢？本文对Java8的主要特性进行了梳理。供温习参考。 1. 接口默认方法以前的接口只允许有抽象方法（没有实现体），java8中提供了接口默认方法支持，即可以提供方法的默认实现，实现类可以直接继承，也可以覆盖。默认方法主要解决接口的修改导致现有实现类不兼容的问题。 123456789101112131415161718192021222324252627282930@RunWith(SpringRunner.class)@SpringBootTestpublic class InterfaceDefaultFunctionTest &#123; public interface MyFunction&lt;T&gt; &#123; T func(T t); //默认方法 default int func2(T t)&#123; return t.hashCode(); &#125; //静态方法 static&lt;T&gt; void print(T t) &#123; System.out.println(t); &#125; &#125; @Test public void testInterface()&#123; MyFunction&lt;String&gt; myFunction = new MyFunction&lt;String&gt;()&#123; @Override public String func(String s) &#123; return s.toUpperCase(); &#125; &#125;; System.out.println(myFunction.func(\"abc\")); System.out.println(myFunction.func2(\"abc\")); LambdaTest.MyFunction.print(\"efg\"); &#125;&#125; 默认方法通过关键字 default 声明。同时也可以在接口中定义静态方法。 2. 函数式接口函数式接口就是有且仅有一个抽象方法的接口（可以有其它非抽象方法），如1所示代码中 MyFunction 就是一个函数式接口，只有一个抽象方法 func， 其它非抽象方法如默认方法 func2， 静态方法 print 不影响其函数式接口的特性。 函数式接口可以使用注解 @FunctionalInterface 标注，该注解会去检查接口是否符合函数式接口的规范要求，不符合的话IDE会给出提示。 java中内置了一些函数式接口， 函数式接口 描述 Consumer 包含方法 void accept(T t)， 对类型为T的对象t进行操作 Supplier 包含方法 T get()，返回类型为T的对象 Function&lt;T,R&gt; 包含方法 R apply(T t)，对类型为T的对象进行操作，返回类型R的对象 Predicat 包含方法 boolean test(T t)， 判断类型为T的对象是否满足条件 以及基于这些接口的其它变种或子接口，如BiConsumer&lt;T,U&gt;，BiFunction&lt;T,U,R&gt;等。还有如Runnable，Callable等接口，也属于函数式接口 —— 都只有一个抽象方法。 12345678910111213@FunctionalInterfacepublic interface BiConsumer&lt;T, U&gt; &#123; void accept(T t, U u); default BiConsumer&lt;T, U&gt; andThen(BiConsumer&lt;? super T, ? super U&gt; after) &#123; Objects.requireNonNull(after); return (l, r) -&gt; &#123; accept(l, r); after.accept(l, r); &#125;; &#125;&#125; 3. Lambda表达式lambda表达式实质就是一个匿名函数，在python中很常见，java到了jdk8提供了支持。lambda表达式的格式形如： (参数) -&gt; {方法体语句}，当参数只有一个时，左边小括号可以省略，当方法体语句只有一条时，右边大括号可以省略。 Java的lambda表达式基本上是对函数式接口实现的一种简化 —— 用lambda表达式直接代替一个函数式接口的具体实现（抽象方法的实现）。当我们使用jdk8在IDE中编写1中代码时，IDE会给出提示， 匿名实现类可以用lambda表达式替换。上述代码使用lambda表达式替换可调整为， 123456@Testpublic void testInterface()&#123; MyFunction&lt;String&gt; myFunction = s -&gt; s.toUpperCase(); System.out.println(myFunction.func(\"abc\")); System.out.println(myFunction.func2(\"abc\"));&#125; lambda表达式甚至可作为方法参数传入（实质也是作为一个函数式接口的实现类实例） 1234567891011121314@FunctionalInterfacepublic interface MyFunction&lt;T&gt; &#123; T func(T t);&#125;public void print(MyFunction&lt;String&gt; function, String s)&#123; System.out.println(function.func(s));&#125;@Testpublic void testInterface()&#123; //将lambda表达式作为方法参数传入 print((String s) -&gt; s.toUpperCase(), \"abc\");&#125; 局部变量在lambda表达式中是只读的，虽可不声明为final，但无法修改。如 123456@Testpublic void testInterface()&#123; int i = 1; //lambda表达式中无法修改局部变量i，将报编译错误 print((String s) -&gt; &#123;i = i+10; return s.toUpperCase();&#125;, \"abc\");&#125; 4. 方法引用当需要使用lambda表达式时，如果已经有了相同的实现方法，则可以使用方法引用来替代lambda表达式，几种场景示例如下。 12345678910111213141516171819202122232425262728293031323334353637@RunWith(SpringRunner.class)@SpringBootTestpublic class FunctionReferenceTest &#123; @Test public void testFunctionReference() &#123; // 实例::实例方法 Consumer&lt;String&gt; consumer = s -&gt; System.out.println(s); //lambda表达式 Consumer&lt;String&gt; consumer2 = System.out::println; //方法引用 consumer.accept(\"abc\"); consumer2.accept(\"abc\"); //类::静态方法 Comparator&lt;Integer&gt; comparator = (x, y) -&gt; Integer.compare(x, y); //lambda表达式 Comparator&lt;Integer&gt; comparator2 = Integer::compare; //方法引用 System.out.println(comparator.compare(10, 8)); System.out.println(comparator2.compare(10, 8)); //类::实例方法， 当引用方法是形如 a.func(b)时，用类::实例方法的形式 BiPredicate&lt;String, String&gt; biPredicate = (a, b) -&gt; a.equals(b); //lambda表达式 BiPredicate&lt;String, String&gt; biPredicate2 = String::equals; //方法引用 System.out.println(biPredicate.test(\"abc\", \"abb\")); System.out.println(biPredicate2.test(\"abc\",\"abb\")); //type[]::new 数组引用 Function&lt;Integer,Integer[]&gt; fun= n-&gt; new Integer[n]; //lambda表达式 Function&lt;Integer,Integer[]&gt; fun2=Integer[]::new; //方法引用 System.out.println(fun.apply(10)); System.out.println(fun2.apply(10)); //构造器引用 Function&lt;String,String&gt; func = n-&gt; new String(n); //lambda表达式 Function&lt;String,String&gt; func2 = String::new; //方法引用 System.out.println(func.apply(\"aaa\")); System.out.println(func2.apply(\"aaa\")); &#125;&#125; 5. Stream APIStream与lambda应该是java8最重要的两大特性。Stream 对集合的处理进行了抽象，可以对集合进行非常复杂的查找、过滤和映射等操作。提供了一种高效的且易于使用的处理数据的方式。Stream的三个特性： Stream本身不会存储元素 Stream不会改变操作对象（即集合），会返回一个新的Stream Stream的中间操作不会立刻执行，而是会等到需要结果的时候才执行 Java8 的Collection接口包含了两个方法 stream(), parallelStream()， 分别返回一个顺序流与一个并行流，所有Collection类型（如List， ）的对象可以调用这两个方法生成流。Java8 的Arrays类也提供了 stream(T[] array)等方法用以生成流。也可以使用静态方法 Stream.iterate() 和 Stream.generate() 来创建无限流。 Stream的中间操作包括 操作 描述 filter(Predicate p) 接收 Lambda ， 从流中过滤出满足条件的元素 distinct() 通过hashCode() 和 equals() 去除重复元素 limit(long maxSize) 截断流，使元素的个数不超过给定数量 skip(long n) 跳过前面的n个元素，若流中元素不足n个，则返回一个空流 map(Function f) 将每个元素使用函数f执行，将其映射成一个新的元素 mapToDouble(ToDoubleFunction f) 将每个元素使用f执行，产生一个新的DoubleStream流 mapToInt(ToIntFunction f) 将每个元素使用f执行，产生一个新的IntStream流 mapToLong(ToLongFunction f) 将每个元素使用f执行，产生一个新的LongStream流 flatMap(Function f) 将流中的每个值都通过f转换成另一个流，然后把所有流连接成一个流 sorted() 按自然顺序排序，产生一个新流 sorted(Comparator comp) 根据比较器排序，产生一个新流 allMatch(Predicate p) 判断是否匹配所有元素 anyMatch(Predicate p) 判断是否匹配至少一个元素 noneMatch(Predicate p) 判断是否没有匹配任意元素 findFirst() 返回第一个元素 findAny() 返回任意一个元素 reduce(T iden, BinaryOperator b) 对流中的元素进行reduce操作，返回T类型对象 reduce(BinaryOperator b) 对流中的元素进行reduce操作，返回Optional对象 Stream的终止操作包括 操作 描述 count() 返回元素总数 max(Comparator c) 返回最大值 min(Comparator c) 返回最小值 forEach(Consumer c) 内部迭代调用Consumer操作 collect(Collector c) 将流转换为其他形式，一般通过Collectors来实现 Stream使用示例 123456789101112131415161718192021222324252627282930313233@Testpublic void testStream() &#123; List&lt;User&gt; list = new ArrayList&lt;&gt;(); //转换为List，这里没啥意义，仅做示范 List&lt;User&gt; users = list.stream().collect(Collectors.toList()); //转换为Set Set&lt;User&gt; users1 = list.stream().collect(Collectors.toSet()); //转换为Collection Collection&lt;User&gt; users2 = list.stream().collect(Collectors.toCollection(ArrayList::new)); //计数 long count = list.stream().collect(Collectors.counting()); //求和 int total = list.stream().collect(Collectors.summingInt(User::getAge)); //求平均值 double avg= list.stream().collect(Collectors.averagingInt(User::getAge)); //获取统计对象，通过该统计对象可获取最大值，最小值之类的数据 IntSummaryStatistics iss= list.stream().collect(Collectors.summarizingInt(User::getAge)); //将值通过\",\"拼接 String str= list.stream().map(User::getName).collect(Collectors.joining(\",\")); //最大值 Optional&lt;User&gt; max= list.stream().collect(Collectors.maxBy(Comparator.comparingInt(User::getAge))); //最小值 Optional&lt;User&gt; min = list.stream().collect(Collectors.minBy(Comparator.comparingInt(User::getAge))); //从累加器开始，对指定的值，这里是年龄，进行sum的reduce操作 int t =list.stream().collect(Collectors.reducing(0, User::getAge, Integer::sum)); //对转换的结果再进行处理 int how = list.stream().collect(Collectors.collectingAndThen(Collectors.toList(), List::size)); //分组 Map&lt;String, List&lt;User&gt;&gt; map= list.stream().collect(Collectors.groupingBy(User::getName)); //根据条件进行分区 Map&lt;Boolean,List&lt;User&gt;&gt; vd= list.stream().collect(Collectors.partitioningBy(u -&gt; u.getName().startsWith(\"W\")));&#125; 6. Optional类Optional是一个容器类，可以避免显式的null判断，基本使用示例如下 1234567891011121314151617181920212223242526272829303132@RunWith(SpringRunner.class)@SpringBootTestpublic class OptionalTest &#123; @Test public void testOptional()&#123; // of 不允许传入null值，否则抛出NPE Optional&lt;Integer&gt; optional = Optional.of(new Integer(10)); System.out.println(optional.get()); // ofNullable 允许传入null，但是直接调用get会抛出NoSuchElementException异常， // 可通过isPresent判断是否存在值 Optional&lt;Integer&gt; optional1 = Optional.ofNullable(null); if(optional1.isPresent()) &#123; System.out.println(optional1.get()); &#125;else&#123; System.out.println(\"optional1 is empty\"); &#125; // orElse 判断是否存在值，存在则返回，不存在则返回参数里的值 Integer value = optional1.orElse(new Integer(0)); // map方法，如果optional有值，则对值进行处理返回新的Optional， // 如果没有值则返回Optional.empty() optional = optional.map(x -&gt; x*x); System.out.println(optional.get()); // 与map类似，只是要求返回值必须是Optional，进一步避免空指针 optional = optional.flatMap(x -&gt;Optional.of(x*x)); System.out.println(optional.get()); &#125;&#125; 7. Base64在java8中，Base64成为了java类库的标准，可直接使用 123456789101112131415import java.util.Base64;@RunWith(SpringRunner.class)@SpringBootTestpublic class Base64Test &#123; @Test public void testBase64()&#123; //base64编码 String encode = Base64.getEncoder().encodeToString(\"abc\".getBytes()); System.out.println(encode); //base64解码 System.out.println(new String(Base64.getDecoder().decode(encode))); &#125;&#125; 8. 日期时间类以前的Date类是非线程安全的，并且一些常用的日期时间运算需要自己编写util工具类。java8推出了java.time包，里面包含了如 LocalDate, LocalTime, LocalDateTime等类，可方便地进行日期时间的运算，如日期间隔、时间间隔，日期时间的加减，格式化等等。 —————————————————————————————作者：空山新雨欢迎关注我的微信公众号：jboost-ksxy","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.jboost.cn/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"}]},{"title":"Docker笔记（十）：使用Docker来搭建一套ELK日志分析系统","slug":"docker-elk","date":"2019-09-07T05:43:37.000Z","updated":"2020-08-26T01:32:05.764Z","comments":true,"path":"docker-elk.html","link":"","permalink":"http://blog.jboost.cn/docker-elk.html","excerpt":"一段时间没关注ELK（elasticsearch —— 搜索引擎，可用于存储、索引日志, logstash —— 可用于日志传输、转换，Kibana —— WebUI，将日志可视化），发现最新版已到7.4了。所以别问程序员为什么这么忙？因为不是在加班就是在学习新框架中。本文整理了使用Docker来快速搭建一套ELK日志分析系统的方法。","text":"一段时间没关注ELK（elasticsearch —— 搜索引擎，可用于存储、索引日志, logstash —— 可用于日志传输、转换，Kibana —— WebUI，将日志可视化），发现最新版已到7.4了。所以别问程序员为什么这么忙？因为不是在加班就是在学习新框架中。本文整理了使用Docker来快速搭建一套ELK日志分析系统的方法。 1. 部署elkgithub上有人整理了一套使用docker compose来部署elk的配置，可直接下载使用。 1git clone https://github.com/deviantony/docker-elk.git 如果没有git，那就安装一下（yum install git），或者直接下载github仓库的源码包。 当前是基于7.2.1版（docker-elk目录下.env文件中定义，可修改）。 调整一下相应的配置。 修改docker-compose，设置es密码等， 12345678910111213141516171819vim docker-compose.yml # 在elasticsearch部分设置环境变量，将jvm堆内存增大到了1g，设置es elastic用户的密码 environment: ES_JAVA_OPTS: \"-Xmx1g -Xms1g\" ELASTIC_PASSWORD: Passw0rd # 将logstash的端口映射从默认的5000改为5044，因为后面会用filebeat，不改也可以，对应就行 ports: - \"5044:5044\" - \"9600:9600\" # 将jvm内存也增大一点 environment: LS_JAVA_OPTS: \"-Xmx512m -Xms512m\" # 在volumes部分增加es数据目录的挂载，对es数据持久化，避免容器销毁数据丢失 volumes: - /mnt/elk/esdata:/usr/share/elasticsearch/data 注意： 因为es容器内部是以elasticsearch用户启动进程的，所以在做持久化数据目录挂载的时候，需要将目录权限进行设置，否则会因为没有访问权限而启动失败。elasticsearch的uid是1000，可以建一个uid为1000的用户，然后将目录所有者赋予该用户。 修改es配置文件，将xpack从trial改为basic，禁用付费功能 1234vim elasticsearch/config/elasticsearch.yml #xpack.license.self_generated.type: trial xpack.license.self_generated.type: basic 修改logstash配置文件，设置es的用户名密码 1234vim logstash/config/logstash.yml xpack.monitoring.elasticsearch.username: elastic xpack.monitoring.elasticsearch.password: Passw0rd 修改logstash的pipeline配置 123456789101112131415161718vim logstash/pipeline/logstash.conf # 这里codec根据具体情况配置 input &#123; beats &#123; port =&gt; 5044 codec =&gt; \"json\" &#125; &#125; ## Add your filters / logstash plugins configuration here output &#123; elasticsearch &#123; hosts =&gt; \"elasticsearch:9200\" index =&gt; \"%&#123;[@metadata][beat]&#125;-%&#123;[@metadata][version]&#125;-%&#123;+YYYY.MM.dd&#125;\" user =&gt; \"elastic\" password =&gt; \"Passw0rd\" &#125; &#125; 修改kibana配置，设置es密码 12345vim kibana/config/kibana.yml ## X-Pack security credentials elasticsearch.username: elastic elasticsearch.password: Passw0rd 配置调整后，使用 docker-compose up -d 即可启动es，logstash，kibana三个容器。第一次启动需要下载所有镜像，会比较慢，启动完后，访问 elk所在服务器IP:5601即可进入kibana页面。 这里默认是起一个es容器，如果想起多个，参考： https://github.com/deviantony/docker-elk/wiki/Elasticsearch-cluster 2. 部署filebeatfilebeat部署在产生日志的服务器上。先下载镜像， 1docker pull docker.elastic.co/kibana/kibana:7.3.1 下载一个示例配置文件 1curl -L -O https://raw.githubusercontent.com/elastic/beats/7.3/deploy/docker/filebeat.docker.yml 修改配置文件 123456789101112131415161718192021222324vim filebeat.docker.ymlfilebeat.config: modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: false#filebeat.autodiscover:# providers:# - type: docker# hints.enabled: true#processors:#- add_cloud_metadata: ~#- add_host_metadata: ~filebeat.inputs:- type: log enabled: true paths: - /var/log/elk/*.logoutput.logstash: hosts: [\"你的elk服务器IP:5044\"] 去掉了一些不必要的配置，基本就是一个input, 一个output。input paths部分配置你日志所在目录，注意这里是容器内的目录，真正服务器的日志目录需要在启动容器时挂载到这里配置的目录。 启动容器 12docker run -d --name filebeat --user=root -v $(pwd)/filebeat.docker.yml:/usr/share/filebeat/filebeat.yml:ro \\ -v /mnt/logs/elk/:/var/log/elk/ docker.elastic.co/beats/filebeat:7.3.1 filebeat -e -strict.perms=false 对配置文件及实际日志目录与容器日志目录进行了挂载。 启动成功后，对应目录下的日志就会通过filebeat，logstash传输到es，进入kibana对日志数据建立索引进行查询了。 3. 总结前面用elk来搭建日志分析系统还是5.1版，两年时间已到7.4，配置方式，包括UI风格都做了很大的调整，很有一种人间一年，技术圈十载的感觉。本文整理了基于Docker来搭建ELK框架的整个过程，供参考。—————————————————————————————作者：空山新雨欢迎关注我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号）","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.jboost.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.jboost.cn/tags/docker/"},{"name":"elk","slug":"elk","permalink":"http://blog.jboost.cn/tags/elk/"}]},{"title":"Docker笔记（九）：网络管理","slug":"docker-9","date":"2019-08-30T05:14:57.000Z","updated":"2020-08-26T01:31:11.977Z","comments":true,"path":"docker-9.html","link":"","permalink":"http://blog.jboost.cn/docker-9.html","excerpt":"Docker的应用运行在容器中，其相互之间或与外部之间是如何通信的，涉及到哪些知识点，本文对相关内容进行整理。因网络这块牵涉的面较多，因此只从日常使用或理解的角度出发，过于专业的就不深入探讨了。","text":"Docker的应用运行在容器中，其相互之间或与外部之间是如何通信的，涉及到哪些知识点，本文对相关内容进行整理。因网络这块牵涉的面较多，因此只从日常使用或理解的角度出发，过于专业的就不深入探讨了。 1. Docker默认的网络拓扑在Docker笔记（二）：Docker管理的对象中，介绍了Docker通过一些驱动程序来实现容器之间或容器与外部的互联，包括bridge（默认的虚拟网桥形式），host（与主机共享网络栈），overlay（跨Docker Daemon容器间的互联），macvlan（为容器分配mac地址），none（禁用所有网络）等。 默认情况下，Docker启动时会创建一个虚拟网桥 docker0，可以理解为一个软件交换机。当创建一个 Docker 容器的时候，会创建一对 veth pair 接口（当数据包发送到一个接口时，另外一个接口也可以收到相同的数据包）。这对接口一端在容器内，即 eth0 ；另一端在宿主机本地并被挂载到 docker0 网桥，名称以veth 开头，如 veth340c305，docker0会在挂载到它上面的网口之间进行转发，从而实现主机与容器之间及容器与容器之间的相互通信。Docker默认的网络拓扑图如下： 我们可以在宿主机上通过ifconfig查看相关的网络接口， 12345678910111213141516171819202122232425~$ ifconfigdocker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 inet6 fe80::42:46ff:fe26:ce0b prefixlen 64 scopeid 0x20&lt;link&gt; ether 02:42:46:26:ce:0b txqueuelen 0 (Ethernet) RX packets 16868344 bytes 127838098551 (127.8 GB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 17929275 bytes 137867853738 (137.8 GB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0veth340c305: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet6 fe80::50f7:7ff:fe8f:6e72 prefixlen 64 scopeid 0x20&lt;link&gt; ether 52:f7:07:8f:6e:72 txqueuelen 0 (Ethernet) RX packets 8093606 bytes 126893792744 (126.8 GB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 8795102 bytes 10834735399 (10.8 GB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0veth6c803b7: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet6 fe80::1045:4cff:fe66:7f5a prefixlen 64 scopeid 0x20&lt;link&gt; ether 12:45:4c:66:7f:5a txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 140 bytes 9832 (9.8 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 通过brctl show可查看网络接口的挂载情况， 1234~$ brctl showbridge name bridge id STP enabled interfacesdocker0 8000.02424626ce0b no veth340c305 veth6c803b7 由上可看出网络接口veth340c305，veth6c803b7都挂在虚拟网桥docker0上。 2. 容器与外部的互联我们前面的许多容器启动命令都有添加类似 -p 8080:8080 的参数，以指定将宿主机端口映射到容器端口，从而通过访问 宿主机IP：宿主机端口 的地址来访问对应端口的容器服务。端口映射的完整格式为 宿主机IP：宿主机端口：容器端口，其中前两个是可以两者都取，或只取其一 宿主机IP：宿主机端口：容器端口：将指定宿主机IP的一个指定端口映射到容器端口，如192.168.40.205:8090:8080 宿主机IP::容器端口：将指定宿主机IP的一个随机端口映射到容器端口上，如果宿主机有多个IP，则可以通过这种格式指定绑定其中一个宿主机IP，随机端口范围为49000~49900 宿主机端口：容器端口：将宿主机所有网络接口IP的指定端口映射到容器端口上，8090:8080等效于0.0.0.0:8090:8080（0.0.0.0即表示所有网络接口地址） 可以使用 docker port 容器ID或名称 容器端口或docker ps命令来查看端口映射情况，如 123456~$ docker port test-dev 80800.0.0.0:32768~$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES696a76944e72 cnbots:dev \"/bin/sh -c '/usr/lo…\" 23 minutes ago Up 23 minutes 0.0.0.0:32768-&gt;8080/tcp test-dev 在容器启动时，可以多次使用 -p 来指定映射多个端口。 如果不指定具体的宿主机端口，则可以使用 -P（大写）来分配一个宿主机的随机端口（范围为49000~49900）， 如docker run -d -P --name test-dev test:dev，然后通过docker port 容器ID或名称 容器端口或docker ps命令来查看具体映射到了哪个端口。 3. 容器之间的互联同一个Docker Daemon下的容器，彼此之间是可以通过容器IP互相访问的（如何查看容器IP？用docker inspect 容器ID或名称命令），如果要实现两个容器之间可以通过容器名直接访问，则可以通过自建一个docker网络。 1234567891011121314151617181920212223# 创建一个自定义网络，-d 表示网络类型，可以为bridge（网桥，软件交换机），或overlay（跨Docker Daemon容器间的互联）~$ docker network create -d bridge my-net0c97fc265ed1cab67d84b9376d6914c9558419c73bb5abc040e75c945cd99f0a# 启动一个centos容器centos1，通过 --network 指定自定义网络~$ docker run -it --name centos1 --network my-net centos:7.3.1611 bash[root@3dcf507bd12a /]# # 再启动一个centos容器centos2（打开另一个窗口），指定同一个自定义网络~$ docker run -it --name centos2 --network my-net centos:7.3.1611 bash[root@16dcce660a89 /]# # 在centos1容器中直接ping centos2[root@3dcf507bd12a /]# ping centos2PING centos2 (172.19.0.2) 56(84) bytes of data.64 bytes from centos2.my-net (172.19.0.2): icmp_seq=1 ttl=64 time=0.111 ms64 bytes from centos2.my-net (172.19.0.2): icmp_seq=2 ttl=64 time=0.058 ms# 在centos2容器中直接ping centos1[root@16dcce660a89 /]# ping centos1PING centos1 (172.19.0.3) 56(84) bytes of data.64 bytes from centos1.my-net (172.19.0.3): icmp_seq=1 ttl=64 time=0.061 ms64 bytes from centos1.my-net (172.19.0.3): icmp_seq=2 ttl=64 time=0.054 ms 由上可见通过自定义网桥连接的容器可以通过容器名称互相访问。如果需要多个容器之间互联，则可以使用Docker Compose。 4. 配置容器的DNS如果要自定义所有容器的DNS，则可以在 /etc/docker/daemon.json 中增加 123456&#123; \"dns\" : [ \"114.114.114.114\", \"8.8.8.8\" ]&#125; 也可以在启动容器时通过参数指定单个容器的DNS配置，--dns=IP_ADDRESS，这会将指定DNS的地址添加到容器的 /etc/resolv.conf 文件中，让容器用这个DNS服务器来解析所有不在 /etc/hosts 中的主机名。 5. Docker网络的底层实现容器的网络访问控制，主要是通过Linux上的iptables防火墙来实现与管理的。 容器访问外部网络容器访问外部网络，需要通过本地系统的转发，可以通过如下命令查看转发是否打开12345$sysctl net.ipv4.ip_forwardnet.ipv4.ip_forward = 1# 为1为打开，为0则未打开，可通过如下命令打开，也可以在Docker服务启动时通过参数--ip-forward=true打开$sysctl -w net.ipv4.ip_forward=1 容器所有到外部网络的访问，源地址都会被 NAT 成本地系统的 IP 地址。这是使用 iptables 的源地址伪装操作实现的， 1234~# iptables -t nat -nLChain POSTROUTING (policy ACCEPT)target prot opt source destination MASQUERADE all -- 172.17.0.0/16 0.0.0.0/0 上述规则将所有源地址在 172.17.0.0/16 的网段（容器IP所在网段），目标地址为任意网段（包括外部网络）的流量动态伪装为从系统网卡发出。MASQUERADE 跟传统 SNAT 的好处是它能动态从网卡获取地址。 外部访问容器 通过 -p 或 -P 指定端口映射，允许外部访问容器端口，实质也是在本地的 iptable 的 nat 表中添加相应的规则，如 12345~# iptables -t nat -nLChain DOCKER (2 references)target prot opt source destination DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:3306 to:172.17.0.2:3306DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:11090 to:172.17.0.3:11090 这里的规则映射了 0.0.0.0 ，意味着将接受主机来自所有网络接口的流量。 容器之间的访问容器之间能互相访问，需要满足两个条件：1）容器的网络拓扑是否已经互联，默认情况下容器都连接到docker0网桥上，默认是互联的。2）本地系统的防火墙iptables是否允许通过。当容器启动时通过–link互联时，也是在iptables中创建对应规则来实现。 6. 总结本文整理了Docker网络相关知识，对容器之间及容器与外部之间的通信机制应该有了一定的了解。除了默认的网络实现，Docker还提供了网络的配置及自定义网络，出于篇幅，本文介绍到这，后续再补充。我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号，欢迎关注，及时获取更新内容）———————————————————————————————————————————————————————————————","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.jboost.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.jboost.cn/tags/docker/"}]},{"title":"开发人员需要掌握的日常Linux命令集","slug":"linux-cmd","date":"2019-08-27T02:58:43.000Z","updated":"2019-08-27T10:46:50.460Z","comments":true,"path":"linux-cmd.html","link":"","permalink":"http://blog.jboost.cn/linux-cmd.html","excerpt":"不会运维的开发不是好测试。","text":"不会运维的开发不是好测试。 本文整理了开发人员日常用到的linux相关命令，供参考。 文件相关cd # 进入某个目录，不接参数进入当前用户目录（等同于cd ~），如/home/devuser，可接绝对路径或相对路径（../..表示上上级目录），也可以接 “-” 回到上次所在目录 pwd # 显示当前所在目录 ls -la # 列出当前目录所有对象，-a表示包含以.开头的隐藏文件或目录ll -h # ll 等同于 ls -l， -h表示按K M G 显示文件大小 df -h # 显示系统各盘符的空间使用情况du -h --max-depth=1 # 显示当前目录下各文件大小，–max-depth=1只列出当前目录下的文件或目录，不会列出子目录下的文件 mv test.log /home/devuser/ # 移动文件（夹） 或重命名 cp [-r] test test.bak # 复制文件，如果是文件夹则加 -r，表示复制文件夹下所有子文件夹内容rm -[r]f /home/devuser/ # 删除文件，如果删除文件夹则加 -r find / -name test.log # 在根目录下查找文件名为test.log的文件find /var/log/ -size +50M -exec rm -f {} \\; # 在/var/log/目录下查找大于50M的文件并删除，建议先将rm改为ls确认find /var/log/ -type f -atime +10 # 搜索在过去10天内未被使用过的文件find /var/log/ -type f -mtime -10 # 搜索在10天内被创建或者修改过的文件find /var/log/ -type f -atime +10|xargs rm -f # |xargs 作用与 -exec类似find ./ -name &quot;*.log&quot; -exec &#39;cat&#39; {} \\; &gt; test.log # 将当前目录下所有.log文件内容合并到一个文件test.log which java # 在系统PATH路径下查找java可执行文件whereis java # 查找二进制、源文件、man文件，从文件索引中查找，而不仅仅从PATH路径下查找 zip test.zip test.log test2.log # 创建一个zip格式的压缩包，可以接多个文件或文件夹zip -r file.zip file1 file2 dir1 # 将几个文件和目录同时压缩成一个zip格式的压缩包unzip test.zip # 解压一个zip格式压缩包 tar -zcvf test.tar.gz 要被压缩的文件名或目录 # 以gzip进行压缩 -z 按gzip，-c 压缩，-v 显示内容 -f 指定文件名tar -zxvf test.tar.gz -C 解压缩到的目录 # 解压到指定目录 -x 解压tar -ztvf test.tar.gz # 不解压，只查看内容 tar -jcvf test.tar.bz2 要被压缩的文件名或目录 # 以bzip2进行压缩tar -jxvf test.tar.bz2 -C 解压缩到的目录 # 解压到指定目录 文本相关touch test.log # 创建空文件echo -e &#39;abc\\ncba&#39;&gt; test.log # 覆盖的形式往文件写入内容 -e 解析转移字符，不然当成字符串echo &#39;aaa&#39; &gt;&gt; test.log # 追加的形式往文件写入内容 cat [-n] test.log |grep [-v] abc # 过滤文件中包含 abc 的行， 加-v表示不包含， -n表示打印行号cat test.log |grep abc|wc -l # 计算文件中包含 abc 的行数 head -n 2 test.log # 查看一个文件的前两行tail -n 2 test.log # 查看一个文件的最后两行tail -n +1000 test.log # 从1000行开始显示，显示1000行以后的cat test.log | head -n 2000 | tail -n +1000 # 显示1000行到2000行的cat test.log | tail -n +1000 | head -n 1000 # 从第1000行开始，显示1000行 more test.log # 一页一页地查看文件内容，空格键往后一页，B键往前一页，不能通过上下键控制翻滚，会一次加载整个文件less test.log # 一页一页地显示文件内容，可以通过上下键控制往前往后翻，可以向上向下搜，不需一次加载整个文件，所以速度比more快，“less is more”， less比more更强大 tail -200f test.log # 查看最后200行，根据文件描述符进行追踪，当文件改名或被删除，追踪停止tail -F test.log # 查看最后10行，只要对应文件名存在，就保持监视，即使文件被删除或改名后，如果再次创建相同的文件名，也会继续追踪 grep abc test.log # 在文件中查找关键词”abc”，类似于 cat test.log|grep abcgrep ^abc test.log # 在文件中查找以”abc”开始的词汇grep [0-9] test.log # 选择文件中所有包含数字的行grep abc -R /var/log/* # 在目录 ‘/var/log’ 及随后的目录中搜索字符串”abc” sed &#39;s/abc/ccc/g&#39; test.log # 将test.log文件中的 “abc” 替换成 “ccc”并打印，不改变原有文件sed &#39;/^$/d&#39; test.log # 从文件中删除所有空白行并打印，不改变原有文件 paste test.log test2.log # 按两列合并两个文件每行的内容并打印，test.log在左边，test2.log在右边paste -d &#39;+&#39; file1 file2 # 合并两个文件每行的内容并打印，中间用”+”拼接 sort test.log # 对文件内容进行排序，每行首字母排序sort test.log test2.log # 排序两个文件的内容sort test.log test2.log | uniq # 取出两个文件的并集(重复的行只保留一份)sort test.log test2.log | uniq -u # 删除交集，留下其他的行sort test.log test2.log | uniq -d # 取出两个文件的交集(同时存在于两个文件中的行) # comm 类似于集合的差集运算，需要两个文件都是排序的comm -1 test.log test2.log # 比较两个文件的内容只删除test.log所包含的内容comm -2 test.log test2.log # 比较两个文件的内容只删除test2.log所包含的内容comm -3 test.log test2.log # 比较两个文件的内容删除两个文件共有的内容 权限相关chmod +x test.sh # 为一个文件增加可执行权限chmod ugo+rwx test.sh # 设置文件的所有者(u)、群组(g)以及其他人(o)读（r，4 ）、写(w，2)和执行(x，1)的权限，+ 改为 - 即删除权限chmod 755 test.sh # 对文件所有者，群组，其他人分别设置7（rwx=4+2+1），5（rx=4+1）,5（rx=4+1）的权限 chown [-R] 用户名:群组名 test.log #改变一个文件的所有者和群组，如果是作用于文件夹下所有文件或目录，则加 -Rchgrp 群组名 test.log # 改变文件的群组 进程相关top # 实时显示系统中各个进程的资源占用状况top -H -p 进程号 # 列出进程的所有线程，按1键根据CPU占有率排序ps -ef|grep 进程名称 # 查看某个进程，一般用户找进程IDkill -9 进程ID # 停止某个进程jps # 查看所有java进程 网络相关ifconfig # 查看系统各网卡信息（IP，mac地址，子网掩码等）ss -s # 查看当前系统tcp、udp连接数 netstat -ano|grep 端口号 # 查看某个端口是否起来lsof -i:端口号 # 查看某个端口对应的进程信息，lsof可能需要额外安装 （sudo yum install lsof） ssh devuser@192.168.40.206 # 远程连接另一台linux主机 curl http://www.baidu.com # get方式请求某个地址curl -i -X POST -H &quot;Content-type:application/json&quot; -d &#39;{&quot;a&quot;:&quot;x&quot;,&quot;b&quot;:[&quot;y&quot;]}&#39; http://xxx # POST方式请求某个接口 wget http://xxx.zip # 下载文件 scp test.log devuser@192.168.40.206:/home/devuser/# 传输文件到另一台主机的目录下，如果是文件夹则加 -r# nc 传输，可用于文件传输（scp需要密码，nc不需要密码），需要安装 sudo yum install ncnc -l 1234 &gt; test.log # 接收方，监听1234端口，将接收内容存于test.lognc 192.168.40.205 1234 &lt; test.log # 发送方，向接收方(ip为192.168.40.205)发送test.log的内容 系统相关top # 查看CPU、内存使用情况，即各进程使用情况free -g # 查看内存使用情况date # 查看系统当前时间uptime # 查看当前CPU使用负载情况，及系统已运行时间，相当于top的第一行su # 切换到root用户su devuser # 切换到devuser用户欢迎关注我的微信公众号：jboost-ksxy （一个不只有实战干货的技术公众号，及时获取更新内容）———————————————————————————————————————————————————————————————","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.jboost.cn/categories/DevOps/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://blog.jboost.cn/tags/linux/"}]},{"title":"k8s云集群混搭模式，可能帮你节省50%的服务成本","slug":"mix-eci","date":"2019-08-21T07:32:30.000Z","updated":"2019-08-22T03:49:19.484Z","comments":true,"path":"mix-eci.html","link":"","permalink":"http://blog.jboost.cn/mix-eci.html","excerpt":"","text":"现在大部分中小企业或团队都是使用云平台来部署自己的服务，如阿里云，亚马逊云等。一般来说，业务的负载都具备一定的规律，比如每天集中在某几个小时，或呈现时间段周期性波峰、波谷交替的现象，如下图 如果使用ECS来部署服务，则可能大部分时间ECS的资源没有得到充分利用，造成成本浪费，尤其对于像GPU之类成本较高的资源就更加了。这个时候，我们可以考虑使用云集群的混搭模式来节约成本。 业务场景假设有一个这样的业务场景，包括如下特点及要求： 整个系统包括业务服务与两层视觉服务 各层服务之间调用需做负载均衡 每天的业务量主要集中在上午几个小时 平时业务量较低时仍要保证服务可用 尽可能降低成本，尤其是GPU服务器成本（GPU贵啊） k8s云集群混搭模式现在各大云平台都已经提供容器云服务，如阿里云有基于ECI（弹性容器实例）的Serverless Kubernetes集群服务，基于ECS节点不需要提供master的Kubernetes托管版集群服务，及自己提供master的Kubernetes专有版集群服务等。为了迎合类似上述业务场景的需求，也提供了Kubernetes + virtual node（虚拟节点）的混合集群服务，如下图所示 其中的虚拟节点基于ECI支持多种功能，如GPU容器实例、大规格容器实例等，增强了Kubernetes集群的弹性，使集群不局限于ECS节点的资源，做到弹性无限扩容。 部署方案结合前面的业务场景，我们可以采用k8s的混合集群服务来部署我们的项目，如下图 实现步骤： 创建Kubernetes托管版集群 加入已有ECS节点 添加一个虚拟节点，通过添加应用 ack-virtual-node 来实现 分别创建无状态的业务Deployment、AI-1 Deployment、AI-2 Deployment（对应三层服务） 分别在业务Deployment上创建公网SLB，AI-1 Deployment、AI-2 Deployment上创建内网SLB 分别在各Deployment上根据CPU或内存使用阈值配置弹性水平伸缩HPA 根据需要可以在某个或某些Deployment上配置定时伸缩，通过添加应用 ack-kubernetes-cronhpa-controller 来实现 因为水平伸缩一般需要一定时间，延迟可能会对业务造成影响，所以在业务负载比较规律的时候，可以通过定时伸缩（就是定时扩展到多少个容器，再定时收缩到多少个容器）来改善；目前定时伸缩配置的查看与更新只能通过kubectl命令行进行。 总结按照官方文档的计费方式，一个普通的2核8G的ECS一年大概费用是2600左右，如果通过容器服务的方式（按秒计费），假设每天起8小时，则一年大概费用1550左右，如果业务负载再集中到几个小时，费用会更低，对于比较稀缺又昂贵的GPU服务就更加了。但是如果服务全部按容器24小时租赁，其成本就又比ECS贵了（一年约4600），所以在平时业务负载较低的时候，可以将容器调度到ECS上保障服务的提供，业务负载高时，通过HPA或cronHPA的方式动态伸缩到虚拟节点上。对于业务负载具有一定规律的服务来说，采用这种混搭的部署方式将极大地降低你的云服务成本。不过目前k8s云集群服务应该推出时间不久，产品的易用性还比较低，对不具备一定容器与编排基础的人使用门槛相对较高。 欢迎关注我的微信公众号：jboost-ksxy （一个不只有实战干货的技术公众号，及时获取更新内容） ——————————————————————————————————————————————————————————————— ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.jboost.cn/categories/DevOps/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.jboost.cn/tags/docker/"},{"name":"k8s","slug":"k8s","permalink":"http://blog.jboost.cn/tags/k8s/"}]},{"title":"小技巧：如何自定义logback日志文件的名称","slug":"trick-logback-prop","date":"2019-08-20T09:42:28.000Z","updated":"2019-08-20T11:04:34.498Z","comments":true,"path":"trick-logback-prop.html","link":"","permalink":"http://blog.jboost.cn/trick-logback-prop.html","excerpt":"在logback.xml中获取自定义变量值。","text":"在logback.xml中获取自定义变量值。 我们可以通过在logback.xml中配置appender来指定日志输出格式及输出文件路径，这在一台主机或一个文件系统上部署单个实例没有问题，但是如果部署多个实例（比如通过容器的方式），多个实例同时往同一文件写日志可能就会引起问题。这时可以将每个实例的日志文件加以区分，如IP或UUID，或两者结合的形式。 可以有4种方式来实现logback.xml中获取自定义变量值： 通过设置环境变量或传递系统属性（比如在程序启动时通过-D传递）的方式，两者是可以直接在logback.xml中通过 ${变量名} 获取的。 自定义logback.xml的加载时机，在其加载前将需要设置的属性注入到logback的context中，这种方式相对复杂，本文不讨论。 通过实现PropertyDefiner接口来提供属性值设置 通过实现LoggerContextListener接口来设置属性值 第一种方式简单，但不能通过程序生成属性值，第二种方式稍显复杂，本文主要介绍后两种方式。 PropertyDefiner方式首先定义一个类，实现PropertyDefiner接口，可以通过继承PropertyDefinerBase会更方便 123456789101112131415161718192021222324252627282930313233343536import ch.qos.logback.core.PropertyDefinerBase;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.net.InetAddress;import java.net.UnknownHostException;import java.util.UUID;/*** * 将本地IP拼接到日志文件名中，以区分不同实例，避免存储到同一位置时的覆盖冲突问题 * @Author ronwxy * @Date 2019/8/20 16:17 */public class IPLogDefiner extends PropertyDefinerBase &#123; private static final Logger LOG = LoggerFactory.getLogger(IPLogDefiner.class); private String getUniqName() &#123; String localIp = null; try &#123; localIp = InetAddress.getLocalHost().getHostAddress(); &#125; catch (UnknownHostException e) &#123; LOG.error(\"fail to get ip...\", e); &#125; String uniqName = UUID.randomUUID().toString().replace(\"-\", \"\"); if (localIp != null) &#123; uniqName = localIp + \"-\" + uniqName; &#125; return uniqName; &#125; @Override public String getPropertyValue() &#123; return getUniqName(); &#125;&#125; 在实现方法 getPropertyValue 中返回你需要生成的值，本例中是返回 本地IP-UUID 的形式。 然后在logback.xml中，添加 &lt;define&gt; 配置，指定属性名（本例中为localIP）及获取属性值的实现类，这样就可以在配置中通过 ${localIP}来引用该属性值了。 12345678910&lt;configuration&gt; &lt;define name=\"localIP\" class=\"com.cnbot.common.IPLogDefiner\"/&gt; &lt;appender name=\"interfaceLogFile\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;File&gt;D:\\\\logs\\\\elk\\\\interface-$&#123;localIP&#125;.log&lt;/File&gt; &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt; &lt;level&gt;INFO&lt;/level&gt; &lt;/filter&gt;# 省略了其它配置 LoggerContextListener方式定义一个实现LoggerContextListener接口的类，在start方法中，将需要设置的属性设置到logback的Context中， 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import ch.qos.logback.classic.Level;import ch.qos.logback.classic.Logger;import ch.qos.logback.classic.LoggerContext;import ch.qos.logback.classic.spi.LoggerContextListener;import ch.qos.logback.core.Context;import ch.qos.logback.core.spi.ContextAwareBase;import ch.qos.logback.core.spi.LifeCycle;import java.net.InetAddress;import java.net.UnknownHostException;import java.util.UUID;/*** * 第二种实现方式 * @Author ronwxy * @Date 2019/8/20 18:45 */public class LoggerStartupListener extends ContextAwareBase implements LoggerContextListener, LifeCycle &#123; private boolean started = false; @Override public void start() &#123; if (started) &#123; return; &#125; Context context = getContext(); context.putProperty(\"localIP\", getUniqName()); started = true; &#125; private String getUniqName() &#123; String localIp = null; try &#123; localIp = InetAddress.getLocalHost().getHostAddress(); &#125; catch (UnknownHostException e) &#123; //LOG.error(\"fail to get ip...\", e); &#125; String uniqName = UUID.randomUUID().toString().replace(\"-\", \"\"); if (localIp != null) &#123; uniqName = localIp + \"-\" + uniqName; &#125; return uniqName; &#125;//省略了其它函数 然后在logback.xml中，配置如上监听器类，这样就可以通过 ${localIP} 获取到上面 context.putProperty(&quot;localIP&quot;, getUniqName()); 设置的值了。 12345678910111213&lt;configuration&gt; &lt;!--&lt;define name=\"localIP\" class=\"com.cnbot.common.IPLogDefiner\"/&gt;--&gt; &lt;contextListener class=\"com.cnbot.common.LoggerStartupListener\"/&gt; &lt;define name=\"localIP\" class=\"com.cnbot.common.IPLogDefiner\"/&gt; &lt;appender name=\"interfaceLogFile\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;File&gt;D:\\\\logs\\\\elk\\\\interface-$&#123;localIP&#125;.log&lt;/File&gt; &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt; &lt;level&gt;INFO&lt;/level&gt; &lt;/filter&gt;# 省略了其它配置 这种方式能设置任意个数的属性值，比前一种方式灵活。 总结在logback.xml中获取自定义属性值，主要是需要在加载前将对应的属性值进行设置，这样加载时才能有效获取。本文虽是自定义日志文件名称，但不局限于此，所有需要动态获取的变量都可以按这种方式实现。 欢迎关注我的微信公众号：jboost-ksxy ——————————————————————————————————————————————————————————————— ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.jboost.cn/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"}]},{"title":"Docker笔记（八）：数据管理","slug":"docker-8","date":"2019-08-12T08:50:37.000Z","updated":"2020-08-26T01:31:05.311Z","comments":true,"path":"docker-8.html","link":"","permalink":"http://blog.jboost.cn/docker-8.html","excerpt":"前面（哪个前面我也忘了）有说过，如果我们需要对数据进行持久化保存，不应使其存储在容器中，因为容器中的数据会随着容器的删除而丢失，而因通过将数据存储于宿主机文件系统的形式来持久化。在Docker容器中管理数据主要有数据卷、宿主机目录挂载两种方式","text":"前面（哪个前面我也忘了）有说过，如果我们需要对数据进行持久化保存，不应使其存储在容器中，因为容器中的数据会随着容器的删除而丢失，而因通过将数据存储于宿主机文件系统的形式来持久化。在Docker容器中管理数据主要有数据卷、宿主机目录挂载两种方式 1. 数据卷的方式数据卷是一个特殊的文件目录（或文件），具备如下特性： 可以在容器之间共享和重用 对数据卷的修改会立马生效 数据卷的更新，不会影响到镜像 数据卷默认会一直存在，不会随容器的删除而消亡 1.1 创建数据卷可以使用docker volume create 数据卷名称的命令来创建一个数据卷， 12[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker volume create volume1volume1 1.2 查看数据卷创建完后，这个数据卷具体对应宿主机哪个文件目录在上面是没法得知的，可以通过docker volume inspect 数据卷名称来查看， 123456789101112[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker volume inspect volume1[ &#123; \"CreatedAt\": \"2019-08-12T19:43:47+08:00\", \"Driver\": \"local\", \"Labels\": &#123;&#125;, \"Mountpoint\": \"/var/lib/docker/volumes/volume1/_data\", \"Name\": \"volume1\", \"Options\": &#123;&#125;, \"Scope\": \"local\" &#125;] 可以看到数据卷volume1对应的文件目录是“/var/lib/docker/volumes/volume1/_data”。 docker inspect xxx这个命令挺有用的，不论是查看镜像相关信息（docker image inspect 镜像名/镜像ID），还是查看容器相关信息（docker container inspect 容器名/容器ID），都可以使用，其中的image,container,volume是可以省略的，只要xxx部分不冲突就行。 可以通过docker volume ls 命令来查看所有数据卷， 123[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker volume lsDRIVER VOLUME NAMElocal volume1 1.3 使用数据卷可以在启动容器时通过 -v 或 –mount 的方式将一个数据卷挂载到容器的某个目录 12[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker run -dit --name ubuntu1 -v volume1:/vol1 ubuntu:18.04b060e793d44de2ca871da257b47598334658952943a13d1c478df5c3ae91a01c 按照 -v 数据卷名:容器目录 的格式，也可以使用 –mount 按照 --mount source=数据卷名,target=容器目录 的格式，如我们再启动一个挂载相同数据卷的容器 ubuntu2， 12[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker run -dit --name ubuntu2 --mount source=volume1,target=/vol2 ubuntu:18.04b30971f8a4bbadee10774fce0b4568b5b7b1c9cde36f4bf84ac911a4cdaf6c8d 可以在数据卷所在目录中创建一个文件来看看效果，先创建文件 hello.txt 1234[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# cd /var/lib/docker/volumes/volume1/_data[root@iZwz9dbodbaqxj1gxhpnjxZ _data]# touch hello.txt[root@iZwz9dbodbaqxj1gxhpnjxZ _data]# lshello.txt 然后通过docker exec来查看容器ubuntu1目录/vol1，及容器ubuntu2目录/vol2的内容 1234[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker exec -it ubuntu1 ls /vol1hello.txt[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker exec -it ubuntu2 ls /vol2hello.txt 可以看到通过挂载目录 /vol1， /vol2 都可以访问到数据卷volume1对应目录下的内容。这就像linux的软链接一样，将容器目录链接到了数据卷目录。并且上述示例也说明，同一个数据卷是可以在被多个容器共享的。 数据卷的共享也可以通过 volumes-from 容器名称/容器ID 参数来实现，如 1234[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker run -dit --name ubuntu3 --volumes-from ubuntu2 ubuntu:18.04bb5c6d61a1e6eeb18ba8c889e471b2f3215f97efca79b311eeca5968b2700df8[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker exec -it ubuntu3 ls /vol2hello.txt 通过--volumes-from ubuntu2来直接使用ubuntu2挂载的容器配置。 1.4 删除数据卷数据卷不会随着容器的删除而自动删除。如果一个数据卷还被某个容器使用，则不能删除；如果一个数据卷只被一个容器使用，则可在删除容器时通过指定 -v 参数同时删除其挂载的数据卷； 12[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker rm -v ubuntu3ubuntu3 可以通过 docker volume rm 数据卷名称 来删除某个数据卷；可以通过 docker volume prune 清理掉所有未被任何容器使用的数据卷。 2. 宿主机目录挂载方式在容器启动时，使用 -v 宿主机目录:容器目录 或 --mount type=bind,source=宿主机目录,target=容器目录的参数格式指定将宿主机目录挂载到容器目录上。宿主机目录必须是绝对路径。两者之间的区别是 -v 如果在宿主机目录不存在时会自动创建目录，而--mount不会。如， 12345678[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker run -dit --name ubuntu1 -v /root/v1:/vol1 ubuntu:18.0425c91911709eebc9290b47b483666f7b7be840df947117f7cad323583905b9f1[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker run -dit --name ubuntu2 --mount type=bind,source=/root/v2,target=/vol1 ubuntu:18.04docker: Error response from daemon: invalid mount config for type \"bind\": bind source path does not exist: /root/v2.See 'docker run --help'.[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# mkdir /root/v2[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker run -dit --name ubuntu2 --mount type=bind,source=/root/v2,target=/vol1 ubuntu:18.045a57285e9261d048dc71cf0476055a290f80538afff2cefd2a24f8b4468b5171 /root/v1,/root/v2都没有事先创建，用 -v 不会报错，会自动创建； --mount则会报错，目录必须先存在。 docker不仅支持目录的挂载，也支持文件的挂载，如， 1234[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker run --rm -it -v $HOME/.bash_history:/root/.bash_history ubuntu:18.04 bash root@3ae4ed4e687d:/# history 1 ll webapps/ 2 ll confluence/images/ 通过将宿主机当前用户的历史操作文件挂载到容器的root用户下的历史操作文件，可在容器中通过history命令查看到宿主机的操作历史。 可通过 docker inspect来查看容器的挂载情况 12345678910111213[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker inspect ubuntu1--省略了其它信息--\"Mounts\": [ &#123; \"Type\": \"bind\", \"Source\": \"/root/v1\", \"Destination\": \"/vol1\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\" &#125; ],--省略了其它信息-- 可在“Mounts”部分看到挂载信息。 3. 只读控制有时候，为了数据安全，我们不允许容器对挂载目录的内容进行修改，即对容器来说，挂载目录是只读的，这可以通过在挂载参数后面加限制实现。 12345[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker run -dit --name ubuntu3 -v /root/v1:/vol1:ro ubuntu:18.0425eca348ed307afcbef92bc03f0a1304b31b52e6db1fa07772b5dbd1040ff7b6[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker exec -it ubuntu3 bashroot@25eca348ed30:/# touch /vol1/hello.txttouch: cannot touch '/vol1/hello.txt': Read-only file system -v是在后面加ro（read-only），--mount则是形如--mount type=bind,source=宿主机目录,target=容器目录,read only的格式，可自行试验。加了read only的挂载我们再通过docker inspect命令查看，可看到两者之间的差异 —— Mode与RW的值。 12345678910\"Mounts\": [ &#123; \"Type\": \"bind\", \"Source\": \"/root/v1\", \"Destination\": \"/vol1\", \"Mode\": \"ro\", \"RW\": false, \"Propagation\": \"rprivate\" &#125; ], 4. 总结如果要对数据进行持久化管理或在容器之间共享数据，则需要将数据通过数据卷或宿主机目录（或文件）挂载的方式来将数据存储于宿主机上，使得数据的生命周期独立于容器的生命周期。这类似于我们不要把重要文件放在系统盘，而应放在其它数据盘一样，因为系统盘会由于重装系统或系统故障导致文件丢失。本文对Docker的数据管理进行了整理，后续对Docker的网络配置管理部分进行整理，欢迎持续关注。 我的微信公众号：jboost-ksxy （一个不只有实战干货的技术公众号，欢迎关注，及时获取更新内容） ——————————————————————————————————————————————————————————————— ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.jboost.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.jboost.cn/tags/docker/"}]},{"title":"就业，该去小公司还是大公司？","slug":"company-choice","date":"2019-08-09T05:29:58.000Z","updated":"2019-08-12T04:00:24.375Z","comments":true,"path":"company-choice.html","link":"","permalink":"http://blog.jboost.cn/company-choice.html","excerpt":"前几天跟一朋友交流，他说一个表弟明年就要毕业了，马上面临找工作，是去一线城市找大公司的工作好，还是留在二线城市中小公司发展好。我说，去大公司好。为什么会有这个结论，这篇文章结合自己的经历说说我的一些感受。","text":"前几天跟一朋友交流，他说一个表弟明年就要毕业了，马上面临找工作，是去一线城市找大公司的工作好，还是留在二线城市中小公司发展好。我说，去大公司好。为什么会有这个结论，这篇文章结合自己的经历说说我的一些感受。 我的第一份工作是在一家外企，当时抱着“逃离”上海的想法去了二线城市的分公司，但是管理文化氛围跟总部几乎都是一样的，这份工作经历对我后面的工作不论是做事风格、习惯上还是思考问题的方式方法上都有很大的影响。后面陆续进入国企，民企，再进入初创公司，从公司规模上可以说各种类型的都有过体验。下面从环境因素，平台效应因素等几个角度说说自己的感受。 环境因素环境对一个人的影响还是很重要的，不论是大家熟知的“近朱者赤近墨者黑”的说法，还是令我们中国人挤破脑袋的“学区房”现象，都说明环境对一个人的成长起着至关重要的作用。 小公司与大公司的环境差异首先体现在人员的素养、水平上。大公司的准入门槛相对高一些，所以人员的素质、水平也相对要高一些，如果你周围牛人比较多的话，跟牛人待久了，你也可能慢慢就步入牛人之列了——“近朱者赤”。而小公司，尤其是初创公司，为了尽快招人干活，往往人员的素质、水平会良莠不齐，你可能很难找到一个各方面让你信服，想跟着他学的真正的“牛人”。 其次在制度、流程规范上。大公司在制度、流程、规范方面相对健全完善，不论是人事管理还是日常合作分工都比较明确，你知道什么时候应该干什么（因为都给你安排好了），处理什么事情应该找谁，都有章可循，有人可找。而在小公司，可能很多人感觉的就一个字——“乱”，人员职责、分工、权限没有明确定义，没有人引导，不知道在什么阶段应该干什么，或者怎么干，明明是个小兵，老板却恨不得你是个全才，啥事都希望你能搞定。有人把在大公司工作比喻是做一颗螺丝钉，而觉得在小公司才能锻炼综合能力，但我觉得在一定的阶段，螺丝钉似的工作才能让你在专业能力上面得到更大的提升，而小公司所谓的综合能力，往往演变的是“打杂”能力，老板为了节约成本，充分发挥（压榨）每个人的能力（价值），往往一人要分饰多角，比如做人事的既要管招聘，又要管行政，甚至还可能被拉去监督项目进度，很难让你在一个专业的领域深度成长。 再次在产品规模上，大公司产品的日活规模可能少则上百万，多则上亿，不论是在技术实现还是产品运营上，都需要较高的要求与水准，你在其中能学习的技能与套路是小公司日活几千或几万的产品规模无法比拟的。 最后在文化氛围上，一般大公司都有形成自己的企业文化，包括周围人的工作风格、习惯，都会对你产生潜移默化的影响。比如我现在的不论是写代码，还是写文字，都会反复检查好几遍的习惯就是在第一家公司工作时养成的。因为你的每一行代码你的leader可能都会仔细帮你review，找出有问题的地方让你反复修正直到合格，你的每一封邮件都会被别人（在外企很多时候还包括美国人、印度人）认真查看，所以促使你在发出前会仔细核查是否有遗漏的点，是否存在错别字或语法错误，久而久之，就养成了这种反复检查的比较严谨的做事风格。而在小公司，一般很难在短时间内形成自己的企业文化，很多事情的处理都比较粗放，缺乏对细节的把握，你很难从企业文化氛围上受益。 如果用游泳来比喻大公司与小公司的差异，我觉得大公司就像是一个掌握各项泳姿、动作标准的游泳运动员，有规范有节奏，从而游得更远；而小公司则更像一个会“狗爬式”的乡下野孩子，虽然路子野，但有效——尽管比较费力，但是能游起来，但能游多远，得看方向对不对，人能不能坚持。 平台效应因素现在有些企业招聘，都明确要求毕业院校必须是985、211，甚至有些岗位直接面向BAT。前不久看到一个案例，上海交大硕博毕业因本科不是211，而被招聘企业直接拒绝。 现实就是这样，看背景，看出身。名企工作与名校毕业一样，对后面的跳槽都会有较大的加分与优势。从小公司跳大公司难，但从大公司跳小公司就容易很多，见过许多阿里系的普通技术人员跳到中小企业做技术管理者的情况。 名企光环，除了对后面的就业与跳槽方面具备优势，在社会活动上也具备一定的优势，比如现在很多技术书籍，相当一部分出自阿里系，不是说非阿里系的人不具备这个能力，而是因为有着阿里这个名企光环，出的东西更容易被人接受与认可，尽管不一定水平有多好。 什么人适合去小公司毕竟不是每个人都能去大公司，那么什么人适合去小公司呢？我觉得可能主要包括两类，一类是自己在某个领域已经取得了较好的成长，具备了独当一面或者懂得如何带领他人来做事情的能力，这种情况一般是为了追求高薪或对某个领域或公司比较看好，有自己想法的人；另一类是目前还不具备进入大公司的资本与能力的人，人总得工作与生活，所以不得不先进入小公司成长，但这部分人除非自身公司发展特别好，否则还是应该尽力往大公司靠，努力进入大公司体验其管理模式与文化氛围，对你整个职业生涯是有很大帮助的。 选择什么样的小公司选择什么样的小公司比较好，虽然很多时候也没有太多的选择，毕竟好的小公司也是可遇不可求的事情，但如果有的话，我觉得还是尽力选择满足如下四个条件的小公司比较好。 靠谱的老板。小公司的管理文化与前途基本由老板的品质与能力决定，所以一个有能力、靠谱的老板是第一要素。 高水平的管理团队，技术、管理、营销各方面。管理团队对于创业公司来说非常重要，只有一个稳定的各方面成熟的团队，成功的几率才会大一点，你在里面能获得的成长空间也更多一些。 产品项目具备长远发展的潜力。企业经营就是做一个别人愿意花钱购买的产品，并寻找一个将产品源源不断卖不出的方式，所以产品是不是刚需，有没有人买单，能不能长久很重要。 可靠的资源与渠道。有可靠的资源与渠道，才能将产品源源不断地卖出去，企业才能保持可持续发展。 以上四点从上往下重要性依次递减，同时满足四个条件的小公司应该是极少的，是可遇不可求的事情，可以按从上往下的重要性进行选择。 另外进入小公司，可能常见的一个东西是期权，我认为期权是一个美丽的梦，如果以上四点都靠谱，没有期权也能获得很好的锻炼与成长，如果不靠谱，那么就算拿了期权大概率也是一个美丽的梦，看起来很美好，但不会成真的那种，所以面对期权（画饼），也要保持理性。 职场没有伊甸园最后，不论是大公司，还是小公司，都不存在职场的伊甸园，只有自己不断成长，进步，自己强大了，才有更多的选择空间。我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号，欢迎关注，及时获取更新内容）———————————————————————————————————————————————————————————————","categories":[{"name":"Career","slug":"Career","permalink":"http://blog.jboost.cn/categories/Career/"}],"tags":[]},{"title":"Docker笔记（七）：常用服务安装——Nginx、MySql、Redis","slug":"docker-7","date":"2019-08-07T05:29:45.000Z","updated":"2020-08-26T01:30:59.026Z","comments":true,"path":"docker-7.html","link":"","permalink":"http://blog.jboost.cn/docker-7.html","excerpt":"开发中经常需要安装一些常用的服务软件，如Nginx、MySql、Redis等，如果按照普通的安装方法，一般都相对比较繁琐 —— 要经过下载软件或源码包，编译安装，配置，启动等步骤，使用 Docker 来安装这些服务软件能极大地简化安装过程，且速度也很快。","text":"开发中经常需要安装一些常用的服务软件，如Nginx、MySql、Redis等，如果按照普通的安装方法，一般都相对比较繁琐 —— 要经过下载软件或源码包，编译安装，配置，启动等步骤，使用 Docker 来安装这些服务软件能极大地简化安装过程，且速度也很快。 本文以下操作假定你已经装好了docker，并做好了镜像配置。如果没有，请参考 Docker笔记（三）：Docker安装与配置 1. MySql 安装1.1 下载镜像1~$ docker pull mysql:5.7 1.2 创建挂载目录1~$ mkdir -p apps/mysql/conf apps/mysql/data apps/mysql/logs 如上分别创建了配置文件目录，数据存放目录，以及日志文件目录 1.3 启动容器实例1~$ docker run -d -p 3306:3306 --name mysql -v /home/devuser/apps/mysql/conf/my.cnf:/etc/mysql/conf.d/my.cnf -v /home/devuser/apps/mysql/logs:/var/log/mysql -v /home/devuser/apps/mysql/data:/var/lib/mysql --restart=always -e MYSQL_ROOT_PASSWORD=Passw0rd mysql:5.7 其中-d： 表示在后台运行-p： 宿主机端口与容器端口映射–name： 容器名称-v： 宿主机目录与容器目录映射–restart=always：除非被docker stop命令明确停止，否则一直尝试重启处于停止态的容器；如果Docker重启，也会自动启动容器-e： 设置环境变量，这里设置了mysql root用户的密码为Passw0rd 如此，MySql服务就跑起来了，很快很简单有木有。 2. Redis 安装2.1 拉取镜像1~$ docker pull redis:5.0.5 2.2 启动容器1~$ docker run -d --name redis -p 6379:6379 -v /home/devuser/apps/redis/data:/data --restart=always redis:5.0.5 redis-server --appendonly yes --requirepass \"Passw1rd\" -p， -v 与上同，不赘述redis-server –appendonly yes : 在容器启动时执行redis-server命令，并打开redis持久化配置–requirepass： 设置密码 2.3 连接12345~$ docker exec -it redis redis-cli -h 172.17.0.4 -p 6379 -a Passw1rdWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.172.17.0.4:6379&gt; keys *(empty list or set)172.17.0.4:6379&gt; 这种方式把密码暴露了，其它登录用户通过history即可看到密码，不是太安全。可改用如下方式， 123456789~$ docker exec -it redis redis-cli127.0.0.1:6379&gt;127.0.0.1:6379&gt; keys *(error) NOAUTH Authentication required.127.0.0.1:6379&gt; auth 'Passw1rd'OK127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; 3. Nginx 安装3.1 拉取镜像1~$ docker pull nginx 会拉取最新的（latest）镜像 3.2 创建目录1~$ mkdir -p apps/nginx/html apps/nginx/logs apps/nginx/conf 3.3 先不指定映射路径启动一个容器12~$ docker run -d -p 80:80 --name nginx nginx1fdcd13457a6eaacb511878e10d84ffbe48fe63fd1fb3705f58b2d4195b151d8 这里如果直接指定映射路径运行会报错， 123~$ docker run -d -p 80:80 --name nginx -v ~/apps/nginx/html:/usr/share/nginx/html -v ~/apps/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v ~/apps/nginx/logs:/var/log/nginx nginxdab56c13f9e76aad37fcf73411c78d495a6466f1c0d214949650dbae44adddf4docker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"process_linux.go:424: container init caused \\\"rootfs_linux.go:58: mounting \\\\\\\"/home/devuser/apps/nginx/conf/nginx.conf\\\\\\\" to rootfs \\\\\\\"/home/docker_image/overlay2/e40ccaf4d845a9af92487b47cbc4d505c5c776800ef8887c5b43833b10661427/merged\\\\\\\" at \\\\\\\"/home/docker_image/overlay2/e40ccaf4d845a9af92487b47cbc4d505c5c776800ef8887c5b43833b10661427/merged/etc/nginx/nginx.conf\\\\\\\" caused \\\\\\\"not a directory\\\\\\\"\\\"\": unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type. 3.4 将运行容器的配置文件复制到宿主机目录下1~$ docker cp 1fdcd13457a6:/etc/nginx/nginx.conf ~/apps/nginx/conf/ 3.5 删除容器并重新运行123~$ docker stop 1fdcd1345~$ docker rm 1fdcd1345~$ docker run -d -p 80:80 --name nginx -v ~/apps/nginx/html:/usr/share/nginx/html -v ~/apps/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v ~/apps/nginx/logs:/var/log/nginx nginx 3.6 更新配置后重新加载1~$ docker kill -s HUP nginx 类似于 nginx -s reload 4. 总结本文没有总结。我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号，欢迎关注，及时获取更新内容）———————————————————————————————————————————————————————————————","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.jboost.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.jboost.cn/tags/docker/"}]},{"title":"Spring Boot（十一）：注解结合JWT实现简单的接口鉴权","slug":"springboot-simpleauth","date":"2019-08-01T12:45:59.000Z","updated":"2019-08-12T04:00:24.375Z","comments":true,"path":"springboot-simpleauth.html","link":"","permalink":"http://blog.jboost.cn/springboot-simpleauth.html","excerpt":"一般服务的安全包括认证（Authentication）与授权（Authorization）两部分，认证即证明一个用户是合法的用户，比如通过用户名密码的形式，授权则是控制某个用户可以访问哪些资源。比较成熟的框架有Shiro、Spring Security，如果要实现第三方授权模式，则可采用OAuth2。但如果是一些简单的应用，比如一个只需要鉴别用户是否登录的APP，则可以简单地通过注解+拦截器的方式来实现。本文介绍了具体实现过程，虽基于Spring Boot实现，但稍作修改（主要是拦截器配置）就可以引入其它Spring MVC的项目。","text":"一般服务的安全包括认证（Authentication）与授权（Authorization）两部分，认证即证明一个用户是合法的用户，比如通过用户名密码的形式，授权则是控制某个用户可以访问哪些资源。比较成熟的框架有Shiro、Spring Security，如果要实现第三方授权模式，则可采用OAuth2。但如果是一些简单的应用，比如一个只需要鉴别用户是否登录的APP，则可以简单地通过注解+拦截器的方式来实现。本文介绍了具体实现过程，虽基于Spring Boot实现，但稍作修改（主要是拦截器配置）就可以引入其它Spring MVC的项目。 1. 涉及的知识点 注解：用来标记某个接口是否需要登录 拦截器：拦截所有请求，判断请求的接口是否需要登录验证（基于是否标记了注解），如果需要，验证相应的信息（token），通过则放行，否则返回错误信息 JWT： Json Web Token，一种流行的认证解决方案，它可以生成携带信息的token，但token一旦生成，其过期时间就不好更新，如果需要实现用户有操作就自动延长过期时间的场景，就相对比较麻烦。我们这里只用来生成token，过期通过redis实现 RedisTemplate： 将token存在redis中，通过redis的过期机制来控制token的有效期 ThreadLocal：可以将一次请求中多个环节需要访问的变量通过ThreadLocal来传递，比如userId 2. 依赖配置在pom.xml中添加JWT与redis依赖 123456789&lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt&lt;/artifactId&gt; &lt;version&gt;$&#123;jwt.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 在application.yml配置文件中添加redis相关配置属性 12345678910111213spring: redis: host: localhost port: 6379 database: 0 password: 123654 timeout: 3000 jedis: pool: min-idle: 2 max-idle: 8 max-active: 8 max-wait: 1000 3. 定义注解注解的定义你可以根据项目的具体场景，比如需要登录的接口比较多，就可以定义如 @SkipAuth 的注解来标记不需要登录的接口，反之，则可以定义如 @NeedAuth 的注解来标记需要登录的接口，总之就是让标记接口这个操作尽可能少。但也可以基于另一种考虑，万一需要登录的接口忘了加不就存在安全问题吗，所以用 @SkipAuth 相对要保险点。 1234@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface SkipAuth &#123;&#125; 4. 定义token管理器123456789101112131415161718192021222324252627282930313233343536373839@Componentpublic class RedisTokenManager &#123; @Autowired private StringRedisTemplate redisTemplate; /** * 生成TOKEN */ public String createToken(String userId) &#123; //使用uuid作为源token String token = Jwts.builder().setId(userId).setIssuedAt(new Date()) .signWith(SignatureAlgorithm.HS256, JwtConstant.JWT_SECRET).compact(); //存储到redis并设置过期时间 redisTemplate.boundValueOps(JwtConstant.AUTHORIZATION + \":\" + userId) .set(token, JwtConstant.TOKEN_EXPIRES_HOUR, TimeUnit.HOURS); return token; &#125; public boolean checkToken(TokenModel model) &#123; if (model == null) &#123; return false; &#125; String token = redisTemplate.boundValueOps(JwtConstant.AUTHORIZATION + \":\" + model.getUserId()).get(); if (token == null || !token.equals(model.getToken())) &#123; return false; &#125; //如果验证成功，说明此用户进行了一次有效操作，延长token的过期时间 redisTemplate.boundValueOps(model.getUserId()) .expire(JwtConstant.TOKEN_EXPIRES_HOUR, TimeUnit.HOURS); return true; &#125; public void deleteToken(String userId) &#123; redisTemplate.delete(userId); &#125;&#125; 在登录接口通过时，调用 createToken 创建token，并保存到redis中，设置过期时间， 在调用未被 @SkipAuth 注解标记的接口时，调用 checkToken 来验证，并更新token的过期时间， 退出登录时，删除token。 5. 定义拦截器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Component@Slf4jpublic class AuthInterceptor extends HandlerInterceptorAdapter &#123; @Autowired private RedisTokenManager tokenManager; public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; String requestPath = request.getRequestURI().substring(request.getContextPath().length()); // 如果不是映射到方法直接通过 if (!(handler instanceof HandlerMethod)) &#123; return true; &#125; HandlerMethod handlerMethod = (HandlerMethod) handler; Method method = handlerMethod.getMethod(); // 如果方法注明了 SkipAuth，则不需要登录token验证 if (method.getAnnotation(SkipAuth.class) != null) &#123; return true; &#125; // 从header中得到token String authorization = request.getHeader(JwtConstant.AUTHORIZATION); // 验证token if(StringUtils.isBlank(authorization))&#123; WebUtil.outputJsonString(ApiResponse.failed(\"未提供有效Token！\"), response); return false; &#125; try &#123; Claims claims = Jwts.parser().setSigningKey(JwtConstant.JWT_SECRET) .parseClaimsJws(authorization).getBody(); String userId = claims.getId(); TokenModel model = new TokenModel(userId, authorization); if (tokenManager.checkToken(model)) &#123; // 通过ThreadLocal设置下游需要访问的值 AuthUtil.setUserId(model.getUserId()); return true; &#125; else &#123; log.info(\"连接\" + requestPath + \"拒绝\"); WebUtil.outputJsonString(ApiResponse.failed(\"未提供有效Token！\"), response); return false; &#125; &#125; catch (Exception e) &#123; log.error(\"连接\" + requestPath + \"发生错误:\", e); WebUtil.outputJsonString(ApiResponse.failed(\"校验Token发生异常！\"), response); return false; &#125; &#125; @Override public void afterCompletion( HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) &#123; //结束后清除，否则由于线程池复用，导致ThreadLocal的值被其他用户获取 AuthUtil.clear(); &#125;&#125; 拦截器通过对请求方法是否标记注解 @SkipAuth 来判断是否需要进行token验证，如果验证通过，则从JWT token中解析出userId，通过AuthUtil工具方法保存到ThreadLocal中，供下游访问。在请求处理结束调用 afterCompletion 方法中，要清除掉ThreadLocal中的值，否则由于线程池的复用，导致被其他用户获取。 然后，注册拦截器 1234567891011121314151617181920@Configurationpublic class WebConfiguration implements WebMvcConfigurer &#123; private AuthInterceptor authInterceptor; @Autowired public void setAuthInterceptor(AuthInterceptor authInterceptor)&#123; this.authInterceptor = authInterceptor; &#125; /** * 注册鉴权拦截器 * @param * @return */ public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(authInterceptor) .addPathPatterns(\"/**\") .excludePathPatterns(\"/error\"); &#125;&#125; 这里将 /error 这个接口排除了，因为如果接口处理过程中出现异常，则spring boot会自动跳转到 /error 接口，又会进入拦截器校验（因为/error接口没有标注 @SkipAuth 注解）。 6. 验证通过以上几步，一个简单的接口认证功能就实现了，我们可以通过添加一个登录接口，两个测试接口（一个需要认证，一个不需要认证）来验证下。登录接口 1234567891011@SkipAuth@RequestMapping(\"/login\")public ApiResponse login(@RequestBody Map&lt;String, Object&gt; params) &#123; String username = MapUtils.getString(params, \"username\"); String password = MapUtils.getString(params, \"password\"); if(\"ksxy\".equals(username) &amp;&amp; \"jboost\".equals(password))&#123; return ApiResponse.success(tokenManager.createToken(username)); &#125; else &#123; return ApiResponse.failed(\"用户名或密码错误\"); &#125;&#125; 登录成功后，通过createToken方法创建了JWT token。测试接口 12345678910@SkipAuth@RequestMapping(\"/skip-auth\")public ApiResponse skipAuth() &#123; return ApiResponse.success(\"不需要认证的接口调用\");&#125;@RequestMapping(\"/need-auth\")public ApiResponse needAuth() &#123; return ApiResponse.success(\"username: \" + AuthUtil.getUserId());&#125; 7. 总结本文介绍了一个简单的接口认证方案，适用于不需要基于用户角色进行授权的场景。如果有较复杂的授权需求，则还是基于Shiro， Spring Security， OAuth2等框架来实现。这里也可以不用JWT，但是需要自己去做一些处理，比如将userId以某种形式包含在token中，解析时取出。本文完整实例代码：https://github.com/ronwxy/springboot-demos/tree/master/springboot-simpleauth","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.jboost.cn/categories/SpringBoot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"}]},{"title":"老被跨域问题烦？看看都有哪些处理方法","slug":"cors","date":"2019-07-30T04:56:24.000Z","updated":"2020-08-26T01:40:22.649Z","comments":true,"path":"cors.html","link":"","permalink":"http://blog.jboost.cn/cors.html","excerpt":"前面写的《IT技术人员的自我修养》，没想到几天内收到了不少良好的反馈，在此也感谢大家的关注。往后会不定时分享一些技术、管理领域的工作经验总结与感悟，欢迎大家持续关注、交流。最近被问及一个跨域的问题，包括之前面试时发现很多面试者对跨域及其处理也是一知半解，故本文对该问题进行了梳理总结，以供参考。","text":"前面写的《IT技术人员的自我修养》，没想到几天内收到了不少良好的反馈，在此也感谢大家的关注。往后会不定时分享一些技术、管理领域的工作经验总结与感悟，欢迎大家持续关注、交流。最近被问及一个跨域的问题，包括之前面试时发现很多面试者对跨域及其处理也是一知半解，故本文对该问题进行了梳理总结，以供参考。 1. 什么是跨域理解什么是跨域，就要先了解一个叫“同源策略”的东西，什么是“同源策略”？这是浏览器为了网站访问安全，对来自不同源的请求做一些必要的访问限制的一种策略。那什么叫“同源”呢？我们知道，一个http请求地址一般包含四部分：协议://域名:端口/路径，所谓同源，就是前面三者，即协议、域名、端口都一样。举例说明，假如我们有一个地址 http://blog.jboost.cn/docker-1.html， 来看以下地址是否与它同源 地址 是否同源 说明 https://blog.jboost.cn/docker-1.html 不同源 协议不同，一个http，一个https http://www.jboost.cn/docker-1.html 不同源 域名不同 http://blog.jboost.cn:8080/docker-1.html 不同源 端口不同，一个是默认端口80，一个是8080 http://blog.jboost.cn/docker-2.html 同源 虽然路径不同，但协议、域名、端口（默认80）都相同 那么浏览器对不同源的请求做了哪些访问限制呢？共有三种限制 对Cookie、LocalStorage，以及IndexDB（浏览器提供的类NoSQL的一个本地数据库）的访问 对DOM的访问 AJAX请求 而跨域就是要打破这种访问限制，对不同源的资源请求也能顺利进行，最常见的就是AJAX请求，比如前后端分离架构中，两者服务域名不同，前端通过AJAX直接访问服务端接口，就会存在跨域问题。 2. 为什么会存在跨域前面说“同源策略”时已经提到，浏览器是为了网站的访问安全，才设置了跨域这道屏障。那么前面所说的三种限制，分别都是如何来保障网站安全的。 对本地存储Cookie、LocalStorage、IndexDB的访问限制我们系统的登录凭证一般是通过在Cookie中设置 SESSIONID（如针对浏览器表单请求）或直接返回 token（如针对REST请求）的形式返回给客户端的，比如Tomcat是通过在Cookie中设置名为 JSESSIONID 的属性来保存的，而一般REST请求的token前端会存储于 LocalStorage 中，如果不存在访问限制，则你访问的其它网站可能就会获取到这些凭证，然后伪造你的身份来发起非法请求，这就太不安全了。 对DOM的访问限制如果不对DOM进行访问限制，那么其它网站，尤其一些钓鱼网站，就可以通过 &lt;iframe&gt; 的形式拿到你访问网站的DOM，进而获取到你输入的一些敏感信息，比如用户名、密码… 对AJAX请求的限制同源策略规定，AJAX请求只能发给同源的网址，否则就会报错。至于为什么要限制，一方面是避免1中所提到伪造非法请求，另一方面我理解是AJAX过于灵活，如果不做限制，可能网站的接口资源就会被其它网站随意使用，就像你的私有物品被别人招呼都不打任意拿去用一样。 总之，同源策略是浏览器提供的最基本的一种安全保障机制或约定。 3. 怎么实现跨域访问我们平常遇到的跨域问题基本都出现在AJAX请求的场景，一般而言，可以通过代理、CORS、JSONP等方式来解决跨域问题。 3.1 代理既然“同源策略”是浏览器端的机制，那我们就可以绕开浏览器，最常见的做法就是使用代理，如 Nginx，比如我们前端项目的域名是 http://blog.jboost.cn，服务端接口域名是 http://api.jboost.cn，我们在 Nginx 中提供如下配置 12345678910server&#123; # 端口 listen 80; # 域名 server_name blog.jboost.cn; # 所有 http:&#x2F;&#x2F;blog.jboost.cn&#x2F;api&#x2F;xxx 请求都会被转发到 http:&#x2F;&#x2F;api.jboost.cn&#x2F;api&#x2F;xxx location ^~ &#x2F;api &#123; proxy_pass http:&#x2F;&#x2F;api.jboost.cn; &#125; &#125; 则前端通过AJAX请求服务端接口 http://api.jboost.cn/api/xxx 都可以改为通过 http://blog.jboost.cn/api/xxx 来访问，从而避免不同源的跨域问题。 3.2 CORSCORS是Cross-Origin Resource Sharing的简写，即跨域资源共享，CORS需要服务端与浏览器同时支持，目前所有浏览器（除IE10以下）都支持CORS，因此，实现CORS，主要就是服务端的工作了。例如在Spring Boot中，我们可通过如下配置注册一个CorsFilter的过滤器来实现跨域支持。 12345678910111213141516171819202122232425@Configuration@ConditionalOnClass(&#123;Servlet.class, CorsFilter.class&#125;)public class CORSAutoConfiguration &#123; @Bean @ConditionalOnMissingBean(name = \"corsFilterRegistrationBean\") public FilterRegistrationBean corsFilterRegistrationBean() &#123; UrlBasedCorsConfigurationSource corsConfigurationSource = new UrlBasedCorsConfigurationSource(); CorsConfiguration corsConfiguration = new CorsConfiguration(); corsConfiguration.applyPermitDefaultValues(); corsConfiguration.setAllowedMethods(Arrays.asList(CorsConfiguration.ALL)); corsConfiguration.addExposedHeader(HttpHeaders.DATE); corsConfigurationSource.registerCorsConfiguration(\"/**\", corsConfiguration); CorsFilter corsFilter = new CorsFilter(corsConfigurationSource); FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(); filterRegistrationBean.setFilter(corsFilter); filterRegistrationBean.setOrder(Ordered.HIGHEST_PRECEDENCE); filterRegistrationBean.addUrlPatterns(\"/*\"); return filterRegistrationBean; &#125;&#125; 其实质就是在响应消息的Header中添加几个属性，主要有 Access-Control-Allow-Origin 必需，表示允许跨域的请求源，可以是具体的域名，也可以是 * ，表示任意域名 Access-Control-Allow-Methods 必需，表示允许跨域访问的HTTP方法，如GET、POST、PUT、DELETE等，可以是 * ，表示所有 Access-Control-Allow-Headers 如果请求包括 Access-Control-Request-Headers 头信息，则必需，表示服务器支持的所有头信息字段 3.3 JSONPJSONP是利用浏览器对JS一些标签（如 &lt;script&gt;, &lt;img&gt;等）的 src 属性不具有同源策略限制的特性实现的，如前端添加 1&lt;script type=\"text/javascript\" src=\"http://api.jboost.cn/hello?name=jboost&amp;callback=jsonpCallback\"/&gt; 并且定义JS方法 jsonpCallback。服务端接口返回内容需要是JS方法jsonpCallback的调用格式，如jsonpCallback({&quot;name&quot;:&quot;jboost&quot;})，这样在jsonpCallback方法中就可以获取服务端实际返回的结果数据{&quot;name&quot;:&quot;jboost&quot;}了。JSONP方式的局限性也很明显，一是只支持GET请求——你没见过哪些&lt;script&gt;, &lt;img&gt;标签是POST请求吧，二是需要对服务端返回数据格式做处理。 4. 总结三种跨域支持的实现，代理方式最简单，对客户端、服务端都不具有侵入性，但如果需要支持的请求源比较多，或者是与第三方对接的话，代理方式就不太适用了。CORS相对来说是一种标准的处理方式，并且通过过滤器的方式对业务代码也没有任何侵入性。而JSONP方式局限性较大，只支持GET，并且需要服务端做返回数据格式的支持。可针对具体情况选择适用的方式。 ![微信公众号](/assets/card-2.png)","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.jboost.cn/categories/DevOps/"}],"tags":[{"name":"web","slug":"web","permalink":"http://blog.jboost.cn/tags/web/"}]},{"title":"Spring Boot从入门到实战（十）：异步处理","slug":"springboot-async","date":"2019-07-22T10:25:49.000Z","updated":"2019-07-24T01:52:03.885Z","comments":true,"path":"springboot-async.html","link":"","permalink":"http://blog.jboost.cn/springboot-async.html","excerpt":"在业务开发中，有时候会遇到一些非核心的附加功能，比如短信或微信模板消息通知，或者一些耗时比较久，但主流程不需要立即获得其结果反馈的操作，比如保存图片、同步数据到其它合作方等等。如果将这些操作都置于主流程中同步处理，势必会对核心流程的性能造成影响，甚至由于第三方服务的问题导致自身服务不可用。这时候就应该将这些操作异步化，以提高主流程的性能，并与第三方解耦，提高主流程的可用性。","text":"在业务开发中，有时候会遇到一些非核心的附加功能，比如短信或微信模板消息通知，或者一些耗时比较久，但主流程不需要立即获得其结果反馈的操作，比如保存图片、同步数据到其它合作方等等。如果将这些操作都置于主流程中同步处理，势必会对核心流程的性能造成影响，甚至由于第三方服务的问题导致自身服务不可用。这时候就应该将这些操作异步化，以提高主流程的性能，并与第三方解耦，提高主流程的可用性。 在Spring Boot中，或者说在Spring中，我们实现异步处理一般有以下几种方式： 1. 通过 @EnableAsync 与 @Asyc 注解结合实现2. 通过异步事件实现3. 通过消息队列实现 1. 基于注解实现我们以前在Spring中提供异步支持一般是在配置文件 applicationContext.xml 中添加类似如下配置 12&lt;task:annotation-driven executor=\"executor\" /&gt;&lt;task:executor id=\"executor\" pool-size=\"10-200\" queue-capacity=\"2000\"/&gt; Spring的 @EnableAsync 注解的功能与&lt;task:annotation-driven/&gt;类似，将其添加于一个 @Configuration 配置类上，可对Spring应用的上下文开启异步方法支持。 @Async 注解可以标注在方法或类上，表示某个方法或某个类里的所有方法需要通过异步方式来调用。 我们以一个demo来示例具体用法，demo地址：https://github.com/ronwxy/springboot-demos/tree/master/springboot-async 添加 @EnableAsync 注解 在一个 @Configuration 配置类上添加 @EnableAysnc 注解，我们一般可以添加到启动类上，如 12345678@SpringBootApplication@EnableAsyncpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 配置相关的异步执行线程池 可通过配置类的方式对异步线程池进行配置，并提供异步执行时出现异常的处理方法，如 1234567891011121314151617181920212223242526272829303132333435363738394041@Configurationpublic class AsyncConfig implements AsyncConfigurer &#123; @Value(\"$&#123;async.corePoolSize:10&#125;\") private int corePoolSize; @Value(\"$&#123;async.maxPoolSize:200&#125;\") private int maxPoolSize; @Value(\"$&#123;async.queueCapacity:2000&#125;\") private int queueCapacity; @Value(\"$&#123;async.keepAlive:5&#125;\") private int keepAlive; public Executor getAsyncExecutor() &#123; ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(corePoolSize); executor.setMaxPoolSize(maxPoolSize); executor.setQueueCapacity(queueCapacity); executor.setKeepAliveSeconds(keepAlive); executor.setThreadNamePrefix(\"async-\"); executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); executor.setDaemon(false); //以用户线程模式运行 executor.initialize(); return executor; &#125; public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() &#123; return new MyAsyncUncaughtExceptionHandler(); &#125; public static class MyAsyncUncaughtExceptionHandler implements AsyncUncaughtExceptionHandler &#123; public void handleUncaughtException(Throwable throwable, Method method, Object... objects) &#123; System.out.println(\"catch exception when invoke \" + method.getName()); throwable.printStackTrace(); &#125; &#125;&#125; 这里我们通过实现 AsyncConfigurer 接口提供了一个异步执行线程池对象，各参数的说明可以参考【线程池的基本原理，看完就懂了】，里面有很详细的介绍。且通过实现 AsyncUncaughtExceptionHandler 接口提供了一个异步执行过程中未捕获异常的处理类。 定义异步方法 异步方法的定义只需要在类（类上注解表示该类的所有方法都异步执行）或方法上添加 @Async 注解即可，如 12345678910111213@Servicepublic class AsyncService &#123; @Async public void asyncMethod()&#123; System.out.println(\"2. running in thread: \" + Thread.currentThread().getName()); &#125; @Async public void asyncMethodWithException() &#123; throw new RuntimeException(\"exception in async method\"); &#125;&#125; 测试 我们可以通过如下测试类来对异步方法进行测试 1234567891011121314151617181920212223@RunWith(SpringRunner.class)@SpringBootTestpublic class AnnotationBasedAsyncTest &#123; @Autowired private AsyncService asyncService; @Test public void testAsync() throws InterruptedException &#123; System.out.println(\"1. running in thread: \" + Thread.currentThread().getName()); asyncService.asyncMethod(); Thread.sleep(3); &#125; @Test public void testAysncWithException() throws InterruptedException &#123; System.out.println(\"1. running in thread: \" + Thread.currentThread().getName()); asyncService.asyncMethodWithException(); Thread.sleep(3); &#125;&#125; 因为异步方法在一个新的线程中执行，可能在主线程执行完后还没来得及处理，所以通过sleep来等待它执行完成。具体执行结果读者可自行尝试运行，这里就不贴图了。 2. 基于事件实现第二种方式是通过Spring框架的事件监听机制实现，但Spring的事件监听默认是同步执行的，所以实际上还是需要借助 @EnableAsync 与 @Async 来实现异步。 添加 @EnableAsync 注解 与上同，可添加到启动类上。 自定义事件类通过继承 ApplicationEvent 来自定义一个事件 1234567891011public class MyEvent extends ApplicationEvent &#123; private String arg; public MyEvent(Object source, String arg) &#123; super(source); this.arg = arg; &#125; //getter/setter&#125; 定义事件处理类支持两种形式，一是通过实现 ApplicationListener 接口，如下 123456789@Component@Asyncpublic class MyEventHandler implements ApplicationListener&lt;MyEvent&gt; &#123; public void onApplicationEvent(MyEvent event) &#123; System.out.println(\"2. running in thread: \" + Thread.currentThread().getName()); System.out.println(\"2. arg value: \" + event.getArg()); &#125;&#125; 二是通过 @EventListener 注解，如下 12345678910@Componentpublic class MyEventHandler2 &#123; @EventListener @Async public void handle(MyEvent event)&#123; System.out.println(\"3. running in thread: \" + Thread.currentThread().getName()); System.out.println(\"3. arg value: \" + event.getArg()); &#125;&#125; 注意两者都需要添加 @Async 注解，否则默认是同步方式执行。 定义事件发送类可以通过实现 ApplicationEventPublisherAware 接口来使用 ApplicationEventPublisher 的 publishEvent()方法发送事件， 12345678910111213@Componentpublic class MyEventPublisher implements ApplicationEventPublisherAware &#123; protected ApplicationEventPublisher applicationEventPublisher; public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) &#123; this.applicationEventPublisher = applicationEventPublisher; &#125; public void publishEvent(ApplicationEvent event)&#123; this.applicationEventPublisher.publishEvent(event); &#125;&#125; 测试 可以通过如下测试类来进行测试， 1234567891011121314@RunWith(SpringRunner.class)@SpringBootTestpublic class EventBasedAsyncTest &#123; @Autowired private MyEventPublisher myEventPublisher; @Test public void testAsync() throws InterruptedException &#123; System.out.println(\"1. running in thread: \" + Thread.currentThread().getName()); myEventPublisher.publishEvent(new MyEvent(this,\"testing event based async\")); Thread.sleep(3); &#125;&#125; 运行后发现两个事件处理类都执行了，因为两者都监听了同一个事件 MyEvent 。 3. 基于消息队列实现以上两种方式都是基于服务器本机运行，如果服务进程出现异常退出，可能导致异步执行中断。如果需要保证任务执行的可靠性，可以借助消息队列的持久化与重试机制。阿里云上的消息队列服务提供了几种类型的消息支持，如顺序消息、定时/延时消息、事务消息等（详情可参考：https://help.aliyun.com/document_detail/29532.html?spm=5176.234368.1278132.btn4.6f43db25Rn8oey ），如果项目是基于阿里云部署的，可以考虑使用其中一类消息服务来实现业务需求。 4. 总结本文对spring boot下异步处理的几种方法进行了介绍，如果对任务执行的可靠性要求不高，则推荐使用第一种方式，如果可靠性要求较高，则推荐使用自建消息队列或云消息队列服务的方式。本文demo源码地址：https://github.com/ronwxy/springboot-demos/tree/master/springboot-async/src/main/java/cn/jboost/async我的个人博客地址：http://blog.jboost.cn我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号，欢迎关注，及时获取更新内容）———————————————————————————————————————————————————————————————","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.jboost.cn/categories/SpringBoot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"}]},{"title":"Docker笔记（六）：容器管理","slug":"docker-6","date":"2019-07-21T03:04:16.000Z","updated":"2020-08-26T01:30:55.241Z","comments":true,"path":"docker-6.html","link":"","permalink":"http://blog.jboost.cn/docker-6.html","excerpt":"容器是Docker中的另一核心概念，在Docker中，应用的运行都是在容器内进行的，容器则基于镜像创建。前面已对Docker镜像做了基本介绍，本文对Docker容器管理的相关内容做一个梳理。","text":"容器是Docker中的另一核心概念，在Docker中，应用的运行都是在容器内进行的，容器则基于镜像创建。前面已对Docker镜像做了基本介绍，本文对Docker容器管理的相关内容做一个梳理。 1. 启动容器启动容器的命令格式如下 1docker run [OPTIONS] IMAGE-NAME [COMMAND] [ARG...] 其中OPTIONS部分可指定容器运行的一些可选项，常用选项包括： -d 将容器以后台进程（daemon）的形式运行 -p 指定容器内应用暴露端口与主机端口的映射，如 -p 8080:80 表示将容器内80端口映射到主机的8080端口（主机端口在前，容器端口在后） -v 指定容器与主机的挂载目录映射，如 -v /var/log:/log 表示将容器的/log目录挂载到主机的/var/log目录（同样主机目录在前，容器目录在后），后续对容器的/log写操作实际作用于主机的/var/log目录 -e 为容器设置环境变量 -t 为容器启动一个伪终端（pseudo-tty） -i 让容器的标准输入保持打开，一般与 -t 配合使用，让容器启动后就打开一个可交互的命令行界面 -w 指定容器的工作目录 COMMAND [ARG..] 部分就是容器需要运行的应用进程启动命令与参数，如果镜像中有通过 CMD， 或 ENTRYPOINT 指定了容器启动程序，则可省略。另外可通过 –name 指定容器的名称，以及 –restart 来指定重启策略，–restart有三种取值，代表容器支持的三种不同的重启策略 取值 描述 always 除非被docker stop命令明确停止，否则一直尝试重启处于停止态的容器；如果Docker重启，也会自动启动容器 unless-stopped 与always的区别是，停止态的容器不在Docker重启的时候被重启 on-failed 在容器退出时返回值不为0的时候，重启容器；如果Docker重启，容器也会被启动，不管之前是否处于停止状态 以启动一个mysql数据库服务为例 12345docker run -d -p 3306:3306 --name mysql \\ -v /home/devuser/apps/mysql/conf/my.cnf:/etc/mysql/conf.d/my.cnf \\ -v /home/devuser/apps/mysql/logs:/var/log/mysql \\ -v /home/devuser/apps/mysql/data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=Passw0rd --restart=always mysql:5.7 上述命令启动了一个mysql容器服务，-d 表示以后台进程运行，执行命令后只返回一个容器ID，不会输出任何其它信息；-p 将容器暴露的端口3306映射到宿主机的3306端口，外部主机就可以通过宿主机IP与3306端口来访问mysql服务； –name 指定了容器名称为mysql； -v 将mysql的配置文件路径、日志路径、数据存储路径映射到了宿主机对应的路径目录；-e 设置了一个环节变量指定mysql root账号的密码；–restart 指定容器在异常退出时，包括Docker重启时，自动启动容器。 我们前面有提过，当我们执行CLI命令时，实际上是客户端（Docker Client）通过发送请求到Docker后台进程（Docker Daemon），由Docker后台进程来执行的，那么当我们执行上述docker run命令的时候，Docker后台进程具体都干了些啥呢？一般来说，包括如下几个操作步骤 检测本地是否存在指定的镜像，如果不存在，就从公共仓库下载 利用镜像创建一个容器，并启动它 分配一个文件系统，并在只读的镜像层上面挂载一层可读写层（容器存储层） 从宿主机配置的网桥接口中桥接一个虚拟接口到容器中去 从地址池配置一个 ip 地址给容器 执行用户指定的应用程序 执行应用程序完毕后容器被终止 2. 管理已有容器一般对已有容器的管理包括如下几个操作： 查看运行中的容器 docker ps 或 docker container ls 查看所有容器 docker ps -a 或 docker container ls -a 停止运行 docker stop xxx 开始停止状态的容器 docker start xxx 重启运行状态的容器 docker restart xxx 删除停止状态的容器 docker rm xxx 强制删除容器（包括运行状态中） docker rm -f xxx 删除所有停止状态的容器 docker container prune 其中xxx既可以是容器ID（短ID即可，只要与其它区分开来），也可以是容器名称。docker rm之前必须要先docker stop将容器置为停止状态，而docker rm -f可以强制删除运行状态的容器，其背后是通过Linux/POSIX信号来实现的，docker rm -f命令直接发出SIGKILL信号，不会给容器内运行进程任何缓冲的时间，立即终止，而docker stop命令却是先发送SIGTERM信号，通知容器进程结束，会为进程预留一个清理并优雅停止的机会，如果一段时间后进程还没有终止，那么就会发送SIGKILL信号，来终止进程的运行。 我们也可以像镜像操作中一样，组合使用命令来更方便地操作，如强制删除所有容器（慎用） 1docker rm -f $(docker ps -aq) 3. 进入容器容器在运行时指定 -d 选项时， 是以后台进程的形式运行的，如果我们需要进入容器查看或操作，可以通过docker exec命令，docker exec命令的格式如下 1docker exec [OPTIONS] container-id COMMAND OPTIONS常用的一般是 -t， -i，意义跟在docker run选项中一样 —— 为容器启动一个伪终端（pseudo-tty），并保持标准输入打开，从而可以像Linux命令行一样进行交互， COMMAND一般为 bash。 另外还有一个命令是docker attach xxx，其中xxx是容器ID，但推荐使用docker exec，因为docker attach中当执行exit退出容器时，容器也会随之终止，但docker exec则不会。 如果不进入容器，也可以通过docker logs xxx，xxx是容器ID，来查看容器的输出信息。 4. 导入导出容器可以使用docker export命令将一个容器的快照进行导出，如 1docker export xxx &gt; mycontainer.tar 其中xxx是容器ID，可以通过docker ps -a查看，上述命令将容器的当前快照导出到了本地文件。 docker import命令则可以将一个容器快照文件导入为镜像，如 1cat mycontainer.tar | docker import - test/myimage:v1.0 可以通过URL来导入，如 1docker import http://test.com/testimage.tgz test/myimage2:v1.0 由此可见，我们获取镜像又多了一个来源——从已有容器快照文件导入。 5. 总结本文对容器的一些基本操作进行了介绍，需要注意的是如之前所说，容器应以无状态的形式运行，所有产生的数据应该通过挂载数据卷的方式写入宿主机文件目录，避免容器销毁时造成数据丢失；尽量使用docker stop + docker rm的方式来替代docker rm -f，使容器内运行程序“优雅”地退出。有时候可能遇到这样的场景，容器创建运行后，我们需要对运行的一些参数进行更新或添加，这时候该怎么操作。后文会对该场景进行介绍，欢迎关注。 我的个人博客地址：http://blog.jboost.cn 我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号，欢迎关注，及时获取更新内容） ——————————————————————————————————————————————————————————————— ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.jboost.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.jboost.cn/tags/docker/"}]},{"title":"Docker笔记（五）：整一个自己的镜像","slug":"docker-5","date":"2019-07-17T08:32:35.000Z","updated":"2020-08-26T01:30:52.018Z","comments":true,"path":"docker-5.html","link":"","permalink":"http://blog.jboost.cn/docker-5.html","excerpt":"获取镜像的途径有两个，一是从镜像仓库获取，如官方的Docker Hub，二是自定义。上文已经介绍如何从镜像仓库获取镜像，本文基于一个Springboot项目，来介绍自定义一个镜像的基本流程。","text":"获取镜像的途径有两个，一是从镜像仓库获取，如官方的Docker Hub，二是自定义。上文已经介绍如何从镜像仓库获取镜像，本文基于一个Springboot项目，来介绍自定义一个镜像的基本流程。 1. 定制镜像的本质我们知道镜像是分层存储的，镜像的构建也是一层一层进行的，一层构建完后，就变为只读，在其上再构建下一层。因此定制镜像，实际上就是定义每一层要干的事，比如执行某个命令，设置一个环境变量，声明一个暴露端口等等。然后在构建时，按照各层的定义，一层一层地完成构建，最终形成一个包含这些层的镜像。 2. Dockerfile文件Docker中定义各层要干的事的文件叫Dockerfile，它是一个文本文件，包含了一条条的指令，每一条指令对应一层镜像，指令的内容就描述了这一层该如何构建。如下示例了一个非常简单的Dockerfile， 12FROM nginxRUN echo &#39;&lt;h1&gt;Hello jboost!&lt;&#x2F;h1&gt;&#39; &gt; &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;index.html 我们定制镜像，必须要以某一个镜像为基础，在其上构建自己需要的层，如上示例中，我们是以nginx镜像为基础，然后在第二层定制了我们自己的内容——修改index.html的内容为&lt;h1&gt;Hello jboost!&lt;/h1&gt;，这样运行容器打开nginx主页时就不会显示默认的页面内容了。 上面示例中接触了Dockerfile的两个指令 FROM：FROM指令指定基础镜像，每一个定制镜像必须要有一个基础镜像，所以必须要有一条FROM指令，并且是Dockerfile的第一条指令 RUN：RUN指令指定需要执行的命令，后面接的命令就像是shell脚本一样可执行 Dockerfile还提供了许多其它指令，后续我们再集中介绍，本文只对接触到的指令做简单说明。 3. 自定义一个镜像这部分以一个Springboot项目为基础，介绍自定义一个镜像涉及的基本环节。项目地址为：https://github.com/ronwxy/swagger-register ，该项目是一个Swagger API文档注册服务，其它项目可将Swagger API信息注册到该服务，进行统一查看与管理。 3.1 定义Dockerfile文件首先，我们在项目的根目录下创建一个Dockerfile文件（文件名就叫Dockerfile），其内容为： 12345678FROM openjdk:8-jdk-alpineENV PROFILE&#x3D;devRUN mkdir &#x2F;app &#x2F;logsCOPY .&#x2F;target&#x2F;swagger-register-1.0.0-SNAPSHOT.jar &#x2F;app&#x2F;app.jarWORKDIR &#x2F;appVOLUME &#x2F;register-dataEXPOSE 11090CMD [&quot;java&quot;, &quot;-Dspring.profiles.active&#x3D;$&#123;PROFILE&#125;&quot;, &quot;-jar&quot;, &quot;app.jar&quot;] 从上往下依次介绍如下 第一行：FORM openjdk:8-jdk-alpine， 表示以openjdk:8-jdk-alpine这个镜像为基础镜像，因为这是一个Springboot项目所以必须要有jdk支持，我们在定制镜像时，可以找一个最适合的镜像作为基础镜像。 第二行：ENV PROFILE=dev， 定义了一个环境变量，这个环境变量可以在后面被引用 第三行：RUN mkdir /app /logs，通过mkdir命令创建了两个目录，用来保存jar执行文件及日志 第四行：COPY ./target/swagger-register-1.0.0-SNAPSHOT.jar /app/app.jar 将target目录下的jar包复制到/app目录下，并且进行重命名 第五行：WORKDIR /app， 指定工作目录为/app，后面各层的当前目录就是指定的工作目录 第六行：VOLUME /register-data， 定义一个匿名数据卷，前面说过写操作不要直接在容器内进行，而要改为写挂载的数据卷目录，这个定义可在运行容器时通过 -v 来覆盖。 第七行：EXPOSE 11090， 声明了运行容器时提供的服务端口，也仅仅是个声明而已，只是告诉使用的人要映射这个端口，通过 -p 可映射端口。 第八行：CMD [“java”, “-Dspring.profiles.active=${PROFILE}”, “-jar”, “app.jar”]， 指定了容器启动命令，因为是一个Springboot项目，所以就是一个java -jar的执行命令，容器启动的时候就会执行该命令来运行Springboot服务，这里引用了第二行定义的环境变量PROFILE 3.2 配置maven插件定义好Dockerfile后，为了方便构建镜像，我们可以借助maven的dockerfile插件dockerfile-maven-plugin，在pom.xml的build部分加入配置如下 123456789101112131415161718192021&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- Docker maven plugin --&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;dockerfile-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.4.10&lt;/version&gt; &lt;configuration&gt; &lt;repository&gt;$&#123;docker.image.prefix&#125;/$&#123;project.artifactId&#125;&lt;/repository&gt; &lt;buildArgs&gt; &lt;JAR_FILE&gt;target/$&#123;project.build.finalName&#125;.jar&lt;/JAR_FILE&gt; &lt;/buildArgs&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- Docker maven plugin --&gt; &lt;/plugins&gt;&lt;/build&gt; repository指定了镜像的名称，docker.image.prefix需要properties部分进行定义，我这里是springboot。 3.3 构建镜像下载源码：https://github.com/ronwxy/swagger-register.git ，然后在项目的根目录下执行如下命令(前提是本地已经装好了docker与maven及jdk) 1mvn clean package -Dmaven.test.skip=true dockerfile:build 该命令首先会执行mvn clean package -Dmaven.test.skip=true对项目进行打包，生成./target/swagger-register-1.0.0-SNAPSHOT.jar文件，然后基于当前目录下的Dockerfile文件进行构建，如下图所示 由上图可看出，该镜像构建分八步(对应Dockerfile的八行指令)，每一步生成一个镜像层，每一层都有唯一的ID。由图中也可以看出，除了COPY之类的命令外，每一层的构建实际上是先基于上一层启动一个容器，然后执行该层定义的操作，再移除这个容器来实现的，如第八步中 12345Step 8/8 : CMD [\"java\", \"-Dspring.profiles.active=$&#123;PROFILE&#125;\", \"-jar\", \"app.jar\"][INFO] [INFO] ---&gt; Running in f4acd0b53bca[INFO] Removing intermediate container f4acd0b53bca[INFO] ---&gt; a9ee579f2d62 先启动一个ID为f4acd0b53bca的容器，在其中执行CMD所定义的命令，然后再移除容器f4acd0b53bca，最后生成ID为a9ee579f2d62的镜像。 构建完后，我们就可以在本地镜像中通过docker iamges看到我们定制的镜像了，如图 图中springboot/swagger-register镜像即为我们刚刚构建好的定制镜像。 3.4 启动容器我们可以通过以下命令来启动一个刚才定制镜像的容器 1docker run -d --name swagger-register -p 11090:11090 -v /home/jenkins/swagger-register/register-data:/register-data -v /home/jenkins/swagger-register/logs:/logs --restart=always springboot/swagger-register:latest 其中： -d 表示以后台进程方式运行 –name 指定容器名称 -p 指定端口映射，左边为宿主机端口，右边为容器服务端口 -v 指定数据卷挂载，左边为宿主机目录，右边为容器目录 –restart=always 表示在docker启动时自动启动该容器 关于容器相关的内容后面详细介绍，这里不展开说明了。启动容器后， 我们就可以浏览器打开地址 http://宿主机ip:11090/doc.html 来访问服务了（打开页面后内容是空的，因为没有任何服务注册Swagger API， 相关内容可参考 swagger api文档集中化注册管理） 4. 总结本文介绍了一个基于Springboot项目的Docker镜像定制及使用过程，对镜像的构建过程，及Dockerfile的基本指令以及容器的运行做了基本介绍。后续会对Dockerfile的其它指令及Dockerfile的一些最佳实践进行更为详细的介绍，欢迎关注。我的个人博客地址：http://blog.jboost.cn我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号，欢迎关注，及时获取更新内容）———————————————————————————————————————————————————————————————","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.jboost.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.jboost.cn/tags/docker/"}]},{"title":"Docker笔记（四）：Docker镜像管理","slug":"docker-4","date":"2019-07-16T13:22:11.000Z","updated":"2020-08-26T01:30:48.666Z","comments":true,"path":"docker-4.html","link":"","permalink":"http://blog.jboost.cn/docker-4.html","excerpt":"在Docker中，应用是通过容器来运行的，而容器的运行是基于镜像的，类似面向对象设计中类与对象的关系——没有类的定义就谈不上实例的创建与使用，没有镜像的定义就谈不上容器的创建与运行。","text":"在Docker中，应用是通过容器来运行的，而容器的运行是基于镜像的，类似面向对象设计中类与对象的关系——没有类的定义就谈不上实例的创建与使用，没有镜像的定义就谈不上容器的创建与运行。 1. 获取镜像镜像从哪里来，一般两个途径，一是公共镜像库，如官方镜像库Docker Hub，上面有大量的高质量的镜像直接可拿来用；二是自定义，我们可基于一个已有镜像，在其基础上增加一些层（还记得镜像的分层存储特性吧），然后构建形成自己的镜像。 如果我们知道某个镜像的名称，则可直接通过docker pull来下载镜像到本地，如ubuntu、redis、nginx等，docker pull命令的格式如下 1docker pull [选项] [Docker Registry的地址[:端口号]/]仓库名[:标签] 其中选项可设置： -a, –all-tags：下载仓库中所有标签（一般指版本）的镜像 –disable-content-trust：跳过镜像验证，默认为true Docker Registry的地址即镜像仓库地址，一般为域名或IP加端口号，如果不指定则默认为Docker Hub；仓库名包含两部分，&lt;用户名&gt;/&lt;软件名&gt;，对于Docker Hub，如果不给出用户名，则默认为library，表示官方提供；标签一般是对应软件的版本号，如果不指定则默认为latest。 比如我们要下一个nginx镜像，则可执行如下命令 12345678[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker pull nginxUsing default tag: latestlatest: Pulling from library/nginxfc7181108d40: Already exists d2e987ca2267: Pull complete 0b760b431b11: Pull complete Digest: sha256:48cbeee0cb0a3b5e885e36222f969e0a2f41819a68e07aeb6631ca7cb356fed1Status: Downloaded newer image for nginx:latest 这里我们没有指定选项，也没有指定镜像仓库地址，那么默认会从Docker Hub获取镜像（但Docker Hub由于在国外，速度比较慢，所以一般要设置国内加速器，参考Docker笔记（三）：Docker安装与配置第二部分：配置国内镜像)，也没有给出用户名，所以默认是library（第三行），没有指定标签，所以默认是latest（第二行），由第四至第六行可见，这个镜像包含三个层，并且第一个层已经存在了（之前下载的镜像已经包含了这个层， 直接复用），镜像分层的概念及层的复用，应该已经理解了。 如果我们不知道镜像的完整名称怎么办，那就搜索一下，有两个途径，一是通过命令，假设我们记不起nginx全称了， 只记得ngi，则可通过如下命令搜索 123456789101112131415[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker search ngiNAME DESCRIPTION STARS OFFICIAL AUTOMATEDnginx Official build of Nginx. 11693 [OK] jwilder/nginx-proxy Automated Nginx reverse proxy for docker con… 1628 [OK]richarvey/nginx-php-fpm Container running Nginx + PHP-FPM capable of… 726 [OK]bitnami/nginx Bitnami nginx Docker Image 69 [OK]linuxserver/nginx An Nginx container, brought to you by LinuxS… 69 tiangolo/nginx-rtmp Docker image with Nginx using the nginx-rtmp… 48 [OK]nginx/nginx-ingress NGINX Ingress Controller for Kubernetes 20 nginxdemos/hello NGINX webserver that serves a simple page co… 18 [OK]jlesage/nginx-proxy-manager Docker container for Nginx Proxy Manager 17 [OK]schmunk42/nginx-redirect A very simple container to redirect HTTP tra… 17 [OK]crunchgeek/nginx-pagespeed Nginx with PageSpeed + GEO IP + VTS + more_s… 13 blacklabelops/nginx Dockerized Nginx Reverse Proxy Server. 12 [OK]... 该命令会从Docker Hub搜索镜像名包含ngi的镜像，其中STARS表示收藏用户数，OFFICIAL为[OK]表示官方提供的镜像，AUTOMATED [OK]表示由自动构建生成，一般选择STARS最多，官方提供的镜像。这种方式获取到的信息有限，比如具体包含哪些版本不知道。还有一个途径是直接在Docker Hub网站上搜索，打开 https://hub.docker.com ， 在搜索框输入ngi，如下图 则会列出所有满足条件的镜像，点开nginx结果链接，可以看到提供的版本（通过版本链接可以查看定义对应镜像的Dockerfile），及相应的文档说明。这种方式获取的信息更加全面，所以推荐这种方式！ 另外，当我们没有执行docker pull，直接通过docker run xx来运行一个容器时，如果没有对应的镜像，则会先自动下载镜像，再基于镜像启动一个容器，比如我们在Docker笔记（三）：Docker安装与配置中检验docker是否安装成功时运行的hello-world 2. 管理本地镜像将镜像下载到本地后，我们可以基于镜像来创建、运行容器，及对镜像进行管理。 查看本地镜像 12345[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEnginx latest f68d6e55e065 2 weeks ago 109MBmysql latest c7109f74d339 5 weeks ago 443MBhello-world latest fce289e99eb9 6 months ago 1.84kB 上面各列依次列出了镜像名称、标签（版本）、镜像ID、创建时间、镜像大小。镜像可以拥有多个标签（版本）。镜像的大小总和一般要大于实际的磁盘占有量，为什么？回忆一下镜像的分层存储概念，层是可以复用的，某个层其中一个镜像有了，另一个镜像就不会再下载了。口说无凭，我们来验证下，docker system df可列出镜像、容器、数据卷所占用的空间 123456[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker system dfTYPE TOTAL ACTIVE SIZE RECLAIMABLEImages 3 1 497.1MB 497.1MB (99%)Containers 1 0 0B 0BLocal Volumes 0 0 0B 0BBuild Cache 0 0 0B 0B 通过docker image ls列出的各镜像大小总共约552MB，但这里列出的镜像大小只有约497MB，这下有凭有据了吧。 根据条件列出镜像 123docker image ls nginx # 根据名称列出镜像docker image ls nginx:latest # 根据名称与标签列出镜像docker image ls -f since=hello-world:latest # -f 是--filter的缩写，过滤器参数，列出在hello-world:latest之后建立的镜像，before=hello-world:latest则查看之前建立的镜像 指定显示格式 12345docker image ls -q # 只显示镜像IDdocker image ls --digests # 列出镜像摘要docker image ls --format \"&#123;&#123;.ID&#125;&#125;: &#123;&#123;.Repository&#125;&#125;\" # 使用Go的模板语法格式化显示，这里显示格式为 镜像ID：镜像名称docker image ls --format \"table &#123;&#123;.ID&#125;&#125;\\t&#123;&#123;.Repository&#125;&#125;\\t&#123;&#123;.Tag&#125;&#125;\" # 自己定义表格格式 虚悬镜像有时候会看到某些镜像既没有仓库名，也没有标签，均为 &lt;none&gt;。这些镜像原本是有镜像名和标签的，随着官方镜像维护，发布了新版本后(新版本会复用之前的镜像名称与标签，一般是bug修复版)，重新docker pull xx 时， 这个镜像名被转移到了新下载的镜像身上，而旧的镜像上的这个名称则被取消，从而成为了&lt;none&gt; 。除了docker pull可能导致这种情况， docker build也同样可以导致这种现象。由于新旧镜像同名，旧镜像名称被取消，从而出现仓库名、标签均为 &lt;none&gt; 的镜像。这类无标签镜像被称为虚悬镜像(dangling image) ，可以用下面的命令专门显示这类镜像： 1docker image ls -f dangling=true 一般虚悬镜像没什么意义了，可以通过如下命令删除 1docker image prune 中间层镜像为了加速镜像构建、重复利用资源，Docker会利用中间层镜像。所以在使用一段时间后，可能会看到一些依赖的中间层镜像。默认的docker image ls列表中只会显示顶层镜像，如果希望显示包括中间层镜像在内的所有镜像的话，可以加 -a 1$ docker image ls -a 这样会看到很多无标签的镜像，与虚悬镜像不同，这些无标签的镜像很多都是中间层镜像，是其它镜像所依赖的镜像。这些无标签镜像不应该删除，否则会导致上层镜像因为依赖丢失而出错。实际上，这些镜像也没必要删除，因为相同的层只会存一遍，而这些镜像是别的镜像的依赖，因此并不会因为它们被列出来而多存了一份，无论如何你也会需要它们。只要删除那些依赖它们的镜像后，这些依赖的中间层镜像也会被连带删除。 删除镜像删除镜像命令格式 1docker image rm [选项] &lt;镜像1&gt; [&lt;镜像2&gt; ...] 选项可以设置： -f, –force 强制删除镜像 –no-prune 不删除没有标签的父镜像 &lt;镜像1&gt;、&lt;镜像2&gt; 等可以是镜像的名称，镜像的全ID，也可以是镜像ID的前面几个数字（只要与其它镜像区分开来就行），或者是镜像摘要。 如删除镜像名称为mysql的镜像 1234567[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# docker image rm mysqlUntagged: mysql:latestUntagged: mysql@sha256:415ac63da0ae6725d5aefc9669a1c02f39a00c574fdbc478dfd08db1e97c8f1bDeleted: sha256:c7109f74d339896c8e1a7526224f10a3197e7baf674ff03acbab387aa027882aDeleted: sha256:35d60530f024aa75c91a123a69099f7f6eaf5ad7001bb983f427f674980d8482Deleted: sha256:49d8bb533eee600076e3a513a203ee24044673fcef0c1b79e088b2ba43db2c17... 由上面命令的执行结果可见，删除镜像包括另个行为：Untagged、Deleted。 当我们使用上面命令来删除镜像的时候，实际上是在要求删除某个/某些标签的镜像。所以首先需要做的是将满足要求的所有镜像标签都取消，这就是Untagged的行为。一个镜像可以对应多个标签，因此当我们删除了所指定的标签后，可能还有别的标签指向了这个镜像，如果是这种情况，那么Delete行为就不会发生，仅仅是取消了这个镜像的符合要求的所有标签。所以并非所有的docker image rm都会产生删除镜像的行为，有可能仅仅是取消了某个标签而已。 当该镜像所有的标签都被取消了，该镜像很可能就失去了存在的意义，因此会触发删除行为。镜像是多层存储结构，因此在删除的时候也是从上层向基础层方向依次进行判断删除。如果某个其它镜像正依赖于当前镜像的某一层，这种情况，依旧不会触发删除该层的行为。直到没有任何镜像依赖当前层时，才会真实的删除当前层。 另外还需要注意是容器对镜像的依赖。如果基于镜像启动的容器存在（即使容器没有运行处于停止状态） ，同样不可以删除这个镜像。我们之前说了容器是以镜像为基础，再加一层容器存储层组成的多层存储结构去运行的。所以如果这些容器是不需要的，应该先将它们删除，然后再来删除镜像。 通过组合命令来删除 12docker image rm $(docker image ls -q nginx) # 删除镜像名称为nginx的所有镜像docker image rm $(docker image ls -q -f since=hello-world:latest) # 删除所有在hello-world:latest之后建立的镜像 3. 总结本文对镜像的获取及本地镜像的基本管理做了介绍，本文镜像的获取途径都是从镜像仓库直接获取，镜像的另一个获取途径便是自定义，接下来会通过实例来进行介绍，欢迎关注。我的个人博客地址：http://blog.jboost.cn我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号，欢迎关注，及时获取更新内容）———————————————————————————————————————————————————————————————","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.jboost.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.jboost.cn/tags/docker/"}]},{"title":"Docker笔记（三）：Docker安装与配置","slug":"docker-3","date":"2019-07-14T11:54:05.000Z","updated":"2020-08-26T01:30:44.638Z","comments":true,"path":"docker-3.html","link":"","permalink":"http://blog.jboost.cn/docker-3.html","excerpt":"Docker分为Docker CE社区免费版与Docker EE企业收费版。Docker EE主要是在安全性及镜像、容器高级管理方面提供了一些额外的支持。对于中小型企业、团队或个人来说，用Docker CE即可。","text":"Docker分为Docker CE社区免费版与Docker EE企业收费版。Docker EE主要是在安全性及镜像、容器高级管理方面提供了一些额外的支持。对于中小型企业、团队或个人来说，用Docker CE即可。 1. 安装Docker CEDocker CE有三个更新渠道： Stable：提供最新的GA（General Availability）稳定版，每六个月一版，如 18.09 表示18年9月版，下一版就是19.03——19年3月版 Test：提供GA之前的Pre-release版 Nightly：提供最新的build版本，每天一版 我们一般使用stable版。Docker CE支持在多种操作系统下安装，本文只介绍比较常见的Ubuntu 18.04 LTS、CentOS7、及Windows 10上的安装与配置。 1.1 Ubuntu 18.04 LTS 上安装Docker CE支持的64位Ubuntu系统版本为 Cosmic 18.10 Bionic 18.04 (LTS) Xenial 16.04 (LTS) Docker CE在Ubuntu上支持 overlay2， aufs， 以及 btrfs 几种存储驱动程序，对于Linux内核版本为4或以上系统的安装，Docker CE默认使用 overlay2，如果需要使用 aufs，则需要手动配置（参考： Use the AUFS storage driver） 卸载旧版本 如果系统安装有旧版本，旧版本命名为 docker， docker.io，或docker.engine，可使用如下命令进行卸载 1$ sudo apt-get remove docker docker-engine docker.io containerd runc 目录/var/lib/docker下的内容，包括镜像、容器、数据卷、网络等，会被保留。 使用APT安装 apt源使用HTTPS来确保软件下载过程中不被篡改，所以首先添加使用HTTPS传输需要的软件包以及CA证书 1234567$ sudo apt-get update$ sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common 为了确认下载软件包的合法性，添加Docker官方的GPG key： 12$ curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add - 由于国内网络原因，我们一般要使用国内源，否则安装将会灰常灰常慢。向source.list中添加Docker软件源（以下命令添加的是stable版本的APT镜像源，如果需要test或nightly版，将stable改为对应test或nightly即可） 1234$ sudo add-apt-repository \\\"deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \\$(lsb_release -cs) \\stable\" 然后，便可更新apt软件包缓存，开始安装了 12$ sudo apt-get update$ sudo apt-get install docker-ce 以上命令默认会安装软件源里的最新版本，如果需要安装指定版本，则可通过查看可用版本，然后指定版本安装，查看版本 1$ apt-cache madison docker-ce 安装指定版本 1$ sudo apt-get install docker-ce=&lt;VERSION_STRING&gt; 使用脚本自动安装 Docker提供了一个方便的安装脚本来在开发测试环境安装Docker CE的edge或测试版，Ubuntu上可使用这套脚本来安装Docker CE的edge版 12$ curl -fsSL https://get.docker.com -o get-docker.sh$ sudo sh get-docker.sh --mirror Aliyun 启动Docker CE 12$ sudo systemctl enable docker #开启开机自动启动$ sudo systemctl start docker #启动docker 用户组配置 docker命令默认是使用Unix socket与Docker引擎进行通信（回顾下除了Unix socket还有REST API及网络端口），只有root用户或docker用户组里的用户才有权限访问Docker引擎的Unix socket，因此，需要将使用docker的用户加入docker用户组（处于安全考虑，一般尽量不要直接使用root用户来操作） 12$ sudo groupadd docker #添加docker用户组$ sudo usermod -aG docker $USER #将当前用户加到docker用户组 退出账号重新登录即可。 测试安装是否成功 1$ docker run hello-world 如果显示如下图，则说明安装已成功 卸载 1$ sudo apt-get purge docker-ce 以上命令可以卸载docker-ce，但是之前的镜像、容器、数据卷等不会自动删除，可通过如下命令彻底删除 1$ sudo rm -rf /var/lib/docker 1.2 CentOS 7 上安装Docker CE支持64位的CentOS7，并且要求内核版本不低于3.10。CentOS 7满足最低内核的要求，但由于版本较低，一些功能（如 overlay2 存储层驱动）无法使用，并且部分功能可能不太稳定。可以通过uname -r命令来查看系统内核版本，如 12[root@iZwz9dbodbaqxj1gxhpnjxZ ~]# uname -r3.10.0-957.1.3.el7.x86_64 卸载旧版本 如果安装了旧版本，需要先卸载。旧版本的Docker称为docker或者docker-engine，卸载命令 12345678910$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 使用yum安装 安装依赖包 123$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 由于国内网络原因，我们一般要使用国内源，否则安装可能会灰常灰常慢。添加yum软件源 1234$ sudo yum-config-manager \\ --add-repo \\ https://mirrors.ustc.edu.cn/docker-ce/linux/centos/docker-ce .repo 如果要安装nightly或test版，执行如下对应的命令 12$ sudo yum-config-manager --enable docker-ce-nightly # 启用nightly， 将--enbale改为disable又可以禁用$ sudo yum-config-manager --enable docker-ce-test # 启用test 安装最新版本 12$ sudo yum makecache fast # 更新软件源缓存$ sudo yum install docker-ce # 安装最新版本 安装指定版本 12$ sudo yum list docker-ce --showduplicates | sort -r # 列出可用版本$ sudo yum install docker-ce-&lt;VERSION_STRING&gt; # 安装指定版本 使用脚本自动安装 执行如下命令，则会自动安装Docker CE的edge版，注意只在开发或测试环境这么用（建议最好还是用stable版） 12$ curl -fsSL get.docker.com -o get-docker.sh$ sudo sh get-docker.sh --mirror Aliyun 启动Docker CE 12$ sudo systemctl enable docker #开启开机自动启动$ sudo systemctl start docker #启动docker 用户组配置 docker命令默认是使用Unix socket与Docker引擎进行通信（回顾下除了Unix socket还有REST API及网络端口），只有root用户或docker用户组里的用户才有权限访问Docker引擎的Unix socket，因此，需要将使用docker的用户加入docker用户组（处于安全考虑，一般尽量不要直接使用root用户来操作） 12$ sudo groupadd docker #添加docker用户组$ sudo usermod -aG docker $USER #将当前用户加到docker用户组 退出账号重新登录即可。 测试安装是否成功 1$ docker run hello-world 如果显示如下图，则说明安装已成功 如果在 CentOS 中使用 Docker CE 看到下面的这些警告信息： 12WARNING: bridge-nf-call-iptables is disabledWARNING: bridge-nf-call-ip6tables is disabled 可以添加内核配置参数以启用这些功能。 1234$ sudo tee -a /etc/sysctl.conf &lt;&lt;-EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF 然后重新加载 sysctl.conf 即可 1$ sudo sysctl -p 卸载 12$ sudo yum remove docker-ce # 卸载docker-ce$ sudo rm -rf /var/lib/docker # 该目录下的镜像、容器、数据卷、网络等不会自动删除 1.3 Windows 10 上安装windows 10上的安装非常简单，直接下载stable版本安装。安装完后，在 Windows 搜索栏输入 Docker 点击 Docker for Windows 开始运行 2. 配置国内镜像Docker默认是从Docker Hub（官方的镜像仓库）拉取镜像的，国内访问一般会比较慢，因此可以配置一些镜像加速器，很多云服务商提供了自己的加速器服务，如Azure中国，阿里云（需要登录获取），七牛云等。 Ubuntu、CentOS上，配置国内镜像只需要在/etc/docker/daemon.json中写入如下内容（如果文件不存在则创建一个） 123456&#123; \"registry-mirrors\": [ \"https://dockerhub.azk8s.cn\", \"https://reg-mirror.qiniu.com\" ]&#125; 然后重新启动服务 12$ sudo systemctl daemon-reload$ sudo systemctl restart docker 对于Windows 10，在系统右下角托盘Docker图标上右键菜单选择Settings ，打开配置窗口后在左侧导航菜单选择 Daemon 。在 Registrymirrors 一栏中填写加速器地址 https://dockerhub.azk8s.cn ，之后点击Apply 保存， Docker 就会自动重启并应用配置的镜像地址了。 可以通过docker info命令来检查加速器是否生效，如果执行命令能看到类似如下信息，则说明加速器配置生效了。 12Registry Mirrors: https:&#x2F;&#x2F;dockerhub.azk8s.cn&#x2F; 3. 总结Docker分Docker CE与Docker EE两个版本，对大多数人来说，一般使用Docker CE就行了。我们在安装Docker CE时，最好安装stable版，比较稳定可靠。同时，Linux安装时，记得配置Docker软件源，不然有可能太慢。安装完后，需要配置镜像加速器，加快镜像的下载速度。工具有了，接下来就是探索实践了，加油吧少年！我的个人博客地址：http://blog.jboost.cn我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号，欢迎关注，及时获取更新内容）———————————————————————————————————————————————————————————————","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.jboost.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.jboost.cn/tags/docker/"}]},{"title":"Docker笔记（二）：Docker管理的对象","slug":"docker-2","date":"2019-07-14T00:36:56.000Z","updated":"2020-08-26T01:30:40.521Z","comments":true,"path":"docker-2.html","link":"","permalink":"http://blog.jboost.cn/docker-2.html","excerpt":"在Docker笔记（一）：什么是Docker中，我们提到了Docker管理的对象包含镜像、容器、网络、数据卷等，本文就来介绍下这些对象及用途。","text":"在Docker笔记（一）：什么是Docker中，我们提到了Docker管理的对象包含镜像、容器、网络、数据卷等，本文就来介绍下这些对象及用途。 1. 镜像所谓镜像，是一个静态的概念。它对我们期望干的事情做了一些定义，比如要运行什么程序，需要哪些依赖，需要什么样的配置，需要开放哪个网络端口等等。Docker的镜像是一个特殊的文件系统，提供了运行时需要的程序、库、资源、配置等文件，还包含一些为运行时准备的配置参数（如环境变量、匿名数据卷、用户等），镜像不包含任何动态数据，其内容在构建之后也不会被改变。镜像的文件系统有一个分层存储的概念，采用的是Union FS技术，因此，镜像并不是简单地由一组文件组成，而是由多层文件系统叠加联合组成。如下图所示 镜像构建时，会一层一层地构建，前一层是后一层的基础，每层构建完后就变成只读的，不会再发生改变。镜像分层存储的一大好处是复用，镜像的每一层可以在不同镜像间复用，这就好比我们开发项目时将一些公共功能封装成jar包，在各个项目可以直接依赖使用一样。关于镜像的更多内容，在后续使用时再详述。 2. 容器相对镜像，容器是一个动态的运行时的概念，它与镜像的关系类似于面向对象中类与实例的关系。容器可以被创建、启动、停止、删除等。容器运行实质上就是运行一个进程，但与那些直接在宿主机上运行的进程不同，容器运行在自己的独立的隔离的命名空间中——拥有自己的root文件系统、网络配置、进程空间，甚至自己的用户ID空间，因此虽然是以进程的形式运行，但好像是运行在一个独立的系统中一样，这样相比直接运行于宿主机的进程，容器的运行显得更为安全。前面说到镜像的分层存储概念，对于容器来说，实际上也是以镜像作为基础层，在其上创建了一个当前容器的存储层，如下图 以镜像ubuntu:15.04为基础层所创建的容器，都有一个自己的可读写的存储层（镜像的存储层是只读的）。容器存储层的生命周期与容器一样，容器销毁时，容器的存储层也会随之消亡，任何保存在容器存储层的数据也都会随容器的删除而丢失，因此一般我们要保持容器存储层的无状态化，所有文件的写操作，都应该使用数据卷或绑定宿主机目录。 3. 数据卷数据卷是一个独立于容器，可供一个或多个容器使用的特殊目录，它绕过了Union FS，不会随容器的销毁而消亡。这好比我们在阿里云上建虚机，再加载一个数据盘一样，一般产生的数据都要保存在数据盘，而不是虚机的系统盘。数据卷具备如下特性： 可以在容器之间共享和重用 对数据卷的修改会立马生效 数据卷的更新，不会影响到镜像 数据卷默认会一直存在，不会随容器的删除而消亡 4. 网络Docker容器是如何与外部进行网络通信的？一般来说，我们在运行容器时，只需要指定容器服务端口与宿主机端口的映射，就可以通过宿主机IP与映射的端口访问容器服务了，因为Docker默认使用了Bridge的模式来实现容器与外部的通信。Docker的网络子系统通过使用一些驱动程序，是可插拔式的，默认提供了如下几种驱动： bridge：默认的网络驱动。运行在容器中的应用程序一般是通过网桥与外部进行通信。 host：容器直接使用宿主机的网络通信。host只在基于Docker 17.06或以上版本的Swarm服务中可用 overlay：overlay可将多个Docker daemon进程连接起来使得Swarm服务之间能相互通信，也可以将overlay用于Swarm服务与容器之间，或运行在不同Docker daemon上的容器之间的通信，不需要操作系统层面的路由配置。 macvlan：macvlan允许你分配一个mac地址给容器，让它像一台物理设备一样加入你的网络中。Docker daemon通过mac地址将请求路由给容器，适用于那些希望直接连到物理网络的遗留应用。 none：禁用所有网络。一般与一个自定义的网络驱动一起使用。none不能用于Swarm服务。 其它第三方网络插件：可从Docker Hub或其它第三方供应商获取安装。 总之，bridge适用于在同一台宿主机运行多个容器的场景；host适用于不应与宿主机进行网络隔离的场景；overlay适用于运行在不同宿主机上的容器间通信，或多个应用通过Swarm服务来共同协作的场景；macvlan适用于从虚拟机迁移配置或希望容器作为物理机一样使用网络的场景。 5. 总结本文对Docker所管理的几个基本对象——镜像、容器、数据卷、网络做了简单介绍，这是认识或学习Docker的基础，在后续实践操作过程中，将会对各部分进行更详细的使用说明，欢迎持续关注。我的个人博客地址：http://blog.jboost.cn我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号，欢迎关注，及时获取更新内容）———————————————————————————————————————————————————————————————","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.jboost.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.jboost.cn/tags/docker/"}]},{"title":"Docker笔记（一）：什么是Docker","slug":"docker-1","date":"2019-07-13T02:13:25.000Z","updated":"2020-08-26T01:30:34.508Z","comments":true,"path":"docker-1.html","link":"","permalink":"http://blog.jboost.cn/docker-1.html","excerpt":"1. 前言接触Docker也有两年多了，断断续续玩过一些应用场景，如安装一些常用工具服务，部署业务项目，基于gitlab+jenkins pipeline+docker的CI/CD实现等。了解其基本知识与操作，但不能说深度掌握，故借此系列进行梳理与学习，也希望对有意学习Docker的人提供参考。","text":"1. 前言接触Docker也有两年多了，断断续续玩过一些应用场景，如安装一些常用工具服务，部署业务项目，基于gitlab+jenkins pipeline+docker的CI/CD实现等。了解其基本知识与操作，但不能说深度掌握，故借此系列进行梳理与学习，也希望对有意学习Docker的人提供参考。 2. Docker简介Docker最初是dotCloud公司（后来也改名为Docker）的一个内部项目，于2013年3月开源。Docker使用Google推出的Go语言实现，基于Linux内核的cgroup、namespace、Union FS等技术（先不用急着了解这些都是啥），对进程进行隔离，是操作系统层面的虚拟化技术。相对于传统的硬件层面的虚拟化技术（虚拟机），Docker显得更为轻量化。下图为传统虚拟机与Docker的结构对比 由上图可看出传统虚拟机技术是在硬件层面虚拟出一套硬件（CPU、内存、磁盘、网卡等）后，在其上运行一个完整的操作系统，再在操作系统上运行应用进程；而Docker的应用进程是直接运行在宿主机的内核上，也不需要进行硬件虚拟，因此，Docker要比传统虚拟机更为轻便。 总结Docker相对传统虚拟化技术的优势如下： 更高的资源利用率：Docker不需要硬件虚拟与运行完整操作系统的开销，所以资源利用率更高，同样配置的主机，采用Docker往往可以运行更多数量的应用。 更高效的使用体验：在操作系统上安装一些常用软件，如mysql，redis等，往往需要折腾好一阵，有些还要手动安装各种依赖，而采用Docker，可能几行命令就可以让一个服务快速运行起来。 一致的运行环境：Docker镜像功能可以把程序运行需要的环境进行封装，确保程序在开发、测试、生产环境都能保持一致性，避免因环境不一致导致程序运行异常。 CI/CD支持：使用Docker可以定制镜像来实现持续集成、持续部署，如基于gitlab + jenkins pipeline + docker的自动化部署。 更轻松的维护：因为Docker保证了运行环境的一致性，因此应用的迁移或缩放将变得很容易；Docker的分层存储与镜像技术，也使得应用重复部分的复用变得更简单，基于基础镜像可以进一步扩展定义自己的镜像，也可以直接使用官方镜像来使用。 3. Docker的基本架构Docker的基本架构图如下 主要包括几部分： Docker daemon（Docker守护进程 dockerd）：Docker的执行引擎，负责监听处理Docker客户端请求与管理Docker相关对象，如镜像、容器、网络、数据卷等。一个Docker守护进程可与其它Docker守护进程进行通信，作为Docker服务进行管理。 Docker client（Docker客户端 docker）：Docker客户端（docker CLI命令）是大多数用户用来与Docker守护进程交互的方式，比如你在命令行执行docker run，Docker客户端将发送该命令请求到Docker守护进程，由守护进程执行。Docker客户端可通过REST API, UNIX Socket或网络接口来与Docker守护进程进行通信，并且可与多个Docker守护进程进行通信。 Docker Registry（Docker注册中心）：用来存储Docker镜像的仓库，类似于Maven的Nexus。Docker官方提供了一个公共镜像仓库Docker Hub（ https://hub.docker.com/ ），docker相关命令默认会从Docker Hub上搜索与下载镜像，我们可以配置一些国内镜像仓库地址来进行加速，甚至搭建自己的私有镜像仓库。 Docker Objects：Docker管理的对象，主要包括镜像、容器、网络、数据卷等。 4. Docker的用途根据第二部分Docker的优势及笔者的经验来看，目前Docker主要用于 常用软件服务的搭建运行，如Mysql、Redis、Nginx等 业务服务的发布部署，尤其是基于SpringBoot的微服务 CI/CD实现，结合Gitlab的webhook，Jenkins的pipeline，实现自动化集成与部署 快速的弹性伸缩，在容器集群化管理的场景中，如Swarm、K8s解决方案中，可基于容器对服务进行快速的弹性伸缩来应对业务量的突发情况 执行环境封装，如一些深度学习框架模型，打成Docker镜像的方式进行发布，可以快速在不同的环境中运行起来 … 5. 总结在微服务架构、DevOps这些概念盛行的时代，容器化技术变得越来越重要，几乎成为每一位开发人员需要掌握的技能。本系列文章是笔者基于自身实践及相关文献参考，对Docker相关技术进行整理，欢迎关注，共同学习。我的个人博客地址：http://blog.jboost.cn我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号，欢迎关注，及时获取更新内容）———————————————————————————————————————————————————————————————","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.jboost.cn/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.jboost.cn/tags/docker/"}]},{"title":"ubuntu18.04上搭建KVM虚拟机环境超完整过程","slug":"ubuntu-kvm","date":"2019-07-11T06:13:23.000Z","updated":"2020-08-26T01:34:13.153Z","comments":true,"path":"ubuntu-kvm.html","link":"","permalink":"http://blog.jboost.cn/ubuntu-kvm.html","excerpt":"看标题这是篇纯运维的文章。在中小型企业中，一般很少配置专业的运维人员，都是由开发人员兼着。同时，对有志于技术管理的开发人员来说，多了解一些运维及整个软件生命周期的知识，是很有帮助的，因为带团队不仅仅是个管人的活，更多的是在你的部下遇到难题或者无人能上的时候，你能协助他解决或亲自上阵，这比只会“吆五喝六”的管理者将能获得更高的敬重与威信。闲话不多说了，记录下整个KVM虚拟机的搭建过程吧。","text":"看标题这是篇纯运维的文章。在中小型企业中，一般很少配置专业的运维人员，都是由开发人员兼着。同时，对有志于技术管理的开发人员来说，多了解一些运维及整个软件生命周期的知识，是很有帮助的，因为带团队不仅仅是个管人的活，更多的是在你的部下遇到难题或者无人能上的时候，你能协助他解决或亲自上阵，这比只会“吆五喝六”的管理者将能获得更高的敬重与威信。闲话不多说了，记录下整个KVM虚拟机的搭建过程吧。 1. KVM安装1.1 配置确认首先需要确认服务器的硬件是否支持虚拟化，执行如下命令确认 12devuser@server_01:~$ egrep -c '(vmx|svm)' /proc/cpuinfo48 如果输出结果大于0，意味着服务器硬件是支持虚拟化的。否则，重启进入BIOS设置中启用VT技术。执行如下命令安装kvm-ok程序，来确定服务器是否能够运行硬件加速的KVM虚拟机 12345devuser@server_01:~$ sudo apt install cpu-checkerdevuser@server_01:~$ sudo kvm-okINFO: /dev/kvm existsKVM acceleration can be used 1.2 安装KVM安装KVM及依赖项 12devuser@server_01:~$ sudo apt updatedevuser@server_01:~$ sudo apt install qemu qemu-kvm libvirt-bin bridge-utils virt-manager 启动libvirtd服务，并设置开机自动启动 12devuser@server_01:~$ sudo systemctl start libvirtd.servicedevuser@server_01:~$ sudo systemctl enable libvirtd.service 执行service libvirtd status查看libvirtd服务状态，如图 1.3 桥接网络配置一般虚拟机网络配置有Bridge、NAT等几种模式。NAT模式下，虚拟机不需要配置自己的IP，通过宿主机来访问外部网络；Bridge模式下， 虚拟机需要配置自己的IP，然后虚拟出一个网卡， 与宿主机的网卡一起挂到一个虚拟网桥上（类似于交换机）来访问外部网络，这种模式下，虚拟机拥有独立的IP，局域网其它主机能直接通过IP与其通信。简单理解，就是NAT模式下，虚机隐藏在宿主机后面了，虚机能通过宿主机访问外网，但局域网其它主机访问不到它，Bridge模式下，虚机跟宿主机一样平等地存在，局域网其它主机可直接通过IP与其通信。一般我们创建虚机是用来部署服务供使用的， 所以都是用Bridge模式。 ubuntu 18中，网络配置通过netplan来实现了，如下，更改配置文件 /etc/netplan/50-cloud-init.yaml 1234567891011121314151617181920212223devuser@cserver_01:~$ sudo vim /etc/netplan/50-cloud-init.yaml# This file is generated from information provided by# the datasource. Changes to it will not persist across an instance.# To disable cloud-init's network configuration capabilities, write a file# /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following:# network: &#123;config: disabled&#125;network: ethernets: enp6s0: dhcp4: true enp7s0: dhcp4: no dhcp6: no version: 2 bridges: br0: interfaces: [enp7s0] dhcp4: no addresses: [192.168.40.241/24] gateway4: 192.168.40.1 nameservers: addresses: [114.114.114.114,8.8.8.8] 将宿主机原有网卡enp7s0挂到网桥br0上，并指定IP地址为192.168.40.241，nameservers指定DNS服务器。修改完后，通过sudo netplan apply重启网络服务生效，然后通过ifconfig查看，原来挂在enp7s0网卡下的IP现在挂到了br0上，宿主机及所有其它虚拟机都通过该网桥来与外部通讯。我们也可以通过brctl show来直观地查看， 1234567devuser@server_01:~$ brctl showbridge name bridge id STP enabled interfacesbr0 8000.2a5be3ec2698 no enp7s0docker0 8000.02424524dcce no veth580af8e veth74119f3 vethe7a2b0f vethfe89039 目前因为还没虚机，所以只有宿主机的网卡enp7s0挂在网桥br0上。同时也可以看到docker容器也是通过网桥docker0来通讯的。 2. 虚拟机安装2.1 安装虚拟机安装命令 123456sudo virt-install --name=dev-server1 --memory=16384,maxmemory=16384 \\--vcpus=4,maxvcpus=4 --os-type=linux --os-variant=rhel7 \\--location=/home/devuser/tools/CentOS-7-x86_64-DVD-1810.iso \\--disk path=/var/lib/libvirt/images/devserver1.img,size=300 \\--bridge=br0 --graphics=none --console=pty,target_type=serial \\--extra-args=\"console=tty0 console=ttyS0\" 其中–name指定虚机名称；–memory=16384,maxmemory=16384配置了16G内存；–vcpus=4,maxvcpus=4配置了4个CPU内核；centos7需要指定–os-variant=rhel7；–disk path=xx,size=300指定了磁盘路径与大小，这里是300G。 如果执行上述命令出现qemu-kvm: could not open &#39;xx/CentOS-7-x86_64-DVD-1810.iso&#39;: Permission denied异常退出时，可通过修改/etc/libvirt/qemu.conf文件将user = &quot;root&quot;，group = &quot;root&quot;前面的注释去掉解决（https://github.com/jedi4ever/veewee/issues/996） 如无问题，安装程序将出现如下配置界面 可通过输入选项对应的数字来选择不同的配置，依次操作如下步骤完成时区设置：输入2，回车，选择时区设置；输入1，回车，选择“Set timezone”；输入2，回车，选择“Asia”；回车，输入64，回车，选择“Shanghai” 然后进行安装设置，依次操作如下：输入5，回车，进入安装设置；输入c，回车，选择默认的磁盘进行安装；输入c，回车，使用默认的“2) Use All Space”；输入1，回车，选择“1) Standard Partition”进行标准分区；输入c，回车，完成分区设置 最后进入root密码设置，操作如下：输入8，回车，进入root密码设置；输入密码，回车；输入确认密码，回车 完成上述设置后，输入b开始进行安装 等待一段时间后，安装程序停在如下界面 按回车继续，最后输入用户名root，及前面设置的密码登录系统 2.2 虚拟机网络配置虚拟机安装完后，是没有分配IP的，我们通过ip a命令查看， 这时候的eth0下面空空如也，什么都没有。在/etc/sysconfig/network-scripts/ifcfg-eth0文件中添加如下内容 1234567891011121314151617181920[root@localhost ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=static #静态指定IPDEFROUTE=yes#IPV4_FAILURE_FATAL=no#IPV6INIT=yes#IPV6_AUTOCONF=yes#IPV6_DEFROUTE=yes#IPV6_FAILURE_FATAL=no#IPV6_ADDR_GEN_MODE=stable-privacyNAME=eth0UUID=449ed621-97a8-45b9-902f-0d347e27de98DEVICE=eth0ONBOOT=yes #开机自动启动IPADDR=192.168.40.96NETMASK=255.255.255.0GATEWAY=192.168.40.1DNS1=192.168.40.1 并通过systemctl restart network重启网络生效，这时候再运行ip a查看，eth0下面已经有配置的IP了。不出意外的话，局域网其它主机就可以通过该IP来远程SSH连接了。 这时候我们再通过brctl show来查看网桥挂载情况，br0下面已经多了一个vnet0虚拟网卡了。 123456789devuser@server_01:~$ brctl showbridge name bridge id STP enabled interfacesbr0 8000.2a5be3ec2698 no enp7s0 vnet0docker0 8000.02424524dcce no veth580af8e veth74119f3 vethd270ee8 vethe7a2b0f vethfe89039 虚拟机装完后，默认的hostname是localhost，针对centos7，我们可以通过如下命令来修改hostname 1[root@localhost ~]# hostnamectl set-hostname dev-server1 然后在/etc/hosts文件中添加127.0.0.1的host映射 dev-server1 123[root@localhost ~]# vi /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 dev-server1::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 Note: 如果出现虚拟机中无法访问外网，外部主机也无法ping通虚拟机的情况，则尝试如下处理 向文件/etc/sysctl.conf添加以下代码，禁用网络过滤器: 123net.bridge.bridge-nf-call-ip6tables = 0net.bridge.bridge-nf-call-iptables = 0net.bridge.bridge-nf-call-arptables = 0 重新加载kernel参数： 123456devuser@server_01: sudo sysctl -pnet.ipv4.ip_forward = 0...net.bridge.bridge-nf-call-ip6tables = 0net.bridge.bridge-nf-call-iptables = 0net.bridge.bridge-nf-call-arptables = 0 3. 虚拟机管理 列出当前运行的虚拟机virsh list 1234devuser@server_01:~$ virsh list Id Name State---------------------------------------------------- 5 dev-server1 running 如果列出所有的，则virsh list --all 从宿主机进入虚拟机virsh console，后面接虚拟机ID或名称 12345678devuser@server_01:~$ virsh console 5Connected to domain dev-server1Escape character is ^]CentOS Linux 7 (Core)Kernel 3.10.0-957.el7.x86_64 on an x86_64dev-server1 login: 输入用户名，密码即可登录虚拟机，按Ctrl+]可退出。 启动与关闭虚拟机virsh start|shutdown 12345devuser@cserver_01:~$ virsh start dev-server1Domain dev-server1 starteddevuser@server_01:~$ virsh shutdown 5Domain 5 is being shutdown libvirtd启动时，自动启动虚拟机 12devuser@server_01:~$ virsh autostart dev-server1Domain dev-server1 marked as autostarted 挂起/恢复虚拟机 12devuser@server_01:~$ virsh suspend dev-server1 # 挂起虚拟机devuser@server_01:~$ virsh resume dev-server1 # 恢复挂起的虚拟机 销毁虚拟机 1devuser@server_01:~$ virsh undefine dev-server1 # 彻底销毁虚拟机，会删除虚拟机配置文件，但不会删除虚拟磁盘 我的个人博客地址：http://blog.jboost.cn 我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号，欢迎关注，及时获取更新内容） ——————————————————————————————————————————————————————————————— ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.jboost.cn/categories/DevOps/"}],"tags":[{"name":"kvm","slug":"kvm","permalink":"http://blog.jboost.cn/tags/kvm/"}]},{"title":"软件项目研发流程该怎么规范","slug":"project-process","date":"2019-07-08T11:14:47.000Z","updated":"2020-08-26T01:45:42.915Z","comments":true,"path":"project-process.html","link":"","permalink":"http://blog.jboost.cn/project-process.html","excerpt":"在软件项目研发管理过程中，是否经常出现这样的场景：开发人员不知道什么时候转测；项目经理拿个Excel文档群里一发，某任务前天就应该完成的，怎么现在还没开始搞；前端问这部分UI是谁在做，什么时候能做完；测试说线上这个bug又是谁改出来的，这次没转测这模块……等等。整个协作感觉一团乱麻，团队内部充满了甩锅与抱怨的氛围。软件项目的研发流程该怎么规范，让团队成员都能目标明确，步调一致，让产品迭代充满节奏感。本文基于笔者项目研发管理经验整理，希望起到抛砖引玉的作用，探讨高效团队的协作流程模式。","text":"在软件项目研发管理过程中，是否经常出现这样的场景：开发人员不知道什么时候转测；项目经理拿个Excel文档群里一发，某任务前天就应该完成的，怎么现在还没开始搞；前端问这部分UI是谁在做，什么时候能做完；测试说线上这个bug又是谁改出来的，这次没转测这模块……等等。整个协作感觉一团乱麻，团队内部充满了甩锅与抱怨的氛围。软件项目的研发流程该怎么规范，让团队成员都能目标明确，步调一致，让产品迭代充满节奏感。本文基于笔者项目研发管理经验整理，希望起到抛砖引玉的作用，探讨高效团队的协作流程模式。 1. 协作流程图 基本原则： 所有问题可跟踪 （需求、Bug、优化） 所有工作透明化 （工作量、进展、Block因素） 2. 各阶段内容详解2.1. 需求收集确认本阶段主要是与产品经理相关的活动内容： 产品经理在每次版本开始之前定期收集各方需求，包括客户反馈、领导意见（对很多中小企业来说，老板就是最大的“用户”）、市场调研及技术团队需求等来源，输出需求列表 在版本开始之前召开版本计划会议，参与者包括项目经理、产品经理，及项目核心成员，按优先级梳理需求列表，输出下次版本的初步任务列表（之所以说初步，是因为该列表后面可能根据评审情况进行调整） 产品经理基于初步任务列表完成详细需求文档，组织团队成员——包括相关UI、开发、测试，召开 需求评审会议，输出评审意见及修正完成时间 产品经理针对需求评审会议中团队提出的意见建议，在修正完成时间内及时修正需求文档，并及时通知团队相关成员，输出确定的需求文档 注：可在需求评审会议后，进行任务的初步认领分配与时间估算，初步确定转测、上线时间节点 2.2. 设计开发 项目经理根据需求文档完成任务拆解，并在任务管理系统中创建对应任务单，指定经办人 各经办人认领任务后，根据自身任务的期限，及时与依赖方沟通，确定依赖任务的完成时间，以免影响自身任务进度，存在问题及时向项目经理反馈。 UI设计完成后，相关开发人员与产品经理需对UI设计进行确认，如果涉及内容较多，可组织UI评审会议（由产品经理或项目经理权衡组织） 涉及流程的开发任务需要有必要的设计，技术相关负责人负责对设计review，没有review的设计不能开发；任务开发完成需要进行代码review 项目经理定期组织项目例会（紧急版本建议每天一次，较长期版本建议一周一次或两次），持续跟进任务进度与问题，并及时协调处理，以保障进度预期 在预定转测时间节点前一天，开发人员编写转测文档，描述本次版本调整内容（附上任务列表）及注意事项，并通知项目相关人员（钉钉群或邮件） 2.3. 测试 需求评审会议后，测试人员需对各功能模块编写测试用例文档，并在转测前组织测试评审会议，对各功能各环节进行复核与查漏补缺 一次版本任务可根据情况分批测试，并确定每轮转测的内容与时间节点；分批测试完成后，需在上线前进行集成测试，注意预留一定的时间用于问题修复 测试完成，需要将测试结论通报项目相关人员（钉钉群或邮件），包括遗留问题与是否达到上线要求结论 注：产品经理可在转测后对开发实现进行验收，以确定开发是否符合需求实际，以便及时进行调整 2.4. 上线 上线人员需在上线前编写上线方案文档，记录此次上线内容，并对此次上线操作进行推演，对所涉及的所有操作按步骤进行记录，如数据库操作，代码merge，jenkins构建等；对可能存在的问题进行备注及对应的处理方案，并提交技术相关负责人review 项目经理结合测试结论及其它各方面情况，决策是否上线，并将意见通知到项目相关人员（钉钉群或邮件） 上线人员按照上线方案文档记录的步骤，依次完成上线操作（上线操作最好至少由两人完成，一人操作，一人检视，避免出错） 上线完成后，测试人员与产品经理对此次上线进行线上验证，确保线上功能流程无问题 验证无误后，由项目经理或其他指定负责人将上线通知发布至利益相关者，包括项目团队所有成员及相关合作方，说明上线时间、上线内容、影响因素、注意事项等（即时通讯群或邮件） 2.5. 复盘 版本结束后，项目经理根据情况对上个周期组织复盘总结会，总结存在的问题与原因，及后续规避的办法，总结积累的经验等 以上各阶段并不是完全串行推进的，相互之间存在一些穿插，比如下一版本需求的收集整理与当前版本的开发是并行推进的，开发与测试也可以以分阶段转测的形式并行推进，等等。 3. 一些常用工具 jira 用于项目任务管理，其中Agile插件可方便查看整体任务面板，对任务状态一目了然，需要求团队成员养成及时更新状态的习惯 confluence 文档管理，用于各类文档的集中化维护，以上所述的如需求文档、开发设计文档、转测文档、上线文档等均可使用confluence以项目空间的形式集中化管理。 gitlab 代码管理 jenkins 项目部署构建工具 nexus 搭建maven私有库 4. 总结团队工作讲求步调与节奏，好的流程与规范可以让一个水平一般的人也能充分发挥其作用，从而让团队整体稳步前进，高效产出。而不好的流程，或根本不重视流程的团队，却往往一盘散沙，甩锅与抱怨充斥，战斗力低下。本文以相对较粗粒度对软件项目的基本流程管理做了介绍，更细节的内容可能需要团队根据内部具体情况进行相应处理与对待。链接： https://pan.baidu.com/s/1WBHsIWoquKTQHJ6IaSql3Q 是笔者基于以前团队敏捷项目管理及一些具体问题的思考分享PPT，供参考。提取码：awya 我的个人博客地址：http://blog.jboost.cn 我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号，欢迎关注，及时获取更新内容） ——————————————————————————————————————————————————————————————— ![微信公众号](/assets/qrcode-05.jpg)","categories":[{"name":"Teamwork","slug":"Teamwork","permalink":"http://blog.jboost.cn/categories/Teamwork/"}],"tags":[{"name":"agile","slug":"agile","permalink":"http://blog.jboost.cn/tags/agile/"}]},{"title":"线程池的基本原理，看完就懂了","slug":"threadpool","date":"2019-07-05T09:32:30.000Z","updated":"2020-08-26T01:34:28.985Z","comments":true,"path":"threadpool.html","link":"","permalink":"http://blog.jboost.cn/threadpool.html","excerpt":"本文内容是基于研发部门内部的分享整理，记录下来供学习或回顾。","text":"本文内容是基于研发部门内部的分享整理，记录下来供学习或回顾。 1. 为什么要用线程池 降低资源消耗。通过重复利用已创建的线程降低线程创建、销毁线程造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配、调优和监控 2. ThreadPoolExecutor线程池类参数详解 参数 说明 corePoolSize 核心线程数量，线程池维护线程的最少数量 maximumPoolSize 线程池维护线程的最大数量 keepAliveTime 线程池除核心线程外的其他线程的最长空闲时间，超过该时间的空闲线程会被销毁 unit keepAliveTime的单位，TimeUnit中的几个静态属性：NANOSECONDS、MICROSECONDS、MILLISECONDS、SECONDS workQueue 线程池所使用的任务缓冲队列 threadFactory 线程工厂，用于创建线程，一般用默认的即可 handler 线程池对拒绝任务的处理策略 当线程池任务处理不过来的时候（什么时候认为处理不过来后面描述），可以通过handler指定的策略进行处理，ThreadPoolExecutor提供了四种策略： ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常；也是默认的处理方式。 ThreadPoolExecutor.DiscardPolicy：丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 可以通过实现RejectedExecutionHandler接口自定义处理方式。 3. 线程池任务执行3.1. 添加执行任务 submit() 该方法返回一个Future对象，可执行带返回值的线程；或者执行想随时可以取消的线程。Future对象的get()方法获取返回值。Future对象的cancel(true/false)取消任务，未开始或已完成返回false，参数表示是否中断执行中的线程 execute() 没有返回值。 3.2. 线程池任务提交过程一个线程提交到线程池的处理流程如下图 如果此时线程池中的数量小于corePoolSize，即使线程池中的线程都处于空闲状态，也要创建新的线程来处理被添加的任务。 如果此时线程池中的数量等于corePoolSize，但是缓冲队列workQueue未满，那么任务被放入缓冲队列。 如果此时线程池中的数量大于等于corePoolSize，缓冲队列workQueue满，并且线程池中的数量小于maximumPoolSize，建新的线程来处理被添加的任务。 如果此时线程池中的数量大于corePoolSize，缓冲队列workQueue满，并且线程池中的数量等于maximumPoolSize，那么通过 handler所指定的策略来处理此任务。 当线程池中的线程数量大于 corePoolSize时，如果某线程空闲时间超过keepAliveTime，线程将被终止。这样，线程池可以动态的调整池中的线程数。 总结即：处理任务判断的优先级为 核心线程corePoolSize、任务队列workQueue、最大线程maximumPoolSize，如果三者都满了，使用handler处理被拒绝的任务。 ** 注意：** 当workQueue使用的是无界限队列时，maximumPoolSize参数就变的无意义了，比如new LinkedBlockingQueue(),或者new ArrayBlockingQueue(Integer.MAX_VALUE); 使用SynchronousQueue队列时由于该队列没有容量的特性，所以不会对任务进行排队，如果线程池中没有空闲线程，会立即创建一个新线程来接收这个任务。maximumPoolSize要设置大一点。 核心线程和最大线程数量相等时keepAliveTime无作用. 3.3. 线程池关闭 shutdown() 不接收新任务,会处理已添加任务 shutdownNow() 不接受新任务,不处理已添加任务,中断正在处理的任务 4. 常用队列介绍 ArrayBlockingQueue： 这是一个由数组实现的容量固定的有界阻塞队列. SynchronousQueue： 没有容量，不能缓存数据；每个put必须等待一个take; offer()的时候如果没有另一个线程在poll()或者take()的话返回false。 LinkedBlockingQueue： 这是一个由单链表实现的默认无界的阻塞队列。LinkedBlockingQueue提供了一个可选有界的构造函数，而在未指明容量时，容量默认为Integer.MAX_VALUE。 队列操作: 方法 说明 add 增加一个元索; 如果队列已满，则抛出一个异常 remove 移除并返回队列头部的元素; 如果队列为空，则抛出一个异常 offer 添加一个元素并返回true; 如果队列已满，则返回false poll 移除并返回队列头部的元素; 如果队列为空，则返回null put 添加一个元素; 如果队列满，则阻塞 take 移除并返回队列头部的元素; 如果队列为空，则阻塞 element 返回队列头部的元素; 如果队列为空，则抛出一个异常 peek 返回队列头部的元素; 如果队列为空，则返回null 5. Executors线程工厂类 Executors.newCachedThreadPool();说明: 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程.内部实现：new ThreadPoolExecutor(0,Integer.MAX_VALUE,60L,TimeUnit.SECONDS,new SynchronousQueue()); Executors.newFixedThreadPool(int);说明: 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。内部实现：new ThreadPoolExecutor(nThreads, nThreads,0L,TimeUnit.MILLISECONDS,new LinkedBlockingQueue()); Executors.newSingleThreadExecutor();说明:创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照顺序执行。内部实现：new ThreadPoolExecutor(1,1,0L,TimeUnit.MILLISECONDS,new LinkedBlockingQueue()) Executors.newScheduledThreadPool(int);说明:创建一个定长线程池，支持定时及周期性任务执行。内部实现：new ScheduledThreadPoolExecutor(corePoolSize) ** 【附】阿里巴巴Java开发手册中对线程池的使用规范 **2. 【强制】创建线程或线程池时请指定有意义的线程名称，方便出错时回溯。正例： 123456public class TimerTaskThread extends Thread &#123; public TimerTaskThread()&#123; super.setName(\"TimerTaskThread\"); ... &#125;&#125; 【强制】线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。说明： 使用线程池的好处是减少在创建和销毁线程上所花的时间以及系统资源的开销，解决资源不足的问题。如果不使用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者“过度切换”的问题。 【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。说明： Executors 返回的线程池对象的弊端如下：1） FixedThreadPool 和 SingleThreadPool:允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。2） CachedThreadPool 和 ScheduledThreadPool:允许的创建线程数量为 Integer.MAX_VALUE， 可能会创建大量的线程，从而导致 OOM。 6. 总结ThreadPoolExecutor通过几个核心参数来定义不同类型的线程池，适用于不同的使用场景；其中在任务提交时，会依次判断corePoolSize， workQueque， 及maximumPoolSize，不同的状态不同的处理。技术领域水太深，如果不是日常使用，基本一段时间后某些知识点就忘的差不多了，因此阶段性地回顾与总结，对夯实自己的技术基础很有必要。我的个人博客地址：http://blog.jboost.cn我的github地址：https://github.com/ronwxy我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号，欢迎关注，及时获取更新内容）———————————————————————————————————————————————————————————————","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.jboost.cn/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"concurrency","slug":"concurrency","permalink":"http://blog.jboost.cn/tags/concurrency/"}]},{"title":"Spring Boot从入门到实战（九）：统一异常处理","slug":"springboot-error","date":"2019-07-03T06:35:02.000Z","updated":"2019-07-22T10:30:36.823Z","comments":true,"path":"springboot-error.html","link":"","permalink":"http://blog.jboost.cn/springboot-error.html","excerpt":"都说管理的精髓就是“制度管人，流程管事”。而所谓流程，就是对一些日常工作环节、方式方法、次序等进行标准化、规范化。且不论精不精髓，在技术团队中，对一些通用场景，统一规范是必要的，只有步调一致，才能高效向前。如前后端交互协议，如本文探讨的异常处理。","text":"都说管理的精髓就是“制度管人，流程管事”。而所谓流程，就是对一些日常工作环节、方式方法、次序等进行标准化、规范化。且不论精不精髓，在技术团队中，对一些通用场景，统一规范是必要的，只有步调一致，才能高效向前。如前后端交互协议，如本文探讨的异常处理。 1. Spring Mvc中的异常处理在spring mvc中，跟异常处理的相关类大致如下 上图中，spring mvc中处理异常的类（包括在请求映射时与请求处理过程中抛出的异常），都是 HandlerExceptionResolver 接口的实现，并且都实现了 Ordered 接口。与拦截器链类似，如果容器中存在多个实现了 HandlerExceptionResolver 接口的异常处理类，则它们的 resolveException 方法会被依次调用，顺序由order决定，值越小的先执行，只要其中一个调用返回不是null，则后续的异常处理将不再执行。 各实现类简单介绍如下： DefaultHandlerExceptionResolver： 这个是默认实现，处理Spring定义的各种标准异常，将其转换为对应的Http Status Code，具体处理的异常参考 doResolveException 方法 ResponseStatusExceptionResolver：用来支持@ResponseStatus注解使用的实现，如果自定义的异常通过@ResponseStatus注解进行了修饰，并且容器中存在ResponseStatusExceptionResolver的bean，则自定义异常抛出时会被该bean进行处理，返回注解定义的Http Status Code及内容给客户端 ExceptionHandlerExceptionResolver：用来支持@ExceptionHandler注解使用的实现，使用该注解修饰的方法来处理对应的异常。不过该注解的作用范围只在controller类，如果需要全局处理，则需要配合@ControllerAdvice注解使用。 SimpleMappingExceptionResolver：将异常映射为视图 HandlerExceptionResolverComposite：就是各类实现的组合，依次执行，只要其中一个处理返回不为null，则不再处理。 因为本文主要是对spring boot如何对异常统一处理进行探讨，所以以上只对各实现做了基本介绍，更加详细的内容可查阅相关文档或后续再补上。 2. Spring Boot中如何统一异常处理通过第一部分介绍，可以使用@ExceptionHandler + @ControllerAdvice 组合的方式来实现异常的全局统一处理。对于REST服务来说，spring mvc提供了一个抽象类 ResponseEntityExceptionHandler， 该类类似于上面介绍的 DefaultHandlerExceptionResolver，对一些标准的异常进行了处理，但不是返回 ModelAndView对象， 而是返回 ResponseEntity对象。故我们可以基于该类来实现REST服务异常的统一处理定义异常处理类 BaseWebApplicationExceptionHandler 如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@RestControllerAdvicepublic class BaseWebApplicationExceptionHandler extends ResponseEntityExceptionHandler &#123; private boolean includeStackTrace; public BaseWebApplicationExceptionHandler(boolean includeStackTrace)&#123; super(); this.includeStackTrace = includeStackTrace; &#125; private final Logger logger = LoggerFactory.getLogger(getClass()); @ExceptionHandler(BizException.class) public ResponseEntity&lt;Object&gt; handleBizException(BizException ex) &#123; logger.warn(\"catch biz exception: \" + ex.toString(), ex.getCause()); return this.asResponseEntity(HttpStatus.valueOf(ex.getHttpStatus()), ex.getErrorCode(), ex.getErrorMessage(), ex); &#125; @ExceptionHandler(&#123;IllegalArgumentException.class, IllegalStateException.class&#125;) public ResponseEntity&lt;Object&gt; handleIllegalArgumentException(Exception ex) &#123; logger.warn(\"catch illegal exception.\", ex); return this.asResponseEntity(HttpStatus.BAD_REQUEST, HttpStatus.BAD_REQUEST.name().toLowerCase(), ex.getMessage(), ex); &#125; @ExceptionHandler(Exception.class) public ResponseEntity&lt;Object&gt; handleException(Exception ex) &#123; logger.error(\"catch exception.\", ex); return this.asResponseEntity(HttpStatus.INTERNAL_SERVER_ERROR, HttpStatus.INTERNAL_SERVER_ERROR.name().toLowerCase(), ExceptionConstants.INNER_SERVER_ERROR_MSG, ex); &#125; protected ResponseEntity&lt;Object&gt; handleExceptionInternal( Exception ex, @Nullable Object body, HttpHeaders headers, HttpStatus status, WebRequest request) &#123; if (HttpStatus.INTERNAL_SERVER_ERROR.equals(status)) &#123; request.setAttribute(WebUtils.ERROR_EXCEPTION_ATTRIBUTE, ex, WebRequest.SCOPE_REQUEST); &#125; logger.warn(\"catch uncustom exception.\", ex); return this.asResponseEntity(status, status.name().toLowerCase(), ex.getMessage(), ex); &#125; protected ResponseEntity&lt;Object&gt; asResponseEntity(HttpStatus status, String errorCode, String errorMessage, Exception ex) &#123; Map&lt;String, Object&gt; data = new LinkedHashMap&lt;&gt;(); data.put(BizException.ERROR_CODE, errorCode); data.put(BizException.ERROR_MESSAGE, errorMessage); //是否包含异常的stack trace if(includeStackTrace)&#123; addStackTrace(data, ex); &#125; return new ResponseEntity&lt;&gt;(data, status); &#125; private void addStackTrace(Map&lt;String, Object&gt; errorAttributes, Throwable error) &#123; StringWriter stackTrace = new StringWriter(); error.printStackTrace(new PrintWriter(stackTrace)); stackTrace.flush(); errorAttributes.put(BizException.ERROR_TRACE, stackTrace.toString()); &#125;&#125; 这里有几点： 定义了一个includeStackTrace变量，来控制是否输出异常栈信息 自定义了一个异常类BizException，表示可预知的业务异常，并对它提供了处理方法，见handleBizException方法 对其它未预知异常，用Exception类型进行最后处理，见handleException方法 重写了超类的handleExceptionInternal方法，统一响应内容的字段与格式 针对REST服务，使用的是@RestControllerAdvice注解，而不是@ControllerAdvice BaseWebApplicationExceptionHandler是通过增强的方式对controller抛出的异常做了统一处理，那如果请求都没有到达controller怎么办，比如在过滤器那边就抛异常了，Spring Boot其实对错误的处理做了一些自动化配置，参考ErrorMvcAutoConfiguration类，具体这里不详述，只提出方案——自定义ErrorAttributes实现，如下所示 1234567891011public class BaseErrorAttributes extends DefaultErrorAttributes &#123; private boolean includeStackTrace; @Override public Map&lt;String, Object&gt; getErrorAttributes(WebRequest webRequest, boolean includeStackTrace) &#123; Map&lt;String, Object&gt; errorAttributes = new LinkedHashMap&lt;String, Object&gt;(); addStatus(errorAttributes, webRequest); addErrorDetails(errorAttributes, webRequest, this.includeStackTrace); return errorAttributes; &#125; 以上只列出了主要部分，具体实现可参考源码。这里同样定义了includeStackTrace来控制是否包含异常栈信息。 最后，将以上两个实现通过配置文件注入容器，如下： 123456789101112131415161718192021222324252627282930313233@Configuration@ConditionalOnClass(&#123;Servlet.class, DispatcherServlet.class, WebMvcConfigurer.class&#125;)@ConditionalOnMissingBean(ResponseEntityExceptionHandler.class)@AutoConfigureBefore(ErrorMvcAutoConfiguration.class)public class ExceptionHandlerAutoConfiguration &#123; @Profile(&#123;\"test\", \"formal\", \"prod\"&#125;) @Bean public ResponseEntityExceptionHandler defaultGlobalExceptionHandler() &#123; //测试、正式环境，不输出异常的stack trace return new BaseWebApplicationExceptionHandler(false); &#125; @Profile(&#123;\"default\",\"local\",\"dev\"&#125;) @Bean public ResponseEntityExceptionHandler devGlobalExceptionHandler() &#123; //本地、开发环境，输出异常的stack trace return new BaseWebApplicationExceptionHandler(true); &#125; @Profile(&#123;\"test\", \"formal\", \"prod\"&#125;) @Bean public ErrorAttributes basicErrorAttributes() &#123; //测试、正式环境，不输出异常的stack trace return new BaseErrorAttributes(false); &#125; @Profile(&#123;\"default\",\"local\",\"dev\"&#125;) @Bean public ErrorAttributes devBasicErrorAttributes() &#123; //本地、开发环境，输出异常的stack trace return new BaseErrorAttributes(true); &#125;&#125; 上面的@Profile主要是控制针对不同环境，输出不同的响应内容。以上配置的意思是在profile为default、local、dev时，响应内容中包含异常栈信息；profile为test、formal、prod时，响应内容不包含异常栈信息。这么做的好处是，开发阶段，当前端联调时，如果出错，可直接从响应内容中看到异常栈，方便服务端开发人员快速定位问题，而测试、生产环境， 就不要返回异常栈信息了。 3. 基于Spring Boot的异常处理规范 异常的表示形式异常一般可通过自定义异常类，或定义异常的信息，比如code，message之类，然后通过一个统一的异常类进行封装。如果每一种异常都定义一个异常类，则会造成异常类过多，所以实践开发中我一般倾向于后者。可以定义一个接口，该接口主要是方便后面的异常处理工具类实现 12345public interface BaseErrors &#123; String getCode(); String getMsg();&#125; 然后定义一个枚举，实现该接口，在该枚举中定义异常信息，如 123456789101112131415161718public enum ErrorCodeEnum implements BaseErrors &#123; qrcode_existed(\"该公众号下已存在同名二维码\"), authorizer_notexist(\"公众号不存在\"), private String msg; private ErrorCodeEnum(String msg) &#123; this.msg = msg; &#125; public String getCode() &#123; return name(); &#125; public String getMsg() &#123; return msg; &#125;&#125; 封装异常处理分场景定义了ClientSideException，ServerSideException，UnauthorizedException，ForbiddenException异常，分别表示客户端异常（400），服务端异常（500），未授权异常（401），禁止访问异常（403），如ClientSideException定义 12345678910public class ClientSideException extends BizException &#123; public &lt;E extends Enum&lt;E&gt; &amp; BaseErrors&gt; ClientSideException(E exceptionCode, Throwable cause) &#123; super(HttpStatus.BAD_REQUEST, exceptionCode, cause); &#125; public &lt;E extends Enum&lt;E&gt; &amp; BaseErrors&gt; ClientSideException(E exceptionCode) &#123; super(HttpStatus.BAD_REQUEST, exceptionCode, null); &#125;&#125; 并且提供一个异常工具类ExceptionUtil，方便不同场景使用， rethrowClientSideException：抛出ClientSideException，将以status code 400返回客户端。由客户端引起的异常调用该方法，如参数校验失败。 rethrowUnauthorizedException： 抛出UnauthorizedException，将以status code 401返回客户端。访问未授权时调用，如token校验失败等。 rethrowForbiddenException： 抛出ForbidenException，将以status code 403返回客户端。访问被禁止时调用，如用户被禁用等。 rethrowServerSideException： 抛出ServerSideException，将以status code 500返回客户端。服务端引起的异常调用该方法，如调用第三方服务异常，数据库访问出错等。 在实际使用时，分两种情况， 不通过try/catch主动抛出异常，如： 1234if (StringUtils.isEmpty(appId)) &#123; LOG.warn(\"the authorizer for site[&#123;&#125;] is not existed.\", templateMsgRequestDto.getSiteId()); ExceptionUtil.rethrowClientSideException(ErrorCodeEnum.authorizer_notexist);&#125; 通过try/catch异常重新抛出（注意：可预知的异常，需要给客户端返回某种提示信息的，必须通过该方式重新抛出。否则将返回统一的code 500,提示“抱歉，服务出错了，请稍后重试”的提示信息）如： 1234567try &#123; String result = wxOpenService.getWxOpenComponentService().getWxMpServiceByAppid(appId).getTemplateMsgService().sendTemplateMsg(templateMessage); LOG.info(\"result: &#123;&#125;\", result);&#125; catch (WxErrorException wxException) &#123; //这里不需要打日志，会统一在异常处理里记录日志 ExceptionUtil.rethrowServerSideException(ExceptionCodeEnum.templatemsg_fail, wxException);&#125; 具体实现参考源码： https://github.com/ronwxy/base-spring-boot/tree/master/spring-boot-autoconfigure/src/main/java/cn/jboost/springboot/autoconfig/error另附demo源码：https://github.com/ronwxy/springboot-demos/tree/master/springboot-error 4. 总结本文写完感觉信息量有点多，对于不具备一定基础的人来说理解可能有点难度。如果有任何疑问，欢迎交流。后续有需要的话也可以针对某个环节再进行细化补充。本文所提的规范不一定是最好的实践，但规范或流程的管理，都是遵循先僵化，后优化，再固化的步骤，先解决有没有的问题，再解决好不好的问题。我的个人博客地址：http://blog.jboost.cn我的github地址：https://github.com/ronwxy我的微信公众号：jboost-ksxy （一个不只有技术干货的公众号，欢迎关注）———————————————————————————————————————————————————————————————欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.jboost.cn/categories/SpringBoot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"}]},{"title":"使用nvm来管理Node多版本","slug":"use-nvm","date":"2019-07-02T00:41:51.000Z","updated":"2020-08-26T01:34:06.707Z","comments":true,"path":"use-nvm.html","link":"","permalink":"http://blog.jboost.cn/use-nvm.html","excerpt":"最近在为前端配置jenkins持续集成环境时，在运行npm install下载依赖包的时候，速度极慢，而本地很快。对比node版本，一个v10.15.3，速度很快，一个v8.10.0，速度极慢。两者都设置了国内镜像。升级node能否解决问题？有没有工具支持node多版本管理，像python的anaconda一样？答案是有，叫nvm —— node version manager。","text":"最近在为前端配置jenkins持续集成环境时，在运行npm install下载依赖包的时候，速度极慢，而本地很快。对比node版本，一个v10.15.3，速度很快，一个v8.10.0，速度极慢。两者都设置了国内镜像。升级node能否解决问题？有没有工具支持node多版本管理，像python的anaconda一样？答案是有，叫nvm —— node version manager。 项目地址： https://github.com/nvm-sh/nvm 1. 安装linux下： 12345# 下载并执行安装curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | bash# 安装后执行source使其生效source ~/.bashrc 为了加速node的下载，可在~/.bashrc中添加 export NVM_NODEJS_ORG_MIRROR=https://npm.taobao.org/mirrors/node windows：参考 https://github.com/coreybutler/nvm-windows/releases 2. 使用 列出所有node版本nvm ls-remote 只列出长期支持版本，一般生产环境使用long term support版nvm ls-remote --lts 安装指定版本nvm install v10.15.3 安装完后即可查看安装的node及npm的版本 12node -v npm -v 查看已安装版本nvm ls 使用指定的版本，重连bash即失效nvm use 10.15.3 设置默认，重连也生效nvm alias default 10.15.3 配置npm国内淘宝镜像 123npm config set registry https://registry.npm.taobao.org --globalnpm config set disturl https://npm.taobao.org/dist --global 3. 总结nvm可在一个系统中非常便捷地管理多个node版本，并能自由切换使用哪个版本，方便需要多版本并存的场景。 我的个人博客地址：http://blog.jboost.cn 我的github地址：https://github.com/ronwxy 我的微信公众号：jboost-ksxy ———————————————————————————————————————— ![微信公众号](/assets/qrcode-05.jpg) 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.jboost.cn/categories/DevOps/"}],"tags":[{"name":"node","slug":"node","permalink":"http://blog.jboost.cn/tags/node/"},{"name":"npm","slug":"npm","permalink":"http://blog.jboost.cn/tags/npm/"}]},{"title":"redission-tomcat 快速实现从单机部署到多机部署","slug":"session-redis","date":"2019-06-29T01:01:24.000Z","updated":"2020-08-26T01:35:25.388Z","comments":true,"path":"session-redis.html","link":"","permalink":"http://blog.jboost.cn/session-redis.html","excerpt":"一些项目初期出于简单快速，都是做单机开发与部署，但是随着业务的扩展或对可用性要求的提高，单机环境已不满足需求。单机部署往多机部署切换，其中可能存在的一个重要环节就是session的共享（如果一开始就是基于token的认证则可忽略）。本文介绍一个基于redis的tomcat session管理开源项目：redission-tomcat，可无代码侵入式地快速实现session共享。","text":"一些项目初期出于简单快速，都是做单机开发与部署，但是随着业务的扩展或对可用性要求的提高，单机环境已不满足需求。单机部署往多机部署切换，其中可能存在的一个重要环节就是session的共享（如果一开始就是基于token的认证则可忽略）。本文介绍一个基于redis的tomcat session管理开源项目：redission-tomcat，可无代码侵入式地快速实现session共享。 1. 简介redisson是与jedis类似的一个redis客户端，其功能比jedis要更丰富一些。redission-tomcat是一个基于redis的tomcat session管理器项目，项目地址：https://github.com/redisson/redisson/tree/master/redisson-tomcat 。相比于其它实现，该项目的存储更为高效，写操作也更为优化。每一个session参数是在调用HttpSession.setAttribute时写入redis的，其它方案却一般是每次都将整个session进行序列化后写入。 2. 使用 将redisson-all-3.11.0.jar，redisson-tomcat-8-3.11.0.jar（针对tomcat8，其它版本可在上述项目地址页面找到下载链接）两个jar包下载放到tomcat的lib目录下。 在tomcat conf目录下的context.xml文件中添加如下配置 123&lt;Manager className=\"org.redisson.tomcat.RedissonSessionManager\"configPath=\"$&#123;catalina.base&#125;/conf/redisson.conf\" readMode=\"MEMORY\" updateMode=\"AFTER_REQUEST\" broadcastSessionEvents=\"false\"/&gt; 其中 configPath：指向Redisson的json或yaml格式的配置文件，第3步中给出。 readMode：session属性的读取模式。可取值 1. MEMORY, 该模式会将session属性同时保存到本地tomcat session与redis中，后续的session更新通过redis事件传播到本地tomcat session；2. REDIS，只将session属性保存到redis中。默认为REDIS。 updateMode：session属性的更新模式。可取值 1. DEFAULT，session属性只通过setAttribute方法保存到redis中；2. AFTER_REQUEST，在每次请求之后，将所有session属性保存至redis。默认为DEFAULT。 broadcastSessionEvents：如果设置为true，则sessionCreated与sessionDestroyed事件将会被广播到所有tomcat实例，并使所有注册的HttpSessionListeners监听器被触发。默认为false。 在tomcat conf目录下新增配置文件redisson.conf，内容如下12345678910111213141516171819202122232425&#123; \"singleServerConfig\":&#123; \"idleConnectionTimeout\":10000, \"connectTimeout\":10000, \"timeout\":3000, \"retryAttempts\":3, \"retryInterval\":1500, \"password\":\"123456\", \"subscriptionsPerConnection\":5, \"clientName\":null, \"address\": \"redis://127.0.0.1:6379\", \"subscriptionConnectionMinimumIdleSize\":1, \"subscriptionConnectionPoolSize\":50, \"connectionMinimumIdleSize\":24, \"connectionPoolSize\":64, \"database\":0, \"dnsMonitoringInterval\":5000 &#125;, \"threads\":16, \"nettyThreads\":32, \"codec\":&#123; \"class\":\"org.redisson.codec.FstCodec\" &#125;, \"transportMode\":\"NIO\"&#125; 以上为单机模式redis环境配置，其中password，address修改为自己的值。如果是集群模式，则配置文件为12345678910111213141516171819202122232425262728293031323334353637&#123; \"sentinelServersConfig\":&#123; \"idleConnectionTimeout\":10000, \"connectTimeout\":10000, \"timeout\":3000, \"retryAttempts\":3, \"retryInterval\":1500, \"failedSlaveReconnectionInterval\":3000, \"failedSlaveCheckInterval\":60000, \"password\":null, \"subscriptionsPerConnection\":5, \"clientName\":null, \"loadBalancer\":&#123; \"class\":\"org.redisson.connection.balancer.RoundRobinLoadBalancer\" &#125;, \"subscriptionConnectionMinimumIdleSize\":1, \"subscriptionConnectionPoolSize\":50, \"slaveConnectionMinimumIdleSize\":24, \"slaveConnectionPoolSize\":64, \"masterConnectionMinimumIdleSize\":24, \"masterConnectionPoolSize\":64, \"readMode\":\"SLAVE\", \"subscriptionMode\":\"SLAVE\", \"sentinelAddresses\":[ \"redis://127.0.0.1:26379\", \"redis://127.0.0.1:26389\" ], \"masterName\":\"mymaster\", \"database\":0 &#125;, \"threads\":16, \"nettyThreads\":32, \"codec\":&#123; \"class\":\"org.redisson.codec.FstCodec\" &#125;, \"transportMode\":\"NIO\"&#125; 我们可以使用nginx来实现负载均衡，参考配置1234567891011121314151617upstream cnserver&#123; server 127.0.0.1:8080 weight&#x3D;2 fail_timeout&#x3D;10s max_fails&#x3D;1; server 127.0.0.1:8081 weight&#x3D;2 fail_timeout&#x3D;10s max_fails&#x3D;1;&#125;server &#123; listen 80; server_name localhost; index index.html index.htm; location &#x2F;rest&#x2F; &#123; index index.html; proxy_pass http:&#x2F;&#x2F;cnserver&#x2F;rest&#x2F;; &#125;&#125; 以上即为使用redisson-tomcat来实现单机部署到多机部署的所有配置。 3. 总结技术架构都是随着业务的发展而不断演进。在业务发展初期，用户量、业务复杂度都相对较低，为了实现快速上线验证，往往采用简单单一的架构。许多项目可能还没来得及进行架构演进升级就GG了，而有幸继续成长的项目必然会随着业务的扩张不断优化与升级。本文介绍的redisson-tomcat可帮助单机项目快速切换到多机支持，当然只是在session管理环节。如果涉及到其它如文件上传，定时任务等分布式支持，则要另做相应调整了。 我的个人博客地址：http://blog.jboost.cn 我的github地址：https://github.com/ronwxy 我的微信公众号：jboost-ksxy （一个不只有实战干货的技术公众号， 欢迎关注） ——————————————————————————————————————————————————————————————— ![微信公众号](/assets/qrcode-05.jpg) 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.jboost.cn/categories/Java/"}],"tags":[{"name":"session","slug":"session","permalink":"http://blog.jboost.cn/tags/session/"},{"name":"tomcat","slug":"tomcat","permalink":"http://blog.jboost.cn/tags/tomcat/"},{"name":"redission","slug":"redission","permalink":"http://blog.jboost.cn/tags/redission/"}]},{"title":"swagger api文档集中化注册管理","slug":"swagger-register","date":"2019-06-28T08:59:05.000Z","updated":"2020-08-26T01:35:00.634Z","comments":true,"path":"swagger-register.html","link":"","permalink":"http://blog.jboost.cn/swagger-register.html","excerpt":"接口文档是前后端开发对接时很重要的一个组件。手动编写接口文档既费时，又存在文档不能随代码及时更新的问题，因此产生了像swagger这样的自动生成接口文档的框架。swagger文档一般是随项目代码生成与更新，访问地址也是基于项目地址，因此对项目数不多的团队还好。如果团队的项目很多，比如采用微服务架构的团队，动则几十甚至上百个服务项目，那就意味着前端开发人员需要记住几十甚至上百个swagger文档地址，那就很不友好了。目前貌似还没有较流行的API文档集中化管理项目（也或者是我没找到），因此花了点时间自己集成了一个，介绍如下。","text":"接口文档是前后端开发对接时很重要的一个组件。手动编写接口文档既费时，又存在文档不能随代码及时更新的问题，因此产生了像swagger这样的自动生成接口文档的框架。swagger文档一般是随项目代码生成与更新，访问地址也是基于项目地址，因此对项目数不多的团队还好。如果团队的项目很多，比如采用微服务架构的团队，动则几十甚至上百个服务项目，那就意味着前端开发人员需要记住几十甚至上百个swagger文档地址，那就很不友好了。目前貌似还没有较流行的API文档集中化管理项目（也或者是我没找到），因此花了点时间自己集成了一个，介绍如下。 1. swagger-bootstrap-ui项目该项目是github上的一个开源项目（https://github.com/xiaoymin/swagger-bootstrap-ui ），对swagger ui做了增强，功能整体看起来要丰富一些。来看看效果， 该项目的调试url地址原本是基于自身服务的，我将它改为了注册服务的url地址，以支持注册服务的接口调试。调整后的源码地址： https://github.com/ronwxy/swagger-bootstrap-ui 2. swagger api注册服务该项目集成了swagger-bootstrap-ui，并提供了swagger api注册接口，接受所有提供了有效配置的服务项目注册，让注册的服务在一个页面上可统一查看，再也不用记太多文档地址了。 启动注册服务后，访问 http://localhost:11090/doc.html 打开文档页面。如上图，可通过下拉列表来选择不同项目，加载项目的接口文档查看或调试。项目地址： https://github.com/ronwxy/swagger-register （如果觉得有用，不要吝啬你的star，反正又不要钱，O(∩_∩)O） 3. 服务端配置在业务服务端，需要提供一些配置。首先，需要配置一些Bean，如下提供了一个配置类（这里只列出了主要部分，完整代码参考： https://github.com/ronwxy/base-spring-boot） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class Swagger2AutoConfiguration &#123; @Bean public Docket restApi() &#123; ParameterBuilder builder = new ParameterBuilder(); builder.name(\"x-auth-token\").description(\"授权token\") .modelRef(new ModelRef(\"string\")) .parameterType(\"header\") .required(false); return new Docket(DocumentationType.SWAGGER_2) .groupName(groupName) .select() .apis(RequestHandlerSelectors.basePackage(apisBasePackage)) .paths(PathSelectors.any()) .build() .globalOperationParameters(Collections.singletonList(builder.build())) .apiInfo(apiInfo()); &#125; @Profile(&#123;\"dev\"&#125;) @Bean public CommandLineRunner swaggerRegistar(ConfigurableApplicationContext context) &#123; return new SwaggerInfoRegistar(context); &#125; /** * use to register swagger api info url to swagger api registry; * * @author liubo */ public class SwaggerInfoRegistar implements CommandLineRunner &#123; @Override public void run(String... args) throws Exception &#123; String url = buildLocalSwaggerDocsUrl(); registerLocalSwaggerUrl(url); &#125; /** * register the v2/api-docs url * * @param url */ private void registerLocalSwaggerUrl(String url) &#123; RestTemplate restTemplate = new RestTemplate(); restTemplate.getMessageConverters().add(new FormHttpMessageConverter()); MultiValueMap&lt;String, Object&gt; body = new LinkedMultiValueMap&lt;&gt;(); body.add(\"project\", getApiTitle()); body.add(\"url\", url); ResponseEntity&lt;Map&gt; re = restTemplate.postForEntity(getSwaggerRegisterUrl(), body, Map.class); if (HttpStatus.OK.equals(re.getStatusCode())) &#123; logger.info(\"swagger api registered success to &#123;&#125;\", getSwaggerRegisterUrl()); &#125; else &#123; logger.warn(\"swagger api registered failed [&#123;&#125;]\", re.getBody().get(\"msg\")); &#125; &#125; &#125;&#125; 该类完成了swagger的基本配置，同时将swagger的/v2/api-docs地址注册到了步骤2中介绍的注册服务。 然后，因为要从注册服务端调用该业务服务的接口进行调试，存在跨域，因此服务需要做跨域支持，配置文件中添加如下定义， 1234567891011121314151617181920@Bean@ConditionalOnMissingBean(name = \"corsFilterRegistrationBean\")public FilterRegistrationBean corsFilterRegistrationBean() &#123; UrlBasedCorsConfigurationSource corsConfigurationSource = new UrlBasedCorsConfigurationSource(); CorsConfiguration corsConfiguration = new CorsConfiguration(); corsConfiguration.applyPermitDefaultValues(); corsConfiguration.setAllowedMethods(Arrays.asList(CorsConfiguration.ALL)); corsConfiguration.addExposedHeader(HttpHeaders.DATE); corsConfigurationSource.registerCorsConfiguration(\"/**\", corsConfiguration); CorsFilter corsFilter = new CorsFilter(corsConfigurationSource); FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(); filterRegistrationBean.setFilter(corsFilter); filterRegistrationBean.setOrder(Ordered.HIGHEST_PRECEDENCE); filterRegistrationBean.addUrlPatterns(\"/*\"); return filterRegistrationBean;&#125; 最后，在属性配置文件application.yml中配置一些必要的属性， 123456swagger: api-title: Demo标题 #会展示在下拉列表框中，一般写项目名称 api-description: Demo描述，集中注册 group-name: Demo项目 apis-base-package: cn.jboost.springboot.swagger # API类所在包名 swagger-registry-path: http://localhost:11090/swagger/register #就是2中注册服务的注册接口地址 配置完后， 就可以像一般项目一样编写接口类，加swagger注解。项目启动时， 会自动向注册服务完成注册，刷新注册服务的文档页面即可在下拉列表看到。 4. 总结本文介绍了一个基于swagger ui增强版项目swagger-bootstrap-ui的接口文档集中化管理实现。采用该实现，将所有swagger在线接口文档集中管理，有效提高前后端对接效率。 如果觉得本文有用，欢迎转发、推荐。 我的个人博客地址：http://blog.jboost.cn 我的github地址：https://github.com/ronwxy 我的微信公众号：jboost-ksxy （欢迎关注，及时获取技术干货分享） ——————————————————————————————————————————————————————————————— ![微信公众号](/assets/qrcode-05.jpg) 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.jboost.cn/categories/Java/"}],"tags":[{"name":"swagger","slug":"swagger","permalink":"http://blog.jboost.cn/tags/swagger/"}]},{"title":"Spring Boot从入门到实战（八）：集成AOPLog来记录接口访问日志","slug":"springboot-aoplog","date":"2019-06-27T03:03:57.000Z","updated":"2019-07-24T01:51:53.445Z","comments":true,"path":"springboot-aoplog.html","link":"","permalink":"http://blog.jboost.cn/springboot-aoplog.html","excerpt":"日志是一个Web项目中必不可少的部分，借助它我们可以做许多事情，比如问题排查、访问统计、监控告警等。一般通过引入slf4j的一些实现框架来做日志功能，如log4j,logback,log4j2，其性能也是依次增强。在springboot中，默认使用的框架是logback。我们经常需要在方法开头或结尾加日志记录传入参数或返回结果，以此来复现当时的请求情况。但是手动添加日志，不仅繁琐重复，也影响代码的美观简洁。本文引入一个基于AOP实现的日志框架，并通过spring-boot-starter的方式完成集成。 原文地址：http://blog.jboost.cn/springboot-aoplog.html","text":"日志是一个Web项目中必不可少的部分，借助它我们可以做许多事情，比如问题排查、访问统计、监控告警等。一般通过引入slf4j的一些实现框架来做日志功能，如log4j,logback,log4j2，其性能也是依次增强。在springboot中，默认使用的框架是logback。我们经常需要在方法开头或结尾加日志记录传入参数或返回结果，以此来复现当时的请求情况。但是手动添加日志，不仅繁琐重复，也影响代码的美观简洁。本文引入一个基于AOP实现的日志框架，并通过spring-boot-starter的方式完成集成。 原文地址：http://blog.jboost.cn/springboot-aoplog.html 1. aop-logging项目项目地址： https://github.com/ronwxy/aop-logging该项目基于 https://github.com/nickvl/aop-logging.git ， 在其基础上添加了ReqId来串联某次客户端请求（参考com.github.nickvl.xspring.core.log.aop.ReqIdFilter）, 添加了方法执行时长（参考com.github.nickvl.xspring.core.log.aop.AOPLogger.logTheMethod方法中elapsedTime）。 该项目提供了基于注解的AOP日志功能。根据不同的日志级别，提供的注解有LogTrace,LogDebug,LogInfo,LogWarn,LogError,LogFatal,LogException，可修饰于类（等同于该类内所有方法上添加）与方法上，前面六个分别表示在不同日志级别下记录方法被调用的日志，LogException表示在方法抛出异常时，记录相应日志。这些注解都提供了一个LogPoint枚举类型的属性value，取值{IN,OUT,BOTH}，分别表示在方法调用入口、方法调用返回前，以及包含两者的位置打印对应日志，默认为BOTH。 2. 集成可以通过基于xml或基于java配置的方式来集成AOP日志功能，我这里基于java配置（基于xml的方式参考源码README文件）并且通过spring-boot-starter的形式进行封装（源码地址： https://github.com/ronwxy/base-spring-boot ），避免每个项目都需要配置。自动配置类如下 12345678910111213141516171819202122232425262728293031@Configuration@ConditionalOnClass(AOPLogger.class)@ConditionalOnMissingBean(AOPLogger.class)public class AopLoggerAutoConfiguration &#123; private static final boolean SKIP_NULL_FIELDS = true; private static final Set&lt;String&gt; EXCLUDE_SECURE_FIELD_NAMES = Collections.emptySet(); @Bean public AOPLogger aopLogger() &#123; AOPLogger aopLogger = new AOPLogger(); aopLogger.setLogAdapter(new UniversalLogAdapter(SKIP_NULL_FIELDS, EXCLUDE_SECURE_FIELD_NAMES)); return aopLogger; &#125; /** * 注册一个过滤器，用来生成一个reqId，标记一次请求，从而将本次请求所产生的日志串联起来 * @param * @return */ @Bean public FilterRegistrationBean reqIdFilter() &#123; ReqIdFilter reqIdFilter = new ReqIdFilter(); FilterRegistrationBean registrationBean = new FilterRegistrationBean(); registrationBean.setFilter(reqIdFilter); List&lt;String&gt; urlPatterns = Collections.singletonList(\"/*\"); registrationBean.setUrlPatterns(urlPatterns); registrationBean.setOrder(100); return registrationBean; &#125;&#125; 将基础框架base-spring-boot通过mvn clean install进行本地安装后，即可在项目中通过依赖进行引入（基础框架中已在spring-boot-parent中引入，直接继承亦可），如 12345&lt;dependency&gt; &lt;groupId&gt;cn.jboost.springboot&lt;/groupId&gt; &lt;artifactId&gt;aoplog-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 3. 使用引入依赖之后，我们再定义一个日志配置文件logback-spring.xml，为了后面方便地将日志导入ELK做集中的日志分析管理，该配置文件中将日志以json格式输出，并根据日志级别分别写入debug.log,info.log,warn.log,error.log以及interface.log（专用于接口访问日志），配置示例如下（完整配置参考： https://github.com/ronwxy/springboot-demos/blob/master/springboot-aoplog/src/main/resources/logback-spring.xml） 1234567891011121314151617181920212223242526272829303132&lt;appender name=\"interfaceLog\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;file&gt;$&#123;logPath&#125;/elk/interface.log&lt;/file&gt; &lt;encoder class=\"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\"&gt; &lt;providers&gt; &lt;pattern&gt; &lt;pattern&gt; &#123; \"project\": \"$&#123;projectName&#125;\", \"timestamp\": \"%date&#123;\\\"yyyy-MM-dd'T'HH:mm:ss,SSSZ\\\"&#125;\", \"log_level\": \"%level\", \"thread\": \"%thread\", \"class_name\": \"%X&#123;callingClass&#125;\", \"class_method\":\"%X&#123;callingMethod&#125;\", \"line_number\": null, \"message\": \"%message\", \"stack_trace\": \"%exception&#123;5&#125;\", \"req_id\": \"%X&#123;reqId&#125;\", \"elapsed_time\": \"#asLong&#123;%X&#123;elapsedTime&#125;&#125;\" &#125; &lt;/pattern&gt; &lt;/pattern&gt; &lt;/providers&gt; &lt;/encoder&gt; &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt; &lt;level&gt;INFO&lt;/level&gt; &lt;/filter&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;$&#123;logPath&#125;/bak/interface.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;totalSizeCap&gt;1GB&lt;/totalSizeCap&gt; &lt;/rollingPolicy&gt; &lt;/appender&gt; 为了将该日志配置文件可以不经修改地达到复用，将一些参数配置外置了，故需在配置文件applicaiton.yml中配置如下参数 12345logger: path: D:\\logs #默认当前项目路径下的logs目录 level: info # 默认info apiPackage: cn.jboost.springboot.aoplog.controller #必须配置, api接口类所在包 rootPackage: cn.jboost.springboot #必须配置，项目根包，记录该包内各类通过slf4j输出的日志 最后，直接在需要记录访问日志的接口类上加注解@LogInfo就行了，如 12345678910@RestController@RequestMapping(\"test\")@LogInfopublic class AoplogTestController &#123; @GetMapping public String test(@RequestParam String user)&#123; return \"Hi \" + user; &#125;&#125; 注意：在pom.xml中默认添加的spring-boot-maven-plugin下需要添加repackage的goal才能自动生成日志目录与日志文件，如下所示 123456789101112131415&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 启动程序，调用@LogInfo标注的接口类下的API时，可以看到控制台有打印接口访问日志，如执行demo程序（源码： https://github.com/ronwxy/springboot-demos/tree/master/springboot-aoplog ），调用 http://localhost:8080/test?user=jboost 时，控制台打印日志如下 12[2019-06-27 14:29:59] [INFO ] [http-nio-8080-exec-1] [cn.jboost.springboot.aoplog.controller.AoplogTestController:184] --calling: test(user=jboost)[2019-06-27 14:29:59] [INFO ] [http-nio-8080-exec-1] [cn.jboost.springboot.aoplog.controller.AoplogTestController:189] --returning: test(1 arguments):Hi jboost 日志文件interface.log中打印日志如下，（其中req_id在本次请求的所有日志都相同，这样就可以将一次请求的所有日志串联起来，便于分析与定位问题；elapsed_time标明了方法执行时长，可用于接口性能监测） 12&#123;\"project\":\"aoplog-test\",\"timestamp\":\"2019-06-27T14:29:59,030+0800\",\"log_level\":\"INFO\",\"thread\":\"http-nio-8080-exec-1\",\"class_name\":\"cn.jboost.springboot.aoplog.controller.AoplogTestController\",\"class_method\":\"test\",\"line_number\":null,\"message\":\"calling: test(user=jboost)\",\"stack_trace\":\"\",\"req_id\":\"5d146267aa147904bc014e71\",\"elapsed_time\":null&#125;&#123;\"project\":\"aoplog-test\",\"timestamp\":\"2019-06-27T14:29:59,036+0800\",\"log_level\":\"INFO\",\"thread\":\"http-nio-8080-exec-1\",\"class_name\":\"cn.jboost.springboot.aoplog.controller.AoplogTestController\",\"class_method\":\"test\",\"line_number\":null,\"message\":\"returning: test(1 arguments):Hi jboost\",\"stack_trace\":\"\",\"req_id\":\"5d146267aa147904bc014e71\",\"elapsed_time\":2&#125; 4. 总结Web项目中经常需要通过查看接口请求及返回参数来定位问题，手动编写代码打印显得繁琐而重复。使用aop-logging通过简单的注解即可实现接口日志自动打印。本文介绍的方案与日志配置模板可直接用于实际项目开发。当然，注解不仅可用于Controller层，也可以用于Service等其它层，但一般Controller层加上即可，避免日志打印过多。 本文示例项目源码地址：https://github.com/ronwxy/springboot-demos/tree/master/springboot-aoplog我的个人博客地址：http://blog.jboost.cn我的github地址：https://github.com/ronwxy我的微信公众号：jboost-ksxy （欢迎关注，及时获取技术干货分享）—————————————————————————————————— 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.jboost.cn/categories/SpringBoot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"},{"name":"logback","slug":"logback","permalink":"http://blog.jboost.cn/tags/logback/"}]},{"title":"案例解析：springboot自动配置未生效问题定位（条件断点）","slug":"issue-conditiontrack","date":"2019-06-25T13:33:00.000Z","updated":"2020-08-26T01:39:29.556Z","comments":true,"path":"issue-conditiontrack.html","link":"","permalink":"http://blog.jboost.cn/issue-conditiontrack.html","excerpt":"Spring Boot在为开发人员提供更高层次的封装，进而提高开发效率的同时，也为出现问题时如何进行定位带来了一定复杂性与难度。但Spring Boot同时又提供了一些诊断工具来辅助开发与分析，如spring-boot-starter-actuator。本文分享一个基于actuator与IDEA条件断点来定位自动配置未生效的案例。望对类似问题分析与处理提供参考。","text":"Spring Boot在为开发人员提供更高层次的封装，进而提高开发效率的同时，也为出现问题时如何进行定位带来了一定复杂性与难度。但Spring Boot同时又提供了一些诊断工具来辅助开发与分析，如spring-boot-starter-actuator。本文分享一个基于actuator与IDEA条件断点来定位自动配置未生效的案例。望对类似问题分析与处理提供参考。 问题确认在前文介绍的 Spring Boot从入门到实战：整合通用Mapper简化单表操作 中，我们对druid连接池做了自动配置，并且注入了druid的监控统计功能，如下 但本地运行后通过 http://localhost:8080/druid/index.html 访问时却出现错误，通过浏览器的开发者工具查看该请求返回404，推测上述代码中定义的StatViewServlet未注入成功。我们用actuator来确认下是否如此。在项目中加入spring-boot-starter-actuator，并且application.yml中添加如下配置 123456789management: endpoints: web: exposure: include: \"*\" exclude: beans,trace endpoint: health: show-details: always 在spring-boot 2.x 版本当中，作为安全性考虑，将actuator 控件中的端口，只默认开放/health 和/info 两个端口，其他端口默认关闭， 因此需要添加如上配置。注意include的值 * 必须加引号，否则无法启动。 重启程序后访问 http://localhost:8080/actuator/conditions 确认上述两个实例化方法未满足@ConditionalOnProperty的条件，从而未执行生效，如图 条件断点从上面分析确认是因为条件注解 @ConditionalOnProperty(prefix = &quot;spring.datasource.druid&quot;, name = &quot;druidServletSettings&quot;) 未满足使方法未执行导致。那这个条件为什么没有满足呢，查看application.yml中也做了 spring.datasource.druid.druidServletSettings属性的配置。 当你无法理清头绪，确定问题原因时，那就Debug吧。查看注解@ConditionalOnProperty源码，找到其实现支持类OnPropertyCondition，如下 123456789101112131415@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Documented@Conditional(&#123;OnPropertyCondition.class&#125;)public @interface ConditionalOnProperty &#123; String[] value() default &#123;&#125;; String prefix() default \"\"; String[] name() default &#123;&#125;; String havingValue() default \"\"; boolean matchIfMissing() default false;&#125; 查看OnPropertyCondition源码，了解它是通过getMatchOutcome方法来判断是否满足注解参数所指定的条件的，如下所示 123456789101112131415161718@Overridepublic ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; List&lt;AnnotationAttributes&gt; allAnnotationAttributes = annotationAttributesFromMultiValueMap( metadata.getAllAnnotationAttributes( ConditionalOnProperty.class.getName())); List&lt;ConditionMessage&gt; noMatch = new ArrayList&lt;&gt;(); List&lt;ConditionMessage&gt; match = new ArrayList&lt;&gt;(); for (AnnotationAttributes annotationAttributes : allAnnotationAttributes) &#123; ConditionOutcome outcome = determineOutcome(annotationAttributes, context.getEnvironment()); (outcome.isMatch() ? match : noMatch).add(outcome.getConditionMessage()); &#125; if (!noMatch.isEmpty()) &#123; return ConditionOutcome.noMatch(ConditionMessage.of(noMatch)); &#125; return ConditionOutcome.match(ConditionMessage.of(match));&#125; 在调用determineOutcome处打断点，调试什么原因导致条件未满足，但是这里是一个for循环，如果for元素过多的话，将可能需要断点阻断很多次才能找到你想要查看的那个元素。所幸IDEA提供了不同类型的断点来处理这类问题，前面 案例解析：使用IDEA异常断点来定位java.lang.ArrayStoreException的问题 我们介绍了异常断点的使用。这里介绍用条件断点来处理这类循环块中的debug问题。 在上述代码for循环中调用determineOutcome行打断点，并在断点上右键，弹出如下窗口 图中Condition框即可输入你要指定的条件，可以直接写java判断表达式代码，并引用该行代码处能访问的变量，如这里我们输入 annotationAttributes.get(&quot;name&quot;).equals(&quot;druidServletSettings&quot;)，然后点击Debug窗口的“Resume Program (F9)”按钮，则在不满足指定条件时，断点处将不会被阻断，直到条件满足，这样就能很容易定位到我们想要查看的元素。（当然这里allAnnotationAttributes变量其实只有一个元素，仅仅是为了演示条件变量的使用，当集合元素很多时，使用条件断点就能体会到它的方便之处） 问题定位通过Debug的方式深入条件注解的判断逻辑（其中循环处可使用条件断点），最终来到如下代码片段 在这里是判断来自所有属性源配置的属性中，是否包含条件注解指定的属性，即spring.datasource.druid.druidServletSettings，由上图可见，spring.datasource.druid.druidServletSettings只是某些属性的前缀，并不存在完全匹配的属性，因此返回false，导致条件不满足。回看注解@ConditionOnProperty的javadoc， 123456789* If the property is not contained in the &#123;@link Environment&#125; at all, the * &#123;@link #matchIfMissing()&#125; attribute is consulted. By default missing attributes do not * match. * &lt;p&gt; * This condition cannot be reliably used for matching collection properties. For example, * in the following configuration, the condition matches if &#123;@code spring.example.values&#125; * is present in the &#123;@link Environment&#125; but does not match if * &#123;@code spring.example.values[0]&#125; is present. * 当Environment中不包含该属性时，则看matchIfMissing的值，该值默认为false，如果包含该属性，则再对比属性值与havingValue的值，相等即满足，不等则不满足。并且该条件注解不能用于匹配集合类型属性。上述spring.datasource.druid.druidServletSettings实际上属于一个Map类型，因此不能想当然地认为该注解是只要属性集中某属性名称包含该值即满足。 总结当难以定位到问题原因时，可以进行Debug，跟踪程序运行的各个步骤，当要在循环中Debug定位到某个元素时，可以用条件断点来实现。@ConditionalOnProperty注解不是存在某属性就行，还需要值相等，并且不适用于集合类型属性。我的个人博客地址：http://blog.jboost.cn我的头条空间： https://www.toutiao.com/c/user/5833678517/#mid=1636101215791112我的github地址：https://github.com/ronwxy我的微信公众号：jboost-ksxy ——————————————————————————————————————————————————————————————— 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.jboost.cn/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"}]},{"title":"Spring Boot从入门到实战（七）：整合通用Mapper简化单表操作","slug":"springboot-tkmapper","date":"2019-06-24T11:32:15.000Z","updated":"2019-07-22T10:30:03.487Z","comments":true,"path":"springboot-tkmapper.html","link":"","permalink":"http://blog.jboost.cn/springboot-tkmapper.html","excerpt":"数据库访问是web应用必不可少的部分。现今最常用的数据库ORM框架有Hibernate与Mybatis，Hibernate貌似在传统IT企业用的较多，而Mybatis则在互联网企业应用较多。通用Mapper（https://github.com/abel533/Mapper） 是一个基于Mybatis，将单表的增删改查通过通用方法实现，来减少SQL编写的开源框架，且也有对应开源的mapper-spring-boot-starter提供。我们在此基础上加了一些定制化的内容，以便达到更大程度的复用。","text":"数据库访问是web应用必不可少的部分。现今最常用的数据库ORM框架有Hibernate与Mybatis，Hibernate貌似在传统IT企业用的较多，而Mybatis则在互联网企业应用较多。通用Mapper（https://github.com/abel533/Mapper） 是一个基于Mybatis，将单表的增删改查通过通用方法实现，来减少SQL编写的开源框架，且也有对应开源的mapper-spring-boot-starter提供。我们在此基础上加了一些定制化的内容，以便达到更大程度的复用。 框架源码地址：https://github.com/ronwxy/base-spring-boot （持续更新完善中，欢迎follow，star）Demo源码地址：https://github.com/ronwxy/springboot-demos/tree/master/springboot-tkmapper 在开源mapper-spring-boot-starter的基础上，增加了如下内容： 针对MySQL数据库与PostgreSQL数据库添加了一些Java类型与数据库类型的转换处理类，如将List、Map类型与MySQL数据库的json类型进行转换处理 对Domain、Mapper、Service、Controller各层进行了封装，将基本的增删改查功能在各层通用化 提供了基于druid连接池的自动配置 其它一些调整，如默认映射复杂类型属性（主要是List、Map类型，其它自定义类型需要自定义转换处理类），将枚举作为简单类型处理 提供了一个parent项目，将一些常用的框架进行集成，实际项目可继承parent简化依赖配置（持续更新完善） 该框架可用于实际基于springboot的项目，只需简单配置数据源，即可引入druid连接池及通用mapper的功能，以及各层基本的增删改查方法。 如何使用？下文给出使用步骤，可参考示例：https://github.com/ronwxy/springboot-demos/tree/master/springboot-tkmapper 1. 框架Maven部署安装下载框架源码后，在项目根路径下执行mvn clean install可安装到本地maven库。如果需要共享，且搭了Nexus私服，则在根路径pom.xml文件中添加distributionManagement配置，指定Nexus仓库分发地址，使用mvn clean deploy安装到远程maven仓库，如 1234567891011121314&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;url&gt; http://ip:port/repository/maven-releases/ &lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;url&gt; http://ip:port/repository/maven-snapshots/ &lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; 上述指定的repository需要在maven的全部配置文件settings.xml中有对应账号配置(id需要一一对应)，如 123456789101112 &lt;servers&gt; &lt;server&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;xxx&lt;/password&gt; &lt;/server&gt;&lt;server&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;xxx&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; 2. pom.xml配置项目中引入该数据库框架有三种方式： 直接引入 cn.jboost.springboot:tkmapper-spring-boot-starter（没有连接池） 直接引入 cn.jboost.springboot:druid-spring-boot-starter（druid连接池支持） 项目继承 cn.jboost.springboot:spring-boot-parent（使用的是druid连接池） 三种方式的pom.xml配置如下 123456789101112131415161718192021#第一种方式&lt;dependency&gt; &lt;groupId&gt;cn.jboost.springboot&lt;/groupId&gt; &lt;artifactId&gt;tkmapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt;#第二种方式&lt;dependency&gt; &lt;groupId&gt;cn.jboost.springboot&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt;#第三种方式&lt;parent&gt; &lt;groupId&gt;cn.jboost.springboot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-parent&lt;/artifactId&gt; &lt;version&gt;1.2-SNAPSHOT&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt; 根据情况引入mysql或postgresql的驱动依赖（其它数据库暂未做类型转换支持，未作测试） 3. 配置数据源如果使用druid连接池，则在application.yml配置文件中，加入如下数据源配置（推荐） 12345678910111213141516171819202122232425262728293031spring: datasource: druid: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/test?autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8 username: root password: # 自定义配置 initialSize: 2 # 初始化大小 minIdle: 1 # 最小连接 maxActive: 5 # 最大连接 druidServletSettings: allow: 127.0.0.1 deny: loginUsername: admin loginPassword: Passw0rd resetEnable: true druidFilterSettings: exclusions: '*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/*' maxWait: 60000 # 配置获取连接等待超时的时间 timeBetweenEvictionRunsMillis: 60000 # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒 minEvictableIdleTimeMillis: 300000 # 配置一个连接在池中最小生存的时间，单位是毫秒 validationQuery: SELECT 'x' testWhileIdle: true testOnBorrow: false testOnReturn: false poolPreparedStatements: true # 打开PSCache，并且指定每个连接上PSCache的大小 maxPoolPreparedStatementPerConnectionSize: 20 filters: stat #,wall（添加wall代码里不能直接拼接sql，druid有sql注入校验） # 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙 connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000 # 通过connectProperties属性来打开mergeSql功能；慢SQL记录 useGlobalDataSourceStat: true # 合并多个DruidDataSource的监控数据 如果不使用连接池，则配置相对简单，如下 123456spring: datasource: url: jdbc:mysql://localhost:3306/test?autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8 username: root password: driver-class-name: com.mysql.jdbc.Driver 4. 定义相应domain，mapper，service，controller各层对象以demo为例（demo数据库脚本见resources/schema.sql），domain定义一个User类, 12345678910111213141516@Table(name = \"user\")@Getter@Setter@ToStringpublic class User extends AutoIncrementKeyBaseDomain&lt;Integer&gt; &#123; private String name; @ColumnType(jdbcType = JdbcType.CHAR) private Gender gender; private List&lt;String&gt; favor; private Map&lt;String, String&gt; address; public enum Gender&#123; M, F &#125;&#125; 需要添加@Table注解指定数据库表名，可通过继承AutoIncrementKeyBaseDomain来实现自增主键，或UUIDKeyBaseDomain来实现UUID主键，如果自定义其它类型主键，则继承BaseDomain。 该框架Service层通用方法实现BaseService只支持单列主键，不支持组合主键（也不建议使用组合主键） 框架默认对List、Map等复杂类型属性会映射到mysql的json类型或postgresql的jsonb类型，如果某个属性不需要映射，可添加@Transient注解；枚举类型需添加@ColumnType指定jdbcType。 dao层定义UserMapper， 123@Repositorypublic interface UserMapper extends BaseMapper&lt;User&gt; &#123;&#125; BaseMapper默认实现了单表的增删改查及批量插入等功能，如需定义复杂查询，可在该接口中定义，然后通过mapper xml文件编写实现。 service层定义 UserService，继承了BaseService的通用功能（具体可查看源码），同样可在该类中自定义方法 12345678910@Servicepublic class UserService extends BaseService&lt;Integer, User&gt; &#123; @Transactional public void createWithTransaction(User user)&#123; create(user); //用于测试事务 throw new RuntimeException(\"抛出异常，让前面的数据库操作回滚\"); &#125;&#125; controller层定义 UserController，继承了BaseController的通用接口（具体可查看源码） 1234@RestController@RequestMapping(\"/user\")public class UserController extends BaseController&lt;Integer, User&gt; &#123;&#125; 如上，只需要定义各层对应的接口或类，继承基础接口或类，便完成了用户基本的增删改查功能，不需要写一行具体的实现代码。 5. 测试、运行 示例中提供了两个新建用户的单元测试，参考SpringbootTkmapperApplicationTests类 运行，在主类上直接运行，然后浏览器里打开 http://localhost:8080/user 则可列出单元测试中创建的用户（其它接口参考BaseController实现） 6. 总结本文介绍框架基于tk.mybatis:mapper-spring-boot-starter做了一些自定义扩展，以更大程度地实现复用。可用于实际项目开发，使用过程中如果遇到问题，可关注公众号留言反馈。我的个人博客地址：http://blog.jboost.cn我的头条空间： https://www.toutiao.com/c/user/5833678517/#mid=1636101215791112我的github地址：https://github.com/ronwxy我的微信公众号：jboost-ksxy ———————————————————————————————————————— 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.jboost.cn/categories/SpringBoot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"}]},{"title":"案例解析：使用IDEA异常断点来定位java.lang.ArrayStoreException的问题","slug":"issue-errortrack","date":"2019-06-21T10:31:03.000Z","updated":"2020-08-26T01:39:00.180Z","comments":true,"path":"issue-errortrack.html","link":"","permalink":"http://blog.jboost.cn/issue-errortrack.html","excerpt":"最近对 base-spring-boot （https://github.com/ronwxy/base-spring-boot） 项目进行了升级。在将其用于应用开发中时遇到java.lang.ArrayStoreException的异常导致程序无法启动。平常开发过程中面对这种描述不够清楚，无法定位具体原因的问题该如何处理？本文分享通过使用IDEA异常断点来定位此类问题的方法。","text":"最近对 base-spring-boot （https://github.com/ronwxy/base-spring-boot） 项目进行了升级。在将其用于应用开发中时遇到java.lang.ArrayStoreException的异常导致程序无法启动。平常开发过程中面对这种描述不够清楚，无法定位具体原因的问题该如何处理？本文分享通过使用IDEA异常断点来定位此类问题的方法。 启动程序时抛出如下异常，导致启动失败 12345678910111213141516171819202122232425org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#39;devGlobalExceptionHandler&#39; defined in class path resource [cn&#x2F;jboost&#x2F;springboot&#x2F;autoconfig&#x2F;error&#x2F;exception&#x2F;ExceptionHandlerAutoConfiguration.class]: Post-processing of merged bean definition failed; nested exception is java.lang.ArrayStoreException: sun.reflect.annotation.TypeNotPresentExceptionProxy at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:570) ~[spring-beans-5.1.7.RELEASE.jar:5.1.7.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) ~[spring-beans-5.1.7.RELEASE.jar:5.1.7.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.7.RELEASE.jar:5.1.7.RELEASE] at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.7.RELEASE.jar:5.1.7.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.7.RELEASE.jar:5.1.7.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.7.RELEASE.jar:5.1.7.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:843) ~[spring-beans-5.1.7.RELEASE.jar:5.1.7.RELEASE] at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:877) ~[spring-context-5.1.7.RELEASE.jar:5.1.7.RELEASE] at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.1.7.RELEASE.jar:5.1.7.RELEASE] at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142) ~[spring-boot-2.1.5.RELEASE.jar:2.1.5.RELEASE] at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775) [spring-boot-2.1.5.RELEASE.jar:2.1.5.RELEASE] at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) [spring-boot-2.1.5.RELEASE.jar:2.1.5.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:316) [spring-boot-2.1.5.RELEASE.jar:2.1.5.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260) [spring-boot-2.1.5.RELEASE.jar:2.1.5.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1248) [spring-boot-2.1.5.RELEASE.jar:2.1.5.RELEASE] at com.cnbot.kindergarten.CnbotKindergartenApplication.main(CnbotKindergartenApplication.java:10) [classes&#x2F;:na]Caused by: java.lang.ArrayStoreException: sun.reflect.annotation.TypeNotPresentExceptionProxy at sun.reflect.annotation.AnnotationParser.parseClassArray(AnnotationParser.java:724) ~[na:1.8.0_201] at sun.reflect.annotation.AnnotationParser.parseArray(AnnotationParser.java:531) ~[na:1.8.0_201] at sun.reflect.annotation.AnnotationParser.parseMemberValue(AnnotationParser.java:355) ~[na:1.8.0_201] at sun.reflect.annotation.AnnotationParser.parseAnnotation2(AnnotationParser.java:286) ~[na:1.8.0_201] at sun.reflect.annotation.AnnotationParser.parseAnnotations2(AnnotationParser.java:120) ~[na:1.8.0_201] at sun.reflect.annotation.AnnotationParser.parseAnnotations(AnnotationParser.java:72) ~[na:1.8.0_201] ... 单纯看异常栈，无法定位问题原因，只能看到是在调用devGlobalExceptionHandler创建bean时出错，错误信息java.lang.ArrayStoreException: sun.reflect.annotation.TypeNotPresentExceptionProxy。这属于框架内部抛出的异常，通常的设置断点Debug的方法很难定位到具体原因，可通过IDEA的异常断点来进行定位，它会在程序运行过程中出现指定异常时进行阻断。 1. 添加异常断点在IDEA的Debug面板中，点击“View Breakpoints”（两个重叠的红色圈按钮），如下 打开“Breakpoints”窗口，在该窗口中点击“+”按钮，选择“Java Exception Breakpoints”， 如下图 然后在弹出的“Enter Exception Class”窗口中输入ArrayStoreException选中对应异常，依次点击OK，Done按钮即完成异常断点添加。 2. 程序debug开始以Debug模式启动程序。 程序运行后，在前面配置的异常出现时，将会进行阻断，如图 可以看到程序阻断在上图高亮的那行代码处，异常便是从这里抛出的。查看parseClassValue方法，可看到这里有catchTypeNotPresentException异常，并且包装成我们在异常栈看到的TypeNotPresentExceptionProxy返回。离真相很近了。 我们可以在上述catch块中添加一个断点，查看异常包装前的状态，如图 重新Debug运行，将定位到上图代码处，查看异常，看到如下图所示信息 该信息表示org.springframework.security.access.AccessDeniedException这个类不存在，导致BaseWebApplicationExceptionHandler类型的bean实例化时出错。这时候问题基本已经定位到了。 查看源码，在BaseWebApplicationExceptionHandler中有对AccessDeniedException的统一处理，但是spring-boot-autoconfigure所有的依赖都是optional的（不会传递依赖），而在新开发的项目中，并没有引入spring-security，因此导致AccessDeniedException这个类找不到而报错。目前通过去掉该部分处理解决。 总结IDEA的Debug支持好几种断点类型，如前文介绍的异常断点，以及比较常用的条件断点等。当无法从异常栈信息找到问题所在时，借用这些类型的断点进行Debug，往往事情就变得简单了。 我的个人博客地址：http://blog.jboost.cn 我的头条空间： https://www.toutiao.com/c/user/5833678517/#mid=1636101215791112 我的github地址：https://github.com/ronwxy 我的微信公众号：jboost-ksxy ———————————————————————————————————————— 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.jboost.cn/categories/Java/"}],"tags":[{"name":"idea","slug":"idea","permalink":"http://blog.jboost.cn/tags/idea/"}]},{"title":"Spring Boot从入门到实战（六）：整合Web项目常用功能","slug":"springboot-base","date":"2019-06-20T14:17:22.000Z","updated":"2019-07-22T10:29:50.279Z","comments":true,"path":"springboot-base.html","link":"","permalink":"http://blog.jboost.cn/springboot-base.html","excerpt":"在Web应用开发过程中，一般都涵盖一些常用功能的实现，如数据库访问、异常处理、消息队列、缓存服务、OSS服务，以及接口日志配置，接口文档生成等。如果每个项目都来一套，则既费力又难以维护。可以通过Spring Boot的Starter来将这些常用功能进行整合与集中维护，以达到开箱即用的目的。","text":"在Web应用开发过程中，一般都涵盖一些常用功能的实现，如数据库访问、异常处理、消息队列、缓存服务、OSS服务，以及接口日志配置，接口文档生成等。如果每个项目都来一套，则既费力又难以维护。可以通过Spring Boot的Starter来将这些常用功能进行整合与集中维护，以达到开箱即用的目的。 项目基于Spring Boot 2.1.5.RELEASE 版。项目地址： https://github.com/ronwxy/base-spring-boot 整个项目分为如下几部分： spring-boot-autoconfigure： 具体的各功能实现，每个功能通过package的形式组织 spring-boot-commons： 一些公共的工具类或共享类 spring-boot-dependencies： 依赖的集中维护管理，集中管理各个依赖的版本号 spring-boot-parent： 提供一个基本的父项目，web服务项目可通过继承该项目创建 spring-boot-starters： 各功能的starter项目，引入相应starter即引入相应功能 spring-boot-dependencies 项目该项目主要是对所有依赖进行集中定义。通过 dependencyManagement 对依赖进行声明， 12345678910111213141516171819&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-boot.version&#125;&lt;/version&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;cn.jboost.springboot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-commons&lt;/artifactId&gt; &lt;version&gt;$&#123;base-spring-boot.version&#125;&lt;/version&gt; &lt;/dependency&gt; ... &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 这样，所有依赖的版本可以集中统一管理，在其它地方引用的时候可以省去版本的声明，如 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; spring-boot-autoconfigure 项目该项目是各功能自动配置的具体实现，以package的形式进行组织，如 tkmapper 包下实现了通用Mapper的自动配置，error 包下实现了错误处理的自动配置， 等等。 该项目继承了spring-boot-dependencies， 在项目的 pom.xml 中，依赖部分声明类似于 1234567891011121314&lt;dependencies&gt; &lt;!-- spring denpendencies --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; ...&lt;/dependencies&gt; 不需要再指定版本号，通过将optional设置为true，表示该依赖不会进行传递，即另外一个项目引用该项目时，optional的依赖不会被传递依赖过去。 在 resources/META-INF/spring.factories 文件中，声明了所有自动配置类， 如下 1234567891011org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\cn.jboost.springboot.autoconfig.tkmapper.MapperAutoConfiguration,\\cn.jboost.springboot.autoconfig.aoplog.AopLoggerAutoConfiguration,\\cn.jboost.springboot.autoconfig.alimq.config.AliMQAutoConfiguration,\\cn.jboost.springboot.autoconfig.qiniu.QiniuAutoConfiguration,\\cn.jboost.springboot.autoconfig.swagger.Swagger2AutoConfiguration,\\cn.jboost.springboot.autoconfig.druid.DruidAutoConfiguration,\\cn.jboost.springboot.autoconfig.error.exception.ExceptionHandlerAutoConfiguration,\\cn.jboost.springboot.autoconfig.alimns.MnsAutoConfiguration,\\cn.jboost.springboot.autoconfig.redis.RedisClientAutoConfiguration,\\cn.jboost.springboot.autoconfig.web.CORSAutoConfiguration spring-boot-starters 项目该项目包含按功能划分的多个子项目，主要用来引入依赖以达到自动配置的依赖条件，使引入对应starter时，能让自动配置生效。如通用Mapper集成的 tkmapper-spring-boot-starter 依赖如下 123456789101112131415161718&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 同时在 resources/META-INF/spring.provides 里声明了该starter的用途，这里可随意编写。 spring-boot-commons 项目可将一些常用的工具类， 或共享类放到这个项目中。比如一些常量定义，加解密工具类等。 spring-boot-parent 项目该项目将Web应用需要的一些常见功能整合进来，应用项目可继承该项目进行构建，从而直接引入相应的功能。 在接下来的spring boot系列博文中，将一一详细介绍各功能的整合集成与应用。同时会不断更新与完善，以达到能直接用于生产项目的水平。 我的个人博客地址：http://blog.jboost.cn 我的头条空间： https://www.toutiao.com/c/user/5833678517/#mid=1636101215791112 我的github地址：https://github.com/ronwxy 我的微信公众号：jboost-ksxy ——————————————————————————————————————————————————————————————— 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.jboost.cn/categories/SpringBoot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"}]},{"title":"研发团队如何借助Gitlab来做代码review","slug":"code-review","date":"2019-06-18T12:03:21.000Z","updated":"2020-08-26T01:45:04.390Z","comments":true,"path":"code-review.html","link":"","permalink":"http://blog.jboost.cn/code-review.html","excerpt":"代码review是代码质量保障的手段之一，同时开发成员之间代码review也是一种技术交流的方式，虽然会占用一些时间，但对团队而言，总体是个利大于弊的事情。如何借助现有工具在团队内部形成代码review的流程与规范，是team leader或技术管理者需要考虑的问题。本文分享一种基于Gitlab代码merge流程的code review方法，以供参考与探讨。如有更好的方法，欢迎交流。","text":"代码review是代码质量保障的手段之一，同时开发成员之间代码review也是一种技术交流的方式，虽然会占用一些时间，但对团队而言，总体是个利大于弊的事情。如何借助现有工具在团队内部形成代码review的流程与规范，是team leader或技术管理者需要考虑的问题。本文分享一种基于Gitlab代码merge流程的code review方法，以供参考与探讨。如有更好的方法，欢迎交流。 1. 设置成员角色首先需要对你团队的成员分配角色，在Gitlab groups里选择一个group，然后左边菜单栏点击 Members，可在 Members 页面添加或编辑成员角色，如下图所示。 其中角色包含如下几类： Guest：权限最小，基本查看功能 Reporter：只能查看，不能push Developer：能push，也能merge不受限制的分支 Master：除了项目的迁移、删除等管理权限没有，其它权限基本都有 Owner：权限最大，包括项目的迁移、删除等管理权限 详细权限参考： https://docs.gitlab.com/ee/user/permissions.html 确定团队中技术水平、经验较好的成员为Master，负责代码的review与分支的合并；其他成员为Developer，提交合并请求，接受review意见；Master之间可以互相review。 2. 配置分支保护在项目页面左侧菜单栏 Settings -&gt; Repository， 进入“Protected Branches”部分配置分支保护，如下图所示。 在这里可以针对每个分支，设置允许什么角色可以merge，允许什么角色可以push，选项包括三个：“Masters”， “Developers + Masters”， “No one”。这里设置成只允许master可以直接push与merge这几个常设分支的代码。（如果更严格一点，可以将“Allowed to push”设置成“No one”） 3. 代码review流程3.1. 开发（开发者负责） 本地切到develop分支， 拉取最新代码（相关命令如下，GUI工具操作自行查相关文档） 123git branch #查看当前位于哪个分支，前面打星号即为当前分支git checkout develop #切换到develop分支git pull #拉取最新代码 从develop分支切出子分支 1git checkout -b feature-1101 #从当前分支切出子分支，命名为\"feature-1101\" 编码、本地自测完之后，提交子分支到远程仓库 123git add * #加入暂存区git commit -m \"commit msg\" #提交到本地仓库git push origin feature-1101 #提交到远程仓库 3.2 发起Merge请求（开发者负责） 在项目主页面，依次点击左侧“Merge Requests”（下图1），“New merge request”（下图2），打开新建Merge请求页面 在新建Merge请求页面，选择merge的源分支，及目标分支，如下图源分支为“feature-1101”，目标分支为“develop”，点击“Compare branches and continue”按钮进入对比与提交页面 在对比与提交页面，可以点击“Changes” tab查看本次修改（这里我为了演示，只是加了两个换行），确认无误，点击“Submit merge request”按钮，提交merge请求 提交之后，将结果页面的浏览器地址发到团队即时通讯群（如钉钉），并@相应的同事申请review 3.3 代码Review（code reviewer负责） 负责代码Review的同事收到申请后，点击merge请求地址，打开页面，查看“Changes” 这里可通过“Inline”单边查看，也可以通过“Side-by-side”两个版本对比查看 review完成后，若无问题，则可点击”Merge”按钮完成merge，同时可删除对应的子分支“feature-1101”，若有问题，则可点击“Close merge request”按钮关闭该merge请求（也可以不关闭复用该merge请求），同时通知开发者进行相应调整，重新提交代码发起merge请求（如果之前没关闭merge请求，则刷新即可看到调整）。 3.4 冲突解决（开发者负责） merge的时候，可能存在代码冲突，这时，开发者可从develop分支重新拉取最新代码进行本地merge， 解决冲突后重新提交代码进行review 12345678git pull origin develop #在当前子分支拉取develop分支的最新代码进行本地merge# 解决冲突代码# 提交git add *git commit -m \"fix merge conflict\"git push origin feature-1101 自行解决不了时，寻求协助 4. 借助阿里钉钉机器人来改善体验前面流程中提醒code reviewer是需要开发者自己来发消息通知的，可不可以把这个流程自动化。我们可以借助Gitlab的webhook与钉钉机器人来实现。 在钉钉群右上角点击“…”，打开群设置，群机器人中点击添加机器人，会显示可以添加的机器人类型，如下图所示 选择Gitlab，点击添加，输入机器人名称，如“Gitlab”，点击完成即创建了一个Gitlab的钉钉机器人。回到“群机器人”窗口，将能看到刚刚创建的Gitlab机器人，如图 点击齿轮按钮，进入设置页，可看到webhook地址，点击复制，复制该机器人的webhook地址。如图 在Gitlab项目主页进入 Settings -&gt; Integrations， 将前面复制的webhook地址填入URL中，Trigger 部分选择“Merge request events”（不要勾太多，不然提醒太多就有点骚扰了），然后点击“Add webhook”就完成了。如图 当有开发人员提交merge请求时，钉钉机器人将在钉钉群里发出通知，code reviewer点击消息里的链接即可进入页面进行code review， review完成，将分支merge之后，钉钉机器人也会发出消息（所有merge相关的事件都会发出消息）。如图 5. 总结团队协作，流程、规范很重要，不同的团队可能有不同的适用流程与规范。此文分享了基于Gitlab与阿里钉钉群机器人的代码review流程，希望对团队研发协作有一定参考价值，也欢迎一起探讨、交流。","categories":[{"name":"Teamwork","slug":"Teamwork","permalink":"http://blog.jboost.cn/categories/Teamwork/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.jboost.cn/tags/git/"}]},{"title":"团队项目的Git分支如何管理","slug":"git-branch","date":"2019-06-17T05:28:19.000Z","updated":"2020-08-26T01:45:15.596Z","comments":true,"path":"git-branch.html","link":"","permalink":"http://blog.jboost.cn/git-branch.html","excerpt":"许多公司的开发团队都采用Git来做代码版本控制。如何有效地协同开发人员之间，以及开发、测试、上线各环节的工作，可能都有各自的流程与规范。本文分享的是作者一直沿用的团队项目Git分支管理规范，希望给有缘阅读的人以参考，如果有更好的实践，也欢迎指教、讨论。","text":"许多公司的开发团队都采用Git来做代码版本控制。如何有效地协同开发人员之间，以及开发、测试、上线各环节的工作，可能都有各自的流程与规范。本文分享的是作者一直沿用的团队项目Git分支管理规范，希望给有缘阅读的人以参考，如果有更好的实践，也欢迎指教、讨论。 分支管理创建项目时（一般是服务型项目，工具型或辅助型项目可以简单一些），会针对不同环境创建三个常设分支： develop：开发环境的稳定分支，公共开发环境基于该分支构建。 pre-release：测试环境的稳定分支，测试环境基于该分支构建。 master：生产环境的稳定分支，生产环境基于该分支构建。仅用来发布新版本，除了从pre-release或生产环境Bug修复分支进行merge，不接受任何其它修改 平时开发工作中，会根据需要由开发人员创建两类临时分支： 功能（feature）分支：为了开发某个特定功能，从develop分支上面分出来的。开发完成后，要merge到develop分支。功能分支的命名，可以采用feature-*的形式命名(*为任务单号) Bug修复（fixbug）分支：为了修复某个bug，从常设分支上面分出来的。修复完成后，再merge到对应的分支。Bug修复分支的命名，可以采用fixbug-*的形式命名（*为bug单号） 流程规范正常开发流程 从develop分支切出一个新分支，根据是功能还是bug，命名为feature-* 或 fixbug-*。 开发者完成开发，提交分支到远程仓库。 开发者发起merge请求（可在gitlab页面“New merge request”），将新分支请求merge到develop分支，并提醒code reviewer进行review code reviewer对代码review之后，若无问题，则接受merge请求，新分支merge到develop分支，同时可删除新建分支；若有问题，则不能进行merge，可close该请求，同时通知开发者在新分支上进行相应调整。调整完后提交代码重复review流程。 转测时，直接从当前develop分支merge到pre-release分支，重新构建测试环境完成转测。 测试完成后，从pre-release分支merge到master分支，基于master分支构建生产环境完成上线。并对master分支打tag，tag名可为v1.0.0_2019032115（即版本号_上线时间） 流程示意图如下所示 并行开发测试环境Bug修复流程并行开发（即前一个版本已经转测但未上线，后一个版本又已在开发中并部分合并到了develop分支）过程中，转测后测试环境发现的bug需要修复，但是develop分支此时又有新内容且该部分内容目前不计划转测，可以pre-release切出一个bug修复分支。完成之后需要同时merge到pre-release分支与develop分支。merge时参考“正常开发流程”。流程示意图如下 生产环境Bug修复流程生产环境的Bug分两种情况： 紧急Bug：严重影响用户使用的为紧急Bug，需立即进行修复。如关键业务流程存在问题，影响用户正常的业务行为。 非紧急Bug或优化：非关键业务流程问题，仅影响用户使用体验，或出现频率较小等，为非紧急Bug，可规划到后续版本进行修复。 非紧急Bug修复参考“正常开发流程”。 紧急Bug修复，需要从master分支切出一个bug修复分支，完成之后需要同时merge到master分支与develop分支（如果需要测试介入验证，则可先merge到pre-release分支，验证通过后再merge到master分支上线）。merge时参考“正常开发流程”。流程示意图如下 我的个人博客地址：http://blog.jboost.cn 我的头条空间： https://www.toutiao.com/c/user/5833678517/#mid=1636101215791112 我的github地址：https://github.com/ronwxy 我的微信公众号：jboost-ksxy ———————————————————————————————————————— 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"Teamwork","slug":"Teamwork","permalink":"http://blog.jboost.cn/categories/Teamwork/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.jboost.cn/tags/git/"}]},{"title":"命令行高效操作Git，看这篇就够了","slug":"use-git","date":"2019-06-16T06:30:07.000Z","updated":"2020-08-26T01:33:59.098Z","comments":true,"path":"use-git.html","link":"","permalink":"http://blog.jboost.cn/use-git.html","excerpt":"对于软件开发人员来说，git几乎是每天都需要接触的工具。但对于相处如此亲密的工作伙伴，你对它的了解又有多少，是不是还在傻瓜式地打开一个GUI工具，点击提交按钮，然后“卧槽，又冲突了”，一脸懵逼到不知所措，责怪谁又在你前面提交了，谁又改了你的代码。","text":"对于软件开发人员来说，git几乎是每天都需要接触的工具。但对于相处如此亲密的工作伙伴，你对它的了解又有多少，是不是还在傻瓜式地打开一个GUI工具，点击提交按钮，然后“卧槽，又冲突了”，一脸懵逼到不知所措，责怪谁又在你前面提交了，谁又改了你的代码。 博主从一开始接触git，就没用过任何GUI工具，都是通过命令行进行操作，发现这种方式不仅对git的理解更深，效率也更高，遇到问题时一般都知道如何来处理，故做此分享。本文所有知识与操作只涉及日常使用场景，更多详细内容可自行查阅其它资料。本文Git版本为 windows-2.20.1版。 基础理论git的理论知识，对使用者来说只需要知道它是分布式版本控制系统，了解如下三个概念即可， 工作区：就是你直接操作的文件目录与内容 暂存区：暂时为你保存还没将内容提交到版本库的一个区域，对应.git目录下的stage或index文件 版本库：分本地版本库与远程版本库，本地版本库就理解为对应.git目录即可，远程版本库就是远程仓库，如gitlab或github的repository。 如下图，我们平时提交代码的过程基本都是从工作区add到暂存区，然后再commit到本地仓库，最后push到远程仓库。 基本命令对于日常工作，掌握如下几个基本命令一般就够了 git status 查看修改状态 git pull origin master 拉取远程仓库master分支合并到本地，master根据场景换成其它分支名 git add file 添加文件到暂存区，可用 * 添加所有 git commit -m &quot;commit message&quot; 提交到本地版本库，并添加注释，注释表明此次修改内容，要清晰准确 git push origin master 将本地版本提交到远程仓库的master分支，master根据场景换成其它分支名 对大部分日常工作来说， 上面几个命令基本就够用了。 新建项目1. 从本地到远程 项目开发的时候，有时候是先在本地建一个项目，再提交到远程仓库的。 创建项目目录（或通过IDE创建），命令行cd到项目目录 执行git init ， 将在项目目录创建.git目录 执行git add * ，将所有文件添加到暂存区，这里要先创建一个.gitignore文件，将不需要版本维护的文件添加进去忽略，不然各种IDE编译文件夹，环境相关文件都加到版本库去了。删除文件用git rm file_name 执行git commit -m &quot;upload project&quot; ，提交到本地仓库 在gitlab或github上创建一个仓库，并将仓库地址复制下来 执行git remote add origin git@server-name:path/repo-name.git ，关联远程仓库，仓库地址如果是http开头则要用户名密码，如果是git开头，则是走的ssh协议，需要将你本机的ssh公钥添加到远程仓库服务上。 执行git push -u origin master ，推送本地仓库内容到远程仓库 这样在远程仓库目录，就能看到你提交上去的文件内容了。 2. 从远程到本地更多的时候，是远程仓库已有项目了，需要下载到本地开发。 git clone git@server-name:path/repo-name.git ， 将远程仓库的内容下载到本地，这里仓库地址的处理同上 修改内容 git add * ，将修改的内容添加到暂存区 git commit -m &quot;fix xxx issue&quot; ，提交到本地仓库 git push -u origin master ， 推送本地仓库内容至远程仓库 版本回退有时候改了文件，想反悔怎么办，git给你“后悔药”。 单个文件的还原： git checkout file_name ，丢弃工作区的修改，还原到上次提交（commit）的版本， git reset HEAD file_name ，把暂存区的修改撤销掉（unstage），重新放回工作区。即还原到上次添加到暂存区（add）的版本 这里涉及几个场景 场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout file_name。 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时（执行了add，但没执行commit），想丢弃修改，分两步，第一步用命令git reset HEAD file_name，就回到了场景1，第二步按场景1操作。 场景3：已经提交了不合适的修改到版本库时，想要撤销本次的全部提交，参考下面的整个版本的还原，不过前提是没有推送到远程库。 整个版本的还原： git reset --hard HEAD^^， 回退到上上个版本 git reset --hard 3628164， 回退到具体某个版本 3628164 是具体某个commit_id缩写 找不到commit_id？ git reflog 可查看每一个命令的历史记录，获取对应操作的commit_id。git log [--pretty=oneline]， 可查看commit记录 上一个版本就是HEAD^，上上一个版本就是HEAD^^，往上100个版本写成HEAD~100。3628164 是具体某个commit_id，不需要写全，只需要唯一确定就行，可往前进也可往后退。（git windows2.20.1版貌似不支持对HEAD^的操作） 多人协作 首先，可以试图用 git push origin branch_name 推送自己的修改； 如果推送失败，则因为远程分支比你的本地更新，需要先用 git pull 试图合并； 如果合并有冲突，则手动解决冲突，并在本地提交； 没有冲突或者解决掉冲突后，再用 git push origin branch-name 推送就能成功！ 如果git pull提示“no tracking information”，则说明本地分支和远程分支的链接关系没有创建，用命令git branch –set-upstream branch-name origin/branch-name 在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致 分支管理平时开发时需要创建子分支来实现你的功能模块，然后再合并到主分支中。 git checkout -b your_branch_name ， 创建并切换分支 git branch ， 查看分支，标有*号表示当前所在分支 git merge dev ， 合并指定dev分支到当前分支 git merge --no-ff -m &quot;merge with no-ff&quot; dev ， 合并分支并生成commit记录 git branch -d dev ， 删除分支 git checkout -b dev = git branch dev + git checkout dev Fast-forward合并，“快进模式”，也就是直接把master指向dev的当前提交，所以合并速度非常快。存在冲突的不能fast forward。git merge --no-ff -m &quot;merge with no-ff&quot; dev Fast forward模式下，删除分支后，会丢掉分支信息。如果强制禁用Fast forward模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息 标签管理当发布版本时，一般需要对当前版本进行标签记录，以便后续进行版本查看或回退。 git tag tag_name ， 对当前分支打标签 git tag ， 查看所有标签 git tag v0.9 6224937 ，针对某个具体commit id打标签 git show tag_name ， 查看标签信息 git tag -a v0.1 -m &quot;version 0.1 released&quot; 3628164 ， 带有说明的标签 git tag -d v0.1 ， 删除标签 git push origin tag_name ， 推送标签到远程 git push origin --tags ， 一次性推送所有标签 删除已经推送到远程的标签： git tag -d v0.9 ， 先本地删除 git push origin :refs/tags/v0.9 ， 然后从远程删除 提高效率的Tips 配置命令别名 123git config --global alias.st status # 后面可以用git st 来代替git status了git config --global alias.ck checkout # 后面可以用 git ck 来代替 git checkout了git config --global alias.cm 'commit -m' # 后面可以用git cm 来代替 git commit -m 了 git pull origin master 或 git push origin master， 可直接 git pull 或 git push， 如果出现“no tracking information”的提示，则说明本地分支和远程分支的链接关系没有创建，用命令 git branch --set-upstream-to=origin/master master 建立关联即可。 总结以上命令虽然看起来多，但平常用的最频繁的应该是“基本命令”与“分支管理”部分，只要多用几次，自然便能记住，应付日常工作完全没有问题，彻底脱离GUI操作，让工作更有效率。 我的个人博客地址：http://blog.jboost.cn 我的头条空间： https://www.toutiao.com/c/user/5833678517/#mid=1636101215791112 我的github地址：https://github.com/ronwxy 我的微信公众号：jboost-ksxy ———————————————————————————————————————— 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.jboost.cn/categories/DevOps/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.jboost.cn/tags/git/"}]},{"title":"案例解析：线程池使用不当导致系统崩溃","slug":"issue-threadpool","date":"2019-06-15T03:01:58.000Z","updated":"2020-08-26T01:38:21.306Z","comments":true,"path":"issue-threadpool.html","link":"","permalink":"http://blog.jboost.cn/issue-threadpool.html","excerpt":"前几天，发现一台阿里云服务器上的Web服务不可用。远程SSH登录不上，尝试几次登录上去之后，执行命令都显示 1-bash: fork: Cannot allocate memory 一看以为是内存泄漏导致溢出。因为执行不了任何命令， 只能通过控制台重启服务器恢复服务。","text":"前几天，发现一台阿里云服务器上的Web服务不可用。远程SSH登录不上，尝试几次登录上去之后，执行命令都显示 1-bash: fork: Cannot allocate memory 一看以为是内存泄漏导致溢出。因为执行不了任何命令， 只能通过控制台重启服务器恢复服务。 初步排查服务恢复后，查看系统日志，linux系统日志路径/var/log/messages，可通过journalctl命令查看，如 12journalctl --since=\"2019-06-12 06:00:00\" --until=\"2019-06-12 10:00:00\"` 可查看since之后，until之前时间段的日志。除了发现crond[14954]: (CRON) CAN&#39;T FORK (do_command): Cannot allocate memory 这个错误日志，未见其它异常（下面的sshd[10764]: error: fork: Cannot allocate memory应是ssh登录执行命名失败的日志） 通过阿里云-云监控-主机监控查看内存使用率指标，这段时间内，内存使用率一直在40%以下，基本可排除内存溢出的可能。 通过搜索查阅到进程数超过操作系统限制可能导致bash: fork: Cannot allocate memory的报错(参考： https://blog.csdn.net/wangshuminjava/article/details/80603847 ）。通过ps -eLf|wc -l查看当前进程线程数(ps -ef只打印进程，ps -eLf会打印所有的线程), 只有1000多个，故障时刻系统到底运行了多少线程已无从得知，只能持续跟进监测。 问题定位几天后，再次通过ps -eLf|wc -l查看，发现线程数已达16000多个。直接执行ps -eLf可看到大量tomcat进程所产生的线程，猜测是不是线程死锁导致大量线程未完成一直hung在那里。 执行 jstack 进程号 &gt; ~/jstack.txt 命令将进程所运行线程情况打印出来分析，发现大量的WAITING状态的线程，如下 1234567891011\"pool-19-thread-1\" #254 prio=5 os_prio=0 tid=0x00007f0b700a6000 nid=0x29a9 waiting on condition [0x00007f0b274df000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &lt;0x00000006ce3d8790&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 根据上述内容可看出线程在等一个条件，并且是在执行LinkedBlockingQueue.take方法的时候，查看该方法的java doc，当队列为空时，该方法将会一直等待直到有元素可用。 12345678/** * Retrieves and removes the head of this queue, waiting if necessary * until an element becomes available. * * @return the head of this queue * @throws InterruptedException if interrupted while waiting */E take() throws InterruptedException; 询问同事在哪里用到了LinkedBlockingQueue，同事回忆起不久前用线程池实现往阿里云OSS服务通过追加的方式上传文件功能，查看代码后发现问题——线程池没有关闭。为了使文件片段保存不存在错乱，每次保存文件时，都new了一个线程池对象， 1ThreadPoolExecutor saveImgThreadPool = new ThreadPoolExecutor(1, 1, 0, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;()); 但处理完后， 没有关闭这个线程池对象，这样线程池仍会通过take方法去取等待队列中是否还有未完成的线程任务，等待队列为空时将会一直等待，这样就导致大量的线程hung在这里了（基本是只要方法被调一次，就会产生一个hung住的线程）。 延伸 线程状态为“waiting for monitor entry”：意味着它 在等待进入一个临界区 ，所以它在”Entry Set“队列中等待。此时线程状态一般都是 Blocked：java.lang.Thread.State: BLOCKED (on object monitor) 线程状态为“waiting on condition”：说明它在等待另一个条件的发生，来把自己唤醒，或者干脆它是调用了 sleep(N)。此时线程状态大致为以下几种：java.lang.Thread.State: WAITING (parking)：一直等那个条件发生（本文案例即为此种场景）；java.lang.Thread.State: TIMED_WAITING (parking或sleeping)：定时的，那个条件不到来，也将定时唤醒自己。 如果大量线程在“waiting for monitor entry”：可能是一个全局锁阻塞住了大量线程。如果短时间内打印的thread dump 文件反映，随着时间流逝，waiting for monitor entry 的线程越来越多，没有减少的趋势，可能意味着某些线程在临界区里呆的时间太长了，以至于越来越多新线程迟迟无法进入临界区。 如果大量线程在“waiting on condition”：可能是它们又跑去获取第三方资源，尤其是第三方网络资源，迟迟获取不到Response，导致大量线程进入等待状态。所以如果你发现有大量的线程都处在 Wait on condition，从线程堆栈看，正等待网络读写，这可能是一个网络瓶颈的征兆，因为网络阻塞导致线程无法执行。也可能是如本文所提到的，由于程序编写不当所致。 参考： https://www.cnblogs.com/rainy-shurun/p/5732341.html 我的个人博客地址：http://blog.jboost.cn 我的头条空间： https://www.toutiao.com/c/user/5833678517/#mid=1636101215791112 我的github地址：https://github.com/ronwxy 我的微信公众号：jboost-ksxy —————————————————————————————————————————————————— 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.jboost.cn/categories/Java/"}],"tags":[{"name":"threadpool","slug":"threadpool","permalink":"http://blog.jboost.cn/tags/threadpool/"}]},{"title":"Spring Boot从入门到实战（五）：写一个自己的starter","slug":"springboot-starter","date":"2019-06-14T07:19:43.000Z","updated":"2019-07-24T01:52:22.312Z","comments":true,"path":"springboot-starter.html","link":"","permalink":"http://blog.jboost.cn/springboot-starter.html","excerpt":"曾遇到几位面试者，简历上写着精通Spring Boot，当聊到自动配置及对starter的理解时，却说不出个所以然来。找工作时，简历一定要注重实际，精通这种字眼还是少用，不然面试官对你期望越高，失望也就越大。其实结合前一篇介绍的Spring Boot自动配置，对Spring Boot的Starter实现将很容易理解，不论是使用其官方提供的Starter，还是自定义自己的Starter，都变得很容易。","text":"曾遇到几位面试者，简历上写着精通Spring Boot，当聊到自动配置及对starter的理解时，却说不出个所以然来。找工作时，简历一定要注重实际，精通这种字眼还是少用，不然面试官对你期望越高，失望也就越大。其实结合前一篇介绍的Spring Boot自动配置，对Spring Boot的Starter实现将很容易理解，不论是使用其官方提供的Starter，还是自定义自己的Starter，都变得很容易。 根据前面介绍，Spring Boot自动配置的实现，主要由如下几部分完成： @EnableAutoConfiguration注解 SpringApplication类 spring-boot-autoconfigure jar包 spring.factories文件 项目结构官方提供的starter，大多包含两个jar包： 一个starter——没有任何实现，只用来管理依赖（即实现这个starter的功能需要依赖哪些jar），一个autoconfigure——包含所有具体实现，包括自动配置类，及META-INF/spring.factories文件。本文示例的自定义starter，为了方便，将两者合并写到了一个。 但是在实际项目中，还是建议像官方一样，定义一个spring-boot-dependencies声明所有依赖及其版本，做统一依赖版本管理，一个spring-boot-autoconfigure，实现所有自动配置类及相应的Bean，一个spring-boot-starters，针对每个模块引入必须的jar依赖，方便项目中引入。 官方提供的starter，命名遵循spring-boot-starter-xxx， 自定义starter，命名遵循xxx-spring-boot-starter。 示例的项目结构如下图 springboot-starter这里为了简单，将starter与autoconfigure整到一个项目，命名也为了与前面demo项目保持一致，没按规范来。 配置类 MyAutoConfig 12345678910111213@Configuration@EnableConfigurationProperties(MyProperties.class)public class MyAutoConfig &#123; @Autowired private MyProperties myProperties; @Bean @ConditionalOnProperty(prefix = \"my\", name = \"disable\", havingValue = \"false\") public MyService myService()&#123; return new MyService(\"Hi \" + myProperties.getName() + \", welcome to visit \" + myProperties.getWebsite()); &#125;&#125; 该类中通过@EnableConfigurationProperties及@Autowired 引入了配置属性Bean MyProperties 以访问用户配置的属性，@Bean注解即向容器中注入方法返回值类型的Bean，这样在容器其它bean中通过@Autowired即可引用访问， @ConditionalOnProperty是条件注解，这里表明当配置属性my.disable=false时才实例化这个MyService bean。 配置属性类 MyProperties 1234567@ConfigurationProperties(prefix = \"my\")public class MyProperties &#123; private String name; private String website; getter/setter;&#125; 配置属性类封装了用户在配置文件中定义的属性，该示例中将前缀为my的属性封装起来，访问name，website对应配置属性key就是my.name，my.website。 服务Bean MyService 1234567891011public class MyService &#123; private String hiStr; public MyService(String hiStr)&#123; this.hiStr = hiStr; &#125; public String sayHi()&#123; return this.hiStr; &#125;&#125; 提供服务功能的bean，也即需要实例化注入到Spring上下文的bean。 spring.factories 12org.springframework.boot.autoconfigure.EnableAutoConfiguration&#x3D;\\ cn.jboost.springboot.starter.MyAutoConfig 指定了自动配置类（带包名的全路径类名） springboot-usingstarter该项目引用springboot-starter，调用MyService服务的项目，主类没什么特别的 1234567@SpringBootApplicationpublic class SpringbootUsingstarterApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbootUsingstarterApplication.class, args); &#125;&#125; 配置文件application.properties 123my.disable&#x3D;falsemy.name&#x3D;jboostmy.website&#x3D;blog.jboost.cn 在测试类SpringbootUsingstarterApplicationTests中编写测试 1234567@Autowiredprivate MyService myService;@Testpublic void testStarter()&#123; System.out.printf(myService.sayHi());&#125; pom.xml中引入springboot-starter依赖 12345678910111213 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;cn.jboost.springboot&lt;/groupId&gt; &lt;artifactId&gt;springboot-starter&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 运行，控制台会打印出 Hi jboost, welcome to visit blog.jboost.cn将配置属性my.disable的值改为true或其它非false的值再运行测试代码试试，会报MyService bean找不到的错误，说明@ConditionalOnProperty注解生效了 本示例仅作实现自定义starter演示用，项目结构、命名都不够规范，仅供参考，项目实战starter在后面继续分享。 本文示例项目源码地址：https://github.com/ronwxy/springboot-demos/tree/master/springboot-starterhttps://github.com/ronwxy/springboot-demos/tree/master/springboot-usingstarter我的个人博客地址：http://blog.jboost.cn我的头条空间： https://www.toutiao.com/c/user/5833678517/#mid=1636101215791112我的github地址：https://github.com/ronwxy我的微信公众号：jboost-ksxy —————————————————————————————————————————————————— 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.jboost.cn/categories/SpringBoot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"}]},{"title":"Spring Boot从入门到实战（四）：Spring Boot配置","slug":"springboot-config","date":"2019-06-11T07:46:02.000Z","updated":"2019-07-22T10:28:12.660Z","comments":true,"path":"springboot-config.html","link":"","permalink":"http://blog.jboost.cn/springboot-config.html","excerpt":"Spring Boot之所以受开发者欢迎， 其中最重要的一个因素就是其配置简单。传统的Spring应用需要手动配置各种.xml文件，为数据库访问，事务支持，缓存功能等提供各项繁杂且重复的配置。Spring Boot将这种繁杂且重复的工作通过预定义的启动器（starter）来实现，只要引入即可拥有相应的功能支持，从而将开发者从复杂的配置工作中解放出来，能够更专注于业务逻辑的开发。","text":"Spring Boot之所以受开发者欢迎， 其中最重要的一个因素就是其配置简单。传统的Spring应用需要手动配置各种.xml文件，为数据库访问，事务支持，缓存功能等提供各项繁杂且重复的配置。Spring Boot将这种繁杂且重复的工作通过预定义的启动器（starter）来实现，只要引入即可拥有相应的功能支持，从而将开发者从复杂的配置工作中解放出来，能够更专注于业务逻辑的开发。 配置方式在Spring Boot中，虽然仍然可以通过之前的.xml文件方式来进行配置，但最好还是通过基于java的配置来进行配置管理。在Spring Boot中，基于java的配置是通过注解@Configuration来实现的 12345678@Configurationpublic class MyConfig &#123; @Bean public MyService myService()&#123; return new MyService(); &#125;&#125; 上述代码将一个MyService的Bean注入了容器，这样在其它地方就可以直接通过@Autowired来引用访问。与.xml文件中通过&lt;bean&gt;&lt;/bean&gt;实例化的效果是一样的。 1234567@Autowiredprivate MyService myService;@RequestMapping(\"/hi\")public String sayHello(@RequestParam String name)&#123; return myService.sayHello(name);&#125; 实际项目开发中，有可能存在一些基于xml配置的旧服务，比如以jar包的形式发布，如果要复用该怎么引入呢？很简单，在@Configuration注解标注的类上，加入@ImportResource注解引用相应的xml文件即可， 123456789@Configuration@ImportResource(\"spring.xml\")public class MyConfig &#123; @Bean public MyService myService()&#123; return new MyService(); &#125;&#125; 这样类路径下spring.xml配置文件中声明的内容都将生效。在一个应用中，可以定义多个@Configuration配置类，这些配置类可以被@ComponentScan自动扫描并注入容器。 如果应用中没有通过@ComponentScan进行自动扫描，则可在主配置类（一般为入口类）上通过@Import({MyConfig.class})的方式类引入其它配置类 自动配置个人认为，自动配置是Spring Boot非常基础但又核心的部分。曾经遇到几个面试者，简历写着精通Spring Boot，当问及自动配置时却支支吾吾不知所云。其实理解Spring Boot的自动配置也不难，基本了解如下几部分差不多就够了： @EnableAutoConfiguration注解 SpringApplication类 spring-boot-autoconfigure jar包 spring.factories文件 @EnableAutoConfiguration注解这个注解的作用是告诉Spring Boot基于添加的jar依赖来自动配置Spring，比如添加了spring-boot-starter-web依赖，则Spring Boot认为你在开发一个web应用，就会自动做好web相应配置。这个注解一般放在主类上。在前面的示例项目中， 我们在主类上都是使用@SpringBootApplication， 查看源码可以知道： @SpringBootApplication 这个注解实际上等效于 @SpringBootConfiguration（等效于@Configuration）， @EnableAutoConfiguration，启用自动配置 @ComponentScan 自动扫描@Component, @Service, @Controller等注解标注的各类组件 三者的组合。如果去掉@EnableAutoConfiguration注解，则Spring Boot将不会自动配置Spring（如实例化必要的Bean），将可能导致应用启动失败。 SpringApplication类在应用主类中，我们是通过SpringApplication的run方法来启动应用的，如： 1234567@SpringBootApplicationpublic class SpringbootConfigApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbootConfigApplication.class, args); &#125;&#125; 查看源码，SpringApplication的静态run方法，实际也是通过创建SpringApplication实例，调用实例方法执行，在SpringApplication构造器方法中，调用了getSpringFactoriesInstances 方法， 12345678910public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources&#123; this.resourceLoader = resourceLoader; Assert.notNull(primarySources, \"PrimarySources must not be null\"); this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources)); this.webApplicationType = WebApplicationType.deduceFromClasspath(); setInitializers((Collection) getSpringFactoriesInstances( ApplicationContextInitializer.class)); setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); this.mainApplicationClass = deduceMainApplicationClass();&#125; 追溯下去，最终会调用到SpringFactoriesLoader的loadSpringFactories方法， 123456789101112131415161718192021222324252627private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) &#123; ... try &#123; Enumeration&lt;URL&gt; urls = (classLoader != null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION)); result = new LinkedMultiValueMap&lt;&gt;(); while (urls.hasMoreElements()) &#123; URL url = urls.nextElement(); UrlResource resource = new UrlResource(url); Properties properties = PropertiesLoaderUtils.loadProperties(resource); for (Map.Entry&lt;?, ?&gt; entry : properties.entrySet()) &#123; String factoryClassName = ((String) entry.getKey()).trim(); for (String factoryName : StringUtils.commaDelimitedListToStringArray((String) entry.getValue())) &#123; result.add(factoryClassName, factoryName.trim()); &#125; &#125; &#125; cache.put(classLoader, result); return result; &#125; catch (IOException ex) &#123; throw new IllegalArgumentException(\"Unable to load factories from location [\" + FACTORIES_RESOURCE_LOCATION + \"]\", ex); &#125;&#125; 在该方法中，会从所有的META-INF目录下加载spring.factories文件里配置的各类型的类名称（包括初始化器，监听器，自动配置类等）。然后上层方法中通过反射机制实例化这些初始化器、监听器，自动配置等，从而完成相应Bean的自动化配置与注入。 spring-boot-autoconfigure 官方提供的starter，如spring-boot-starter-web， 都依赖了spring-boot-starter， 而spring-boot-starter又依赖了spring-boot-autoconfigure。 在spring-boot-autoconfigure中提供了大量官方提供的自动配置类，并且包含META-INF/spring.factories文件，如下图 spring.factories 由上图可看出，spring.factories包含了 org.springframework.context.ApplicationContextInitializer 应用初始化器 org.springframework.context.ApplicationListener 应用监听器 org.springframework.boot.autoconfigure.AutoConfigurationImportListener 自动配置引入监听器 org.springframework.boot.autoconfigure.AutoConfigurationImportFilter 自动配置引入过滤器 org.springframework.boot.autoconfigure.EnableAutoConfiguration 自动配置类 org.springframework.boot.diagnostics.FailureAnalyzer 失败分析器 org.springframework.boot.autoconfigure.template.TemplateAvailabilityProvider 模板提供者 其中org.springframework.boot.autoconfigure.EnableAutoConfiguration即实现自动配置的@Configuration配置类列表。 Spring Boot就是通过这种自动配置机制，以starter依赖包的方式，使开发者非常方便地使用项目开发中的许多常用功能，如数据库访问、缓存、队列等。同时，用户也可以根据自身需求，自定义自己的starter（后面介绍）。 通过注解控制自动配置Spring Boot自动配置包含了许多条件类注解及顺序类注解，这些注解可方便地让自动配置按照某种条件或者顺序进行配置。 其中条件类注解包括： 类级别条件注解 @ConditionalOnClass： 类路径中存在指定的类才进行该配置；@ConditionalOnMissingClass： 类路径中不存在指定的类才进行该配置 实例级别条件注解 @ConditionalOnBean：只有在当前上下文中存在指定Bean时，才进行该配置@ConditionalOnMissingBean： 只有在当前上下文不存在指定Bean时，才进行该配置 属性级别条件注解 @ConditionalOnProperty：当存在某个指定属性，且值为指定值时，才进行该配置 资源级别条件注解 @ConditionalOnResource：在类路径下存在指定的Resource时，才进行配置 Web应用条件注解 @ConditionalOnWebApplication：该应用为Web应用时进行该配置@ConditionalOnNotWebApplication： 该应用不为Web应用时进行该配置 SpEL（ Spring Expression Language）表达式注解 @ConditionalOnExpression： 计算SpEL表达式值，值为true时才进行该配置 顺序类注解包括： @AutoConfigureAfter： 在指定的配置类初始化后再加载 @AutoConfigureBefore： 在指定的配置类初始化前加载 @AutoConfigureOrder： 数值越小越先初始化 注意：自动配置类不应该位于组件扫描路径（@ComponentScan注解指定的扫描路径）下，否则上述条件注解与顺序注解可能不会生效。建议只在自动配置的类上注解@ConditionalOnBean， @ConditionalOnMissingBean，因为这可以保证在用户定义bean已经添加到ApplicationContext之后才会加载。这两个注解放在class上，则相当于class里面每一个@Bean标注的方法都加上了。 自动配置是非侵入式的，你可以在任何地方自定义配置来覆盖自动配置中的某些内容，比如你在应用中通过@Configuration类注入一个自定义的DataSource，默认的基于内存的DataSource将被覆盖 禁用某个自动配置类有时候引入的自动配置可能包含我们不想让其生效的配置类，这时候可以通过@EnableAutoConfiguration注解的属性进行排除，使其不生效。 1@EnableAutoConfiguration(exclude = &#123;XXAutoConfiguration.class&#125;) 其中XXAutoConfiguration为某个自动配置类，如果该类不在应用的类路径中，则可以通过属性excludeName指定完整类路径来排除。@SpringBootApplicationz注解同样支持 1@SpringBootApplication(exclude = &#123;XXAutoConfiguration.class&#125;) 本文示例项目源码地址：https://github.com/ronwxy/springboot-demos/tree/master/springboot-config我的个人博客地址：http://blog.jboost.cn我的头条空间： https://www.toutiao.com/c/user/5833678517/#mid=1636101215791112我的github地址：https://github.com/ronwxy我的微信公众号：jboost-ksxy —————————————————————————————————————————————————— 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.jboost.cn/categories/SpringBoot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"}]},{"title":"Spring Boot从入门到实战（三）：Spring Boot自定义属性","slug":"springboot-properties","date":"2019-06-10T10:47:51.000Z","updated":"2020-01-13T11:34:42.896Z","comments":true,"path":"springboot-properties.html","link":"","permalink":"http://blog.jboost.cn/springboot-properties.html","excerpt":"Web项目开发中，经常需要自定义一些属性，如数据库连接，第三方服务接口地址，第三方服务的appKey、appSecret等，以及针对不同环境，这些属性的值还需要有相应的调整，如开发环境、测试环境、生产环境所用数据库不同，则针对不同环境的同一属性需要配置不同的值。","text":"Web项目开发中，经常需要自定义一些属性，如数据库连接，第三方服务接口地址，第三方服务的appKey、appSecret等，以及针对不同环境，这些属性的值还需要有相应的调整，如开发环境、测试环境、生产环境所用数据库不同，则针对不同环境的同一属性需要配置不同的值。 传统自定义属性配置及访问（参考Github示例测试类）在传统的Spring Web应用中，自定义属性一般是通过在类路径中（如resources目录）添加一个类似my.properties配置文件（文件名自定义），然后在xml配置中通过 1&lt;util:properties id=\"myProps\" location=\"classpath:my.properties\"/&gt; 引入属性文件。再定义一个Bean来读取这些属性，Bean配置： 12345678&lt;bean class=\"org.springframework.beans.factory.config.MethodInvokingFactoryBean\"&gt; &lt;property name=\"staticMethod\" value=\"cn.jboost.springboot.properties.MyPropertiesUtil.init\"/&gt; &lt;property name=\"arguments\"&gt; &lt;list&gt; &lt;ref bean=\"myProps\"/&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; Bean定义： 123456789101112public class MyPropertiesUtil &#123; private static Properties properties; public static void init(Properties props) &#123; properties = props; &#125; public static String getValue(String key) &#123; return properties.getProperty(key); &#125;&#125; 在其它需要访问的地方通过 MyPropertiesUtil.getValue() 方法来访问具体某个属性的值。 Spring Boot自定义属性配置及优先级在Spring Boot中，可以在多个地方配置属性，包括.properties文件，.yaml文件，环境变量， 系统属性，命令行参数等， 这些属性都会被Spring Boot加载到Environment中，可通过@Value注解，Environment实例，或 @ConfigurationProperties注解的类来访问。 属性加载优先级顺序： 如果有使用devtools，devtools 全局设置的属性（用户目录 ~/.spring-bootdevtools.properties） 测试类的注解@TestPropertySource 测试类注解 @SpringBootTest#properties 配置的属性 命令行参数 SPRING_APPLICATION_JSON里的属性（环境变量或系统属性） ServletConfig初始化参数 ServletContext初始化参数 JNDI参数 java:comp/env Java系统属性 System.getProperties() 操作系统环境变量 RandomValuePropertySource 配置的属性 random.* jar包外部的applictaion-{profile}.properties，applictaion-{profile}.yml配置文件 jar包内部的applictaion-{profile}.properties，applictaion-{profile}.yml配置文件 jar包外部的applictaion.properties，applictaion.yml配置文件 jar包内部的applictaion.properties，applictaion.yml配置文件 @Configuration类上的 @PropertySource注解指定的配置文件 默认属性： SpringApplication.setDefaultProperties 上述属性配置，除了粗体标注的外，其它一般应用较少。序号低的配置优先级高于序号高的配置，即如果存在相同属性配置 ，则序号低的配置会覆盖序号高的配置。applictaion-{profile}.properties 一般用于具体某个环境特有的属性配置，如application-dev.properties用于开发环境，可通过 spring.profiles.active=dev指定加载dev环境配置 常用属性配置方式 命令行参数启动Spring Boot应用时，可以指定命令行参数，如： 1java -jar springboot-properties.jar --my.name=jboost@command_line 该参数值将会覆盖应用在其它地方配置的同名属性值。命令行参数放在xx.jar 的后面。 可以通过SpringApplication.setAddCommandLineProperties(false) 禁用命令行参数配置 Java系统属性同样在启动Spring Boot应用时，可以指定Java系统属性，一般见于自定义jvm参数，如： 1java -Dmy.name=jboost@system_properties -jar springboot-properties.jar Java系统属性放在java命令之后。 操作系统环境变量（实际应用其实较少）配置过JAVA_HOME的应该理解何为环境变量。某些操作系统可能不支持.分隔的属性名，可以改为以下划线连接。Spring Boot将myName, my.name, MY_NAME视为等效。 应用属性配置文件（.properties文件或 .yml文件）.properties文件属性配置格式： 123my.name&#x3D;jboostmy.list[0]&#x3D;aaa &#x2F;&#x2F;配置列表my.list[1]&#x3D;bbb .yml文件属性配置格式： 12345my: name: devlink list: //配置列表 - aaa - bbb yml中，属性名与值之间冒号后面必须有空格。 应用属性配置文件位置： jar包所在当前目录下的子目录/config（外置属性文件） jar包所在当前目录（外置属性文件） classpath根目录下的子目录/config（内置属性文件） classpath根目录（内置属性文件） 序号低的优先级高于序号高的优先级，即jar包外的配置优先级高于jar包内的配置。同一目录下，.properties文件的优先级高于.yml文件。application-{profile}.properties的优先级高于application.properties。 Spring Boot自定义属性访问方式（参考Github示例测试类） 类中属性上添加 @Value(“${xx}”) 注解方式。如： 12@Value(\"$&#123;my.name&#125;\")private String name; 可以指定默认值，如 @Value(“${my.name:jboost}”)， 当my.name未配置时，默认使用值”jboost” 通过@ConfigurationProperties注解的类来访问。如定义： 12345678@Component@ConfigurationProperties(prefix = \"my\")public class MyConfigProperties &#123; private String name; private String website; //省略了getter、setter函数&#125; 然后在需要访问的Bean中，通过@Autowired 注入MyConfigProperties实例，通过getName()方法即可访问my.name属性值。 123456789@Autowiredprivate MyConfigProperties myConfigProperties;@Testpublic void testConfigurationProperties()&#123; System.out.println(\"test @ConfigurationProperties ==========\"); System.out.println(myConfigProperties.getName()); System.out.println(myConfigProperties.getWebsite());&#125; 通过Environment 实例访问。如： 123456789@Autowiredprivate Environment env;@Testpublic void testEnvironment()&#123; System.out.println(\"test Environment ==========\"); System.out.println(env.getProperty(\"my.name\")); System.out.println(env.getProperty(\"my.website\", \"default value\"));&#125; 另外也可以通过 spring-boot-starter-actuator 的接口来查看项目加载的属性配置，在pom.xml中加入 spring-boot-starter-actuator 依赖，因为 spring-boot-starter-actuator 在2.x版本中，出于安全性考虑，将actuator 控件中的端口，只默认开放/health 和/info 两个端口，其他端口默认关闭，因此需要添加配置management.endpoints.web.exposure.include= *，management.endpoints.web.exposure.exclude=beans,trace，management.endpoint.health.show-details=ALWAYS，启动项目后，访问 http://localhost:8080/actuator/env ，返回的 propertySources 即为加载的所有属性源，优先级从上往下依次降低，与上文所述优先级相符 本文示例项目源码地址：https://github.com/ronwxy/springboot-demos/tree/master/springboot-properties 我的个人博客地址：http://blog.jboost.cn 我的头条空间： https://www.toutiao.com/c/user/5833678517/#mid=1636101215791112 我的github地址：https://github.com/ronwxy 我的微信公众号：jboost-ksxy ——————————————————————————————————————————————————————————————— 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.jboost.cn/categories/SpringBoot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"}]},{"title":"Spring Boot从入门到实战（二）：第一个Spring Boot应用","slug":"springboot-firstapp","date":"2019-06-06T12:46:50.000Z","updated":"2019-07-22T10:27:17.454Z","comments":true,"path":"springboot-firstapp.html","link":"","permalink":"http://blog.jboost.cn/springboot-firstapp.html","excerpt":"Spring Boot应用可以通过如下三种方法创建： 通过 https://start.spring.io/ 网站创建 通过 Spring Initializr 创建 自主创建","text":"Spring Boot应用可以通过如下三种方法创建： 通过 https://start.spring.io/ 网站创建 通过 Spring Initializr 创建 自主创建 推荐开发工具 JDK 1.8+ IntelliJ IDEA maven 3.3+ 在开始之前，先确认是否安装上述工具，在命令行输入 java -version 查看JDK是否正确安装， 输入 mvn -version 查看maven是否正确安装，如果未正确安装，请先查阅相关文档完成安装。 12345678910111213PS D:\\&gt; java -versionjava version \"1.8.0_201\"Java(TM) SE Runtime Environment (build 1.8.0_201-b09)Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)PS D:\\&gt;PS D:\\&gt;PS D:\\&gt; mvn -versionApache Maven 3.6.0 (97c98ec64a1fdfee7767ce5ffb20918da4f719f3; 2018-10-25T02:41:47+08:00)Maven home: D:\\tool\\apache-maven-3.6.0\\bin\\..Java version: 1.8.0_201, vendor: Oracle Corporation, runtime: C:\\Program Files\\Java\\jdk1.8.0_201\\jreDefault locale: zh_CN, platform encoding: GBKOS name: \"windows 10\", version: \"10.0\", arch: \"amd64\", family: \"windows\"PS D:\\&gt; 1. 通过 https://start.spring.io/ 网站创建进入 https://start.spring.io/，填写对应的信息，如下图所示其中project选 Maven Project， Spring Boot版本选 2.1.5 版， Project Metadata部分， Group一般用你域名的倒序字符串，Artifact即项目名称，选择Packaging类型为Jar，Java版本为8，在Dependencies部分输入Web，选中第一个Spring Web Starter，然后点击“Generate the project”按钮，下载生成的项目。解压项目，在IntelliJ IDEA中 File -&gt; Open 选中项目解压目录打开，即可看到生成的项目结构如下图具体各文件含义后面详述。 2. 通过Spring Initializr创建（推荐）IntelliJ IDEA中File -&gt; New -&gt; Project...打开新建项目窗口（这里也可以选择New Module, IDEA的Project类似于Eclipse的Workspace，Module则类似于Eclipse的Project，有时候为了将一些项目统一管理，可以建一个Project，然后在Project内部建立Module），如下图所示 选择Spring Initializr，点击Next，填写相应信息， 如下图所示 点击Next，选择Spring Boot版本以及相应依赖，如下图（这里选择2.1.5版本及Spring Web Starter依赖） 然后依次点击Next, Finish完成项目创建。可以看到创建的项目结构与第一种方法一致。 有的旧IDEA版本下项目可能不能编译，IDE未将其识别为maven项目，只需在pom.xml文件上右键，点击Add as Maven project即可。 3. 自主创建自主创建即像普通Java Maven项目一样，先创建maven项目，然后参考1、2方法中创建的项目结构与目录，手动进行添加。 上述三种创建方法，第1种需要网站生成再下载解压导入，第2种直接基于IDE创建，第3种完全自主手动创建。实际开发中推荐采用第2种创建初始项目原型，再根据具体需求删除或添加相应目录与文件。 4. 项目结构通过上述方法创建的项目，结构如下图所示 其中 SpringbootFirstappApplication 为项目入口类，通过SpringApplication.run()方法来启动项目 入口类上的注解 @SpringBootApplication 表明，这是一个Spring Boot项目，它会为你自动做一些Spring Boot项目的处理 resources 下的static目录为静态资源目录，可以放置js，css，img之类的资源，templates目录可放置模板文件，一般做前后端分离开发，这两个目录可删除 application.properties 文件为项目的配置文件，可在该文件中配置项目所需要的各项配置属性 SpringbootFirstappApplicationTests 生成的测试类，可基于此进行单元测试编写 pom.xml即为maven配置文档，可看到项目已继承spring-boot-starter-parent，并且引入了spring-boot-starter-web，spring-boot-starter-test两项依赖，以及spring-boot-maven-plugin 5. 运行 上述创建的项目可直接运行，大致有如下几种运行方式： 直接在项目入口类SpringbootFirstappApplication中右键，点击Run &#39;SpringbootFirstappAp...&#39;运行 在项目根目录下打开终端，或IDEA的Terminal中执行mvn spring-boot:run（前提是项目pom.xml文件中引入了spring-boot-maven-plugin） 使用mvn package打包，然后通过java -jar target\\springboot-firstapp-1.0.0-SNAPSHOT.jar 启动（一般用于远程环境的部署启动） 如果打包成war，将war包部署到tomcat等Servlet容器运行 项目启动后，从启动日志可看出默认端口为8080，但打开 http://localhost:8080 会显示一个404报错页面，这是因为我们还没有编写任何服务。下面我们添加一个非常简单的Rest服务接口，在项目的根包下（我这里是cn.jboost.springboot.firstapp，实际项目中一般会创建一个controller的子包）添加HelloController类，代码如下 1234567@RestController(\"/hello\")public class HelloController &#123; @GetMapping public String hello(@RequestParam(name = \"name\")String name)&#123; return \"您好，\" + name; &#125;&#125; 其中@RestController注解会将返回结果以字符串的方式解析，@GetMapping等效于@RequestMapping(method = {RequestMethod.GET})重启应用，然后浏览器地址栏中输入 http://localhost:8080/hello?name=jboost， 页面输出如下图： ![接口调用](/assets/firstapp7.png) 至此，一个可运行的Web项目即已搭建完成，是不是非常简单。 本文示例项目源码地址：https://github.com/ronwxy/springboot-demos/tree/master/springboot-firstapp我的个人博客地址：http://blog.jboost.cn我的头条空间： https://www.toutiao.com/c/user/5833678517/#mid=1636101215791112我的github地址：https://github.com/ronwxy我的微信公众号：jboost-ksxy ——————————————————————————————————————————————— 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.jboost.cn/categories/SpringBoot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"}]},{"title":"Spring Boot从入门到实战（一）：Spring Boot简介","slug":"springboot-overview","date":"2019-06-06T06:29:02.000Z","updated":"2019-07-22T10:26:52.326Z","comments":true,"path":"springboot-overview.html","link":"","permalink":"http://blog.jboost.cn/springboot-overview.html","excerpt":"Spring Boot这几年非常流行，差不多是基于Spring框架应用开发的首选，同时在微服务架构领域，如Spring Cloud 框架中，Spring Boot也是基础，因此掌握Spring Boot，应成为Java开发人员必不可少的技能。","text":"Spring Boot这几年非常流行，差不多是基于Spring框架应用开发的首选，同时在微服务架构领域，如Spring Cloud 框架中，Spring Boot也是基础，因此掌握Spring Boot，应成为Java开发人员必不可少的技能。 简述传统的基于Spring的Java Web应用，需要配置 web.xml, applicationContext.xml 等大量xml配置信息，然后将应用打成war包放入web应用服务器(如Tomcat, Jetty等)中运行。有过实践经验的开发者应能体会到这个过程繁杂且重复。Spring Boot将这种繁杂且重复的工作通过自动化配置等手段实现，从而将开发者从复杂的配置工作中解放出来，能够更专注于业务逻辑的开发。因此，Spring Boot并不是Spring的替代解决方案，它本身并不提供Spring框架的核心特性以及扩展功能，而是和Spring框架紧密结合用于提升Spring开发者体验，提高开发效率的的工具框架。截至本文，Spring Boot最新GA版本为2.1.5。 特性Spring Boot框架大致包括如下特性： 自动化配置。Spring Boot 通过autoconfiguration的方式（后面会详细讨论何为autoconfiguration）来简化配置管理。比如如果需要访问数据库，则只需要引入相应的starter依赖包，Spring Boot便会自动为你配置访问数据库所需要的Bean，如 DataSource， JdbcTemplate等。使用Spring Boot，项目中几乎不需要任何 xml 配置文件。 内嵌的Web服务容器。Spring Boot内嵌了Tomcat、Jetty、Undertow。因此，Spring Boot应用可以像普通java应用一样打成jar包直接通过 java -jar 执行，而不需传统web应用一样需要打成war包部署到独立的web服务容器中。 简化依赖管理。Spring Boot官方提供了大量的starter依赖包，帮你管理了使用某个功能所需要的依赖，开发者只需要引入starter依赖，即可使用对应的功能。如spring-boot-starter-web，spring-boot-starter-jdbc等。同时自己也可以自定义starter，为某些通用功能提供模块化共享支持。 提供生产环境级的应用配置、度量指标、操作控制接口。Spring Boot的spring-boot-starter-actuator提供了查看应用配置信息，获取应用运行指标，以及控制应用（如关闭应用）三种类型的接口。通过这些接口，可以排查问题，监控服务运行情况等。 Spring Boot的这些特性，使得应用Spring Boot开发Web应用非常便捷、高效，因此在快速应用开发（Rapid Application Development）领域以及微服务架构方面，Spring Boot都是比较好的选择。 工具该序列涉及的开发工具包括但不限于： JDK 1.8+ , 一般用的是1.8 Maven 3.3+ , 我们用的是Maven3.6.0 IntelliJ IDEA Ultimate Edition， 需要激活，参考这里 MySQL，可选，数据库访问示例需要 Redis， 可选，缓存示例需要 我的个人博客地址：http://blog.jboost.cn 我的头条空间： https://www.toutiao.com/c/user/5833678517/#mid=1636101215791112 我的github地址：https://github.com/ronwxy 我的微信公众号：jboost-ksxy ——————————————————————————————————————————————————————————————— 欢迎关注我的微信公众号，及时获取最新分享","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.jboost.cn/categories/SpringBoot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"}]},{"title":"写在前面","slug":"ahead","date":"2019-06-05T08:48:37.000Z","updated":"2019-06-10T07:36:24.244Z","comments":true,"path":"ahead.html","link":"","permalink":"http://blog.jboost.cn/ahead.html","excerpt":"","text":"一点感悟在软件与互联网技术领域从业多年，从一个一知半解的职场菜鸟成长为行业“老司机”，也从一个邯郸学步的技术新手晋升成为能带领团队披荆斩棘，在技术范畴能掌握话语权的技术管理者。其间也与大多数同行一样，踩过不少坑，加过不少班，背过不少锅……，但同时，也为自己不断成长、进步——包括技术、能力层面，也包括薪酬、职位层面，而感到欣慰。但技术领域日新月异，接触的越多，越发现自己的无知，因此 Stay hungry，Stay foolish，保持持续学习的热情，永远不要满足于现状，才能保持自身竞争力，不至于在年龄增长时，出现所谓的“中年危机”。 一点初衷大学期间也曾玩过新浪博客，写过一些心路历程与人生感悟（^_^），随着年龄的增长，逐渐失去了用文字来抒发情感的激情。工作后，开始接触技术博客，也断断续续写过一些分享，但终因阶段性忙或懒惰，没能坚持下来。与之前抒发情感与感悟不同，技术博客更多的是一种经验的自我梳理总结与分享。一方面为那些踏入职场不久实践经验较缺乏的同行提供参考，另一方面也是对自我日常技术工作的整理，以达到“好记性不如烂笔头”的效果。因此，虽然现今从事一线编码工作相对较少，心中一直还是有一个将以往及现在所接触的实践经验记录与分享出来的想法。于是，花了点时间整了这个博客，希望能坚持下去。 一点期望凡事做了，总希望有所回报。整理文章其实需要花费不少时间与精力，因此也希望发出来的分享能为大家带来切实的收获，获得大家的肯定与良性反馈。有更好建议，也欢迎大家通过留言或其它方式与我交流。希望这是一个好的开始，加油！","categories":[],"tags":[]}],"categories":[{"name":"Framework","slug":"Framework","permalink":"http://blog.jboost.cn/categories/Framework/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/categories/SpringCloud/"},{"name":"Career","slug":"Career","permalink":"http://blog.jboost.cn/categories/Career/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.jboost.cn/categories/Kubernetes/"},{"name":"DevOps","slug":"DevOps","permalink":"http://blog.jboost.cn/categories/DevOps/"},{"name":"Java","slug":"Java","permalink":"http://blog.jboost.cn/categories/Java/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.jboost.cn/categories/Docker/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.jboost.cn/categories/SpringBoot/"},{"name":"Architecture","slug":"Architecture","permalink":"http://blog.jboost.cn/categories/Architecture/"},{"name":"Teamwork","slug":"Teamwork","permalink":"http://blog.jboost.cn/categories/Teamwork/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://blog.jboost.cn/tags/springboot/"},{"name":"nacos","slug":"nacos","permalink":"http://blog.jboost.cn/tags/nacos/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://blog.jboost.cn/tags/SpringCloud/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.jboost.cn/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"http://blog.jboost.cn/tags/k8s/"},{"name":"nginx","slug":"nginx","permalink":"http://blog.jboost.cn/tags/nginx/"},{"name":"vue","slug":"vue","permalink":"http://blog.jboost.cn/tags/vue/"},{"name":"java","slug":"java","permalink":"http://blog.jboost.cn/tags/java/"},{"name":"redis","slug":"redis","permalink":"http://blog.jboost.cn/tags/redis/"},{"name":"RateLimiter","slug":"RateLimiter","permalink":"http://blog.jboost.cn/tags/RateLimiter/"},{"name":"限流","slug":"限流","permalink":"http://blog.jboost.cn/tags/%E9%99%90%E6%B5%81/"},{"name":"cpu100%","slug":"cpu100","permalink":"http://blog.jboost.cn/tags/cpu100/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.jboost.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"helm","slug":"helm","permalink":"http://blog.jboost.cn/tags/helm/"},{"name":"jenkins","slug":"jenkins","permalink":"http://blog.jboost.cn/tags/jenkins/"},{"name":"ansible","slug":"ansible","permalink":"http://blog.jboost.cn/tags/ansible/"},{"name":"docker","slug":"docker","permalink":"http://blog.jboost.cn/tags/docker/"},{"name":"log-pilot","slug":"log-pilot","permalink":"http://blog.jboost.cn/tags/log-pilot/"},{"name":"linux","slug":"linux","permalink":"http://blog.jboost.cn/tags/linux/"},{"name":"mybatis","slug":"mybatis","permalink":"http://blog.jboost.cn/tags/mybatis/"},{"name":"arch","slug":"arch","permalink":"http://blog.jboost.cn/tags/arch/"},{"name":"elk","slug":"elk","permalink":"http://blog.jboost.cn/tags/elk/"},{"name":"web","slug":"web","permalink":"http://blog.jboost.cn/tags/web/"},{"name":"kvm","slug":"kvm","permalink":"http://blog.jboost.cn/tags/kvm/"},{"name":"agile","slug":"agile","permalink":"http://blog.jboost.cn/tags/agile/"},{"name":"concurrency","slug":"concurrency","permalink":"http://blog.jboost.cn/tags/concurrency/"},{"name":"node","slug":"node","permalink":"http://blog.jboost.cn/tags/node/"},{"name":"npm","slug":"npm","permalink":"http://blog.jboost.cn/tags/npm/"},{"name":"session","slug":"session","permalink":"http://blog.jboost.cn/tags/session/"},{"name":"tomcat","slug":"tomcat","permalink":"http://blog.jboost.cn/tags/tomcat/"},{"name":"redission","slug":"redission","permalink":"http://blog.jboost.cn/tags/redission/"},{"name":"swagger","slug":"swagger","permalink":"http://blog.jboost.cn/tags/swagger/"},{"name":"logback","slug":"logback","permalink":"http://blog.jboost.cn/tags/logback/"},{"name":"idea","slug":"idea","permalink":"http://blog.jboost.cn/tags/idea/"},{"name":"git","slug":"git","permalink":"http://blog.jboost.cn/tags/git/"},{"name":"threadpool","slug":"threadpool","permalink":"http://blog.jboost.cn/tags/threadpool/"}]}